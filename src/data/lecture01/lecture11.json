[
  {
    "id": 1,
    "questionDe": "(s1) Warum dürfen sich Ball Tree Sphären auf derselben Ebene überlappen?",
    "questionJa": "なぜBall Treeの同じ階層にある球（スフィア）は重なってもよいのか？",
    "answerDe": ["Weil überlappende Sphären die Flexibilität bei der Raumabdeckung verbessern und die Suche nicht beeinträchtigen."],
    "answerJa": ["空間全体を柔軟にカバーでき、探索の正確性にも問題がないため。"],
    "explanationDe": [
      "Ball Trees sind hierarchische Strukturen zur schnellen Näherungssuche. Die Knoten (Sphären) auf derselben Ebene dürfen sich überlappen, da das die Abdeckung des Datenraums verbessert.",
      "Durch die Hierarchie kann der Suchraum dennoch effizient eingeschränkt werden, auch wenn sich Sphären überlappen.",
      "Beispiel: Zwei benachbarte Cluster können überlappende Balls haben, ohne dass falsche Zuordnungen entstehen."
    ],
    "explanationJa": [
      "Ball Treeは、高速な近傍検索を実現する階層構造です。同じ階層の球が重なることで、データ空間全体をより柔軟にカバーできます。",
      "この重なりによって検索精度が下がることはなく、むしろ効率的な探索が可能になります。",
      "例：隣接する2つのクラスタが少し重なっていても、検索アルゴリズムは正しく対象を絞り込めます。"
    ],
    "originalSlideText": "Clustering\nBall Tree Sphären auf der gleichen Ebene dürfen\n✔ überlappen",
    "explanationImage": "",
    "questionImage": ""
  },
  {
    "id": 2,
    "questionDe": "(s2) Wie erkennt man mit der modified Hubert Statistic die optimale Anzahl von Clustern?",
    "questionJa": "modified Hubert statisticを使って、最適なクラスタ数はどう見つけるか？",
    "answerDe": ["Man wählt das k mit dem größten Anstieg der Statistikwerte."],
    "answerJa": ["統計量が最も大きく上昇するポイントのクラスタ数kを選ぶ。"],
    "explanationDe": [
      "Die modified Hubert Statistic bewertet die Konsistenz der Clusterergebnisse.",
      "Man betrachtet den Verlauf der Statistik über verschiedene k-Werte und identifiziert den Punkt mit dem stärksten Anstieg.",
      "Dieser Punkt deutet darauf hin, dass ab diesem k die Clusterstruktur deutlich besser wird."
    ],
    "explanationJa": [
      "modified Hubert statisticはクラスタリングの安定性（整合性）を評価するための指標です。",
      "さまざまなクラスタ数kに対してこの統計値の変化を見ていき、最も大きく値が増加するところが、構造が大きく改善されたことを示しています。",
      "そのため、そのポイントのkを最適なクラスタ数とみなします。"
    ],
    "originalSlideText": "Clustering\nBei der modified hubert statistic ist die beste Clusteranzahl\n✔ das k nach dem größten Anstieg",
    "explanationImage": "",
    "questionImage": ""
  },
  {
    "id": 3,
    "questionDe": "(s3) In welchem Aspekt ist Consensus Clustering effizienter als Evidence Accumulation?",
    "questionJa": "Consensus ClusteringはEvidence Accumulationに比べて、どの点でより効率的か？",
    "answerDe": ["Im Speicherbedarf, da es keine vollständigen Ähnlichkeitsmatrizen speichern muss."],
    "answerJa": ["メモリ使用量の点。完全な類似度行列を保持する必要がないため。"],
    "explanationDe": [
      "Evidence Accumulation verwendet eine große Ähnlichkeitsmatrix zwischen allen Datenpunkten, was bei vielen Punkten sehr speicherintensiv ist.",
      "Consensus Clustering reduziert diesen Bedarf, da es typischerweise aggregierte oder vereinfachte Informationen speichert.",
      "Das macht es skalierbarer für große Datensätze."
    ],
    "explanationJa": [
      "Evidence Accumulationでは、すべてのデータ点間の類似度行列を保持する必要があり、大量のメモリを使用します。",
      "一方、Consensus Clusteringでは、そのような巨大な行列を使わず、集約的な情報で済ませるため、メモリ消費が抑えられます。",
      "そのため、大規模なデータに対しても効率よく使うことができます。"
    ],
    "originalSlideText": "Clustering\nConsensus Clustering hat den Vorteil gegenüber Evidence Accumulation\n✔ im Speicherbedarf",
    "explanationImage": "",
    "questionImage": ""
  },
  {
    "id": 4,
    "questionDe": "(s4) Nach welchem Prinzip erfolgt die Gruppenzuordnung im Nearest Neighbor Verfahren?",
    "questionJa": "Nearest Neighbor法では、どのような原理でデータをグループに分類するか？",
    "answerDe": ["Daten werden basierend auf der geringsten Distanz zu bekannten Nachbarn klassifiziert."],
    "answerJa": ["既知の近傍との距離が最も小さいことを基に分類される。"],
    "explanationDe": [
      "Beim Nearest Neighbor Verfahren wird ein neuer Punkt der Klasse des nächsten bekannten Punkts (Nachbarn) zugewiesen.",
      "Die zugrunde liegende Annahme ist, dass ähnliche Punkte nahe beieinander im Merkmalsraum liegen.",
      "Beispiel: Wenn ein neuer Punkt nah an einem Cluster von Punkten mit Klasse A liegt, wird er ebenfalls als Klasse A klassifiziert."
    ],
    "explanationJa": [
      "Nearest Neighbor法では、新しいデータ点は、既知のデータのうち最も近い（距離が小さい）点のクラスに分類されます。",
      "これは、『似たものは近くにある』という前提に基づいており、特徴空間において近いもの同士を同じグループとみなします。",
      "例：新しいデータがクラスAの点の集まりに近ければ、その点もクラスAと判断されます。"
    ],
    "originalSlideText": "Clustering\nDaten werden beim Nearest Neighbor Verfahren aufgrund ihrer\n✔ geringen Distanz in Gruppen klassifiziert",
    "explanationImage": "",
    "questionImage": ""
  },
  {
    "id": 5,
    "questionDe": "(s5) Wie erfolgt die Clusterzuordnung beim divisiven Clustering?",
    "questionJa": "divisives Clusteringでは、どのようにしてデータがクラスタに割り当てられるか？",
    "answerDe": ["Jeder Datenpunkt wird eindeutig einem Cluster zugeordnet."],
    "answerJa": ["各データ点は明確に1つのクラスタに割り当てられる。"],
    "explanationDe": [
      "Beim divisiven Clustering wird der gesamte Datensatz anfangs als ein Cluster betrachtet und dann schrittweise in kleinere Cluster aufgeteilt.",
      "Dabei wird jeder Punkt klar einem Teilcluster zugeordnet – es gibt keine Wahrscheinlichkeiten oder Überlappungen.",
      "Dendrogramme sind typisch für agglomeratives Clustering, aber nicht unbedingt für divisives."
    ],
    "explanationJa": [
      "divisives Clustering（分割的クラスタリング）は、最初にすべてのデータを1つの大きなクラスタとみなし、そこから段階的に分割していく手法です。",
      "この方法では、各データ点が必ずどれか1つのクラスタに明確に属するように分類されます（重なりや確率的な所属はない）。",
      "なお、Dendrogram（樹形図）は主にagglomeratives（凝集的）Clusteringで使われることが多く、divisivesでは必須ではありません。"
    ],
    "originalSlideText": "Clustering\nDivisives Clustering:\n✔ Die Datenpunkte werden eindeutig einem Cluster zugeordnet",
    "explanationImage": "",
    "questionImage": ""
  },
  {
    "id": 5,
    "questionDe": "(s5) Wie erfolgt die Clusterzuordnung beim divisiven Clustering?",
    "questionJa": "divisives Clusteringでは、どのようにしてデータがクラスタに割り当てられるか？",
    "answerDe": ["Jeder Datenpunkt wird eindeutig einem Cluster zugeordnet."],
    "answerJa": ["各データ点は明確に1つのクラスタに割り当てられる。"],
    "explanationDe": [
      "Beim divisiven Clustering wird der gesamte Datensatz anfangs als ein Cluster betrachtet und dann schrittweise in kleinere Cluster aufgeteilt.",
      "Dabei wird jeder Punkt klar einem Teilcluster zugeordnet – es gibt keine Wahrscheinlichkeiten oder Überlappungen.",
      "Dendrogramme sind typisch für agglomeratives Clustering, aber nicht unbedingt für divisives."
    ],
    "explanationJa": [
      "divisives Clustering（分割的クラスタリング）は、最初にすべてのデータを1つの大きなクラスタとみなし、そこから段階的に分割していく手法です。",
      "この方法では、各データ点が必ずどれか1つのクラスタに明確に属するように分類されます（重なりや確率的な所属はない）。",
      "なお、Dendrogram（樹形図）は主にagglomeratives（凝集的）Clusteringで使われることが多く、divisivesでは必須ではありません。"
    ],
    "originalSlideText": "Clustering\nDivisives Clustering:\n✔ Die Datenpunkte werden eindeutig einem Cluster zugeordnet",
    "explanationImage": "",
    "questionImage": ""
  }
  
,
{
  "id": 6,
  "questionDe": "(s6) Wie funktioniert das Prinzip von Evidence Accumulation im Clustering?",
  "questionJa": "Evidence Accumulationでは、クラスタリングはどのような原理で行われるか？",
  "answerDe": ["Es aggregiert Clustering-Ergebnisse aus mehreren Läufen zu einer Konsensstruktur."],
  "answerJa": ["複数回のクラスタリング結果を集約して、全体としての構造を見出す。"],
  "explanationDe": [
    "Evidence Accumulation führt mehrere Clustering-Durchläufe mit unterschiedlichen Parametern oder Subsets der Daten aus.",
    "Die resultierenden Clusterzuordnungen werden aggregiert, um ein stabileres Gesamtbild der Clusterstruktur zu erhalten.",
    "Ziel ist es, die Robustheit und Konsistenz der gefundenen Cluster zu erhöhen."
  ],
  "explanationJa": [
    "Evidence Accumulationでは、異なる条件（パラメータやデータのサブセット）で何度もクラスタリングを実行します。",
    "その結果得られるクラスタの情報を集約（accumulate）することで、より信頼性の高いクラスタ構造を抽出します。",
    "このようにして、クラスタ構造の安定性や一貫性を高めることが目的です。"
  ],
  "originalSlideText": "Clustering\nEvidence Accumulation erstellt\n✔ in mehrfachen Läufen Clustering Ergebnisse",
  "explanationImage": "",
  "questionImage": ""
}
,
{
  "id": 7,
  "questionDe": "(s7) Welches Maß verwendet hierarchisches Clustering zur Bestimmung der Nähe zwischen Objekten?",
  "questionJa": "階層的クラスタリングでは、オブジェクト間の近さを測るためにどのような尺度が使われるか？",
  "answerDe": ["Ein Ähnlichkeitsmaß."],
  "answerJa": ["類似度尺度（Ähnlichkeitsmaß）"],
  "explanationDe": [
    "Beim hierarchischen Clustering wird die Ähnlichkeit (statt der Distanz) zwischen Objekten oder Clustern verwendet, um sie zusammenzufassen.",
    "Ein Beispiel für ein Ähnlichkeitsmaß ist der Kosinus-Ähnlichkeitswert, bei dem Vektoren mit ähnlicher Richtung als nah angesehen werden.",
    "Im Gegensatz dazu verwenden viele andere Verfahren explizite Distanzmaße wie euklidische Distanz."
  ],
  "explanationJa": [
    "階層的クラスタリングでは、データ同士をどれだけ『似ているか』という視点でグループ化します。",
    "そのため、ユークリッド距離のような単なる『距離』ではなく、類似度（Ähnlichkeitsmaß）を用います。",
    "例：ベクトルの方向の一致度を測るコサイン類似度は、典型的な類似度尺度の一つです。"
  ],
  "originalSlideText": "Clustering\nHierarchisches Clustering benutzt ein\n✔ Ähnlichkeitsmaß",
  "explanationImage": "",
  "questionImage": ""
}
,
{
  "id": 8,
  "questionDe": "(s8) Welche Vorgehensweise beschreibt das hierarchische Clustering im Allgemeinen?",
  "questionJa": "階層的クラスタリングは、一般にどのような進め方をとるか？",
  "answerDe": ["Es folgt einer bottom-up Technik (agglomerativ)."],
  "answerJa": ["ボトムアップ方式（凝集的手法）で行われる。"],
  "explanationDe": [
    "Im agglomerativen (bottom-up) Clustering beginnt man mit einzelnen Punkten und fasst sie schrittweise zu größeren Clustern zusammen.",
    "Dies steht im Gegensatz zum divisiven (top-down) Ansatz, der mit einem großen Cluster startet und diesen aufteilt.",
    "Die bottom-up Technik ist besonders anschaulich in Dendrogrammen darstellbar."
  ],
  "explanationJa": [
    "階層的クラスタリングでは、一般にボトムアップ方式（agglomerative clustering）をとります。",
    "これは、各データ点を個別のクラスタとして開始し、似ている点同士を順次結合していく方法です。",
    "この手法はDendrogram（樹形図）で視覚的に表現しやすく、クラスタの結合プロセスが直感的に理解できます。"
  ],
  "originalSlideText": "Clustering\nHierarchisches Clustering ist eine\n✔ bottom up Technik",
  "explanationImage": "",
  "questionImage": ""
}
,
{
  "id": 9,
  "questionDe": "(s9) Wie unterteilen KD-Bäume den Datenraum zur Strukturierung von Daten?",
  "questionJa": "KD木は、データ空間をどのように分割して構造化するか？",
  "answerDe": ["In rechteckige Hyperflächen."],
  "answerJa": ["直方体状のハイパー面（高次元の長方形）に分割する。"],
  "explanationDe": [
    "KD-Bäume (k-d trees) zerlegen den Raum rekursiv entlang einzelner Dimensionen.",
    "Dabei entstehen axis-parallel rechteckige Bereiche, sogenannte Hyperflächen.",
    "Diese Struktur ermöglicht effiziente Bereichsanfragen und Nachbarschaftssuche."
  ],
  "explanationJa": [
    "KD木（k-d tree）は、空間を再帰的に1つの軸ごとに分割していきます。",
    "この分割によって、高次元空間は軸に沿った直方体（ハイパー面）に区切られます。",
    "この構造により、近傍検索や範囲検索などが高速に行えるようになります。"
  ],
  "originalSlideText": "Clustering\nKD Bäume teilen den Raum\n✔ in rechteckige Hyperflächen auf",
  "explanationImage": "",
  "questionImage": ""
}
,
{
  "id": 10,
  "questionDe": "(s10) Für welche Art von Daten eignet sich die lineare Regression besonders gut?",
  "questionJa": "線形回帰（Lineare Regression）は、どのような種類のデータに最も適しているか？",
  "answerDe": ["Für numerische Daten."],
  "answerJa": ["数値データ。"],
  "explanationDe": [
    "Lineare Regression ist ein Modell, das versucht, eine lineare Beziehung zwischen unabhängigen (Input) und abhängigen (Ziel) Variablen zu finden.",
    "Da das Modell kontinuierliche Werte berechnet, funktioniert es am besten mit numerischen Eingabedaten.",
    "Kategorische Variablen müssen vorher umkodiert werden (z. B. mittels One-Hot-Encoding), bevor sie im Modell verwendet werden können."
  ],
  "explanationJa": [
    "線形回帰は、入力（説明変数）と出力（目的変数）との間に線形な関係があると仮定して予測を行うモデルです。",
    "このモデルは連続値を扱うため、数値データとの相性が良いです。",
    "カテゴリ変数を扱うには、事前にOne-Hotエンコーディングなどで数値に変換する必要があります。"
  ],
  "originalSlideText": "Clustering\nLineare Regression funktionieren\n✔ gut mit numerischen Daten",
  "explanationImage": "",
  "questionImage": ""
}
,
{
  "id": 11,
  "questionDe": "(s11) Wie funktioniert die Vorhersage bei der linearen Regression in Bezug auf neue Datenpunkte?",
  "questionJa": "線形回帰は、新しいデータ点に対してどのように予測を行うか？",
  "answerDe": ["Sie berechnet für eine Instanz einen Vorhersagewert basierend auf den Modellparametern."],
  "answerJa": ["1つのインスタンスに対して、モデルパラメータに基づいて予測値を計算する。"],
  "explanationDe": [
    "Ein trainiertes lineares Regressionsmodell kann auf einen neuen Datenpunkt (Instanz) angewendet werden, um einen Zielwert vorherzusagen.",
    "Die Vorhersage erfolgt durch Einsetzen der Merkmalswerte in die Regressionsgleichung.",
    "Die Form ist: y = β₀ + β₁x₁ + ... + βₙxₙ"
  ],
  "explanationJa": [
    "線形回帰モデルが学習済みであれば、新しいデータ点（1インスタンス）に対して、目的変数の予測値を計算することができます。",
    "これは、説明変数をモデルの数式に代入して行います。",
    "一般的な形は y = β₀ + β₁x₁ + ... + βₙxₙ です。"
  ],
  "originalSlideText": "Clustering\nLineare Regressionen\n✔ können für eine Instanz unbekannte Datenpunkte Vorhersagen",
  "explanationImage": "",
  "questionImage": ""
}
,
{
  "id": 12,
  "questionDe": "(s12) Was gibt eine logistische Regression als Ergebnis einer Vorhersage zurück?",
  "questionJa": "ロジスティック回帰では、予測結果として何が出力されるか？",
  "answerDe": ["Ein kontinuierlicher Wert zwischen 0 und 1, der als Wahrscheinlichkeit interpretiert werden kann."],
  "answerJa": ["0から1の間の連続値であり、確率として解釈できる。"],
  "explanationDe": [
    "Die logistische Regression ist ein Klassifikationsmodell, das eine Wahrscheinlichkeitsausgabe liefert.",
    "Obwohl das Ergebnis kontinuierlich ist, wird es typischerweise für binäre Entscheidungen (z. B. ab 0.5 = Klasse 1) genutzt.",
    "Beispiel: Ein Wert von 0.83 kann als 83 % Wahrscheinlichkeit interpretiert werden, dass das Objekt zur Klasse 1 gehört."
  ],
  "explanationJa": [
    "ロジスティック回帰は分類モデルですが、出力は0～1の間の連続値になります。",
    "この値は通常、クラス1である確率として扱われ、0.5以上ならクラス1と判断するといった方法で使われます。",
    "例：出力が0.83であれば、「83％の確率でクラス1に属する」と解釈されます。"
  ],
  "originalSlideText": "Clustering\nLogistische Regressionen\n✔ geben kontinuierliche Vorhersage",
  "explanationImage": "",
  "questionImage": ""
}
,
{
  "id": 13,
  "questionDe": "(s13) Wie hilft der Silhouette Coefficient bei der Bewertung von Clustering-Ergebnissen?",
  "questionJa": "Silhouette Coefficientはクラスタリングの評価にどのように役立つか？",
  "answerDe": [
    "Er hilft dabei, die optimale Clusteranzahl zu bestimmen.",
    "Er misst, wie gut ein Punkt zu seinem eigenen Cluster passt im Vergleich zu anderen Clustern."
  ],
  "answerJa": [
    "最適なクラスタ数を決定するのに役立つ。",
    "各点が自分の属するクラスタとどれだけ適合しているか、他クラスタとの分離度を含めて評価できる。"
  ],
  "explanationDe": [
    "Der Silhouette Coefficient ist ein Maß zur Bewertung der Clusterqualität.",
    "Für jeden Punkt wird berechnet, wie nah er seinem eigenen Cluster ist (a) und wie weit er vom nächsten fremden Cluster entfernt ist (b).",
    "Die Silhouette ergibt sich aus (b - a) / max(a, b) und liegt zwischen -1 und 1.",
    "Ein Wert nahe +1 zeigt, dass der Punkt gut in seinem Cluster liegt. Durchschnittswerte über alle Punkte helfen, die beste Clusteranzahl zu wählen."
  ],
  "explanationJa": [
    "Silhouette Coefficientは、クラスタリングの質を評価するための指標で、1つ1つの点に対して計算されます。",
    "点が属するクラスタ内の平均距離（a）と、最も近い別クラスタとの平均距離（b）を使って、(b - a) / max(a, b) の式で求められます。",
    "この値は -1～+1 の範囲で、+1 に近いほどクラスタリングが良好であることを示します。",
    "全データの平均スコアを見ることで、最も適切なクラスタ数を判断できます。"
  ],
  "originalSlideText": "Clustering\nMit dem Silhouette Coefficient kann man\n✔ Die optimale Clusteranzahl berechnen",
  "explanationImage": "",
  "questionImage": ""
}
,
{
  "id": 14,
  "questionDe": "(s14) Welchen Abstand verwendet das Single Linkage Verfahren zur Clusterbildung?",
  "questionJa": "Single Linkage法では、クラスタをまとめる際にどのような距離を使うか？",
  "answerDe": [
    "Es verwendet den minimalen Abstand zwischen zwei Punkten aus unterschiedlichen Gruppen.",
    "Dies führt zu einer schrittweisen Verbindung nahe beieinanderliegender Punkte."
  ],
  "answerJa": [
    "異なるクラスタ間にある2点の最小距離を用いる。",
    "これにより、最も近い点同士をつなげていく形式でクラスタを形成する。"
  ],
  "explanationDe": [
    "Single Linkage ist eine Methode im hierarchischen Clustering, bei der jeweils die Cluster mit dem geringsten Abstand zweier Punkte zusammengeführt werden.",
    "Diese Methode kann zu 'langen Ketten' führen, bei denen Cluster stark auseinandergezogen wirken.",
    "Vorteil: empfindlich für feine Strukturen. Nachteil: anfällig für Ausreißer."
  ],
  "explanationJa": [
    "Single Linkageは階層的クラスタリングの一種で、異なるクラスタ間にある最も近い2点の距離を使ってクラスタを結合します。",
    "この方法では、細長く連結されたようなクラスタ（チェーン構造）ができることがあり、それが弱点になることもあります。",
    "細かい構造を捉えるのに向いていますが、外れ値の影響を受けやすいという特徴もあります。"
  ],
  "originalSlideText": "Clustering\nSingle Linkage nutzt den\n✔ minimalen Abstand zweier Punkte aus den Gruppen",
  "explanationImage": "",
  "questionImage": ""
}
,
{
  "id": 15,
  "questionDe": "(s15) Wie wählt das k-Means Verfahren initial die Clusterzentren aus?",
  "questionJa": "k-means法は初期のクラスタ中心をどのように選ぶか？",
  "answerDe": [
    "Die Clusterzentren werden zufällig aus den Datenpunkten ausgewählt.",
    "Diese Zufallswahl kann zu schlechter Konvergenz führen."
  ],
  "answerJa": [
    "クラスタ中心はデータ点からランダムに選ばれる。",
    "この初期化のランダム性が結果に大きく影響する可能性がある。"
  ],
  "explanationDe": [
    "Im Standard-k-Means-Verfahren werden zu Beginn k Datenpunkte zufällig als Clusterzentren gewählt.",
    "Diese Initialisierung kann problematisch sein, da schlecht gewählte Startpunkte zu suboptimaler Konvergenz oder falschen Ergebnissen führen können.",
    "Deshalb wurde k-Means++ entwickelt, welches die Zentren gezielter auswählt, um bessere Ergebnisse zu erzielen."
  ],
  "explanationJa": [
    "k-meansアルゴリズムでは、初期のクラスタ中心をランダムにデータから選びます。",
    "この初期値のランダム性により、局所最適に陥るなどの問題が起こることがあります。",
    "これを改善する方法として、初期中心を戦略的に選ぶk-means++があり、より安定した結果を得ることができます。"
  ],
  "originalSlideText": "Clustering\nk means berechnet die Clusterzentren initial\n✔ zufällig",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 16,
  "questionDe": "(s16) Wie kann das k-means Verfahren effizienter gestaltet werden?",
  "questionJa": "k-meansアルゴリズムをより効率的にする方法は？",
  "answerDe": [
    "Durch die Verwendung von kd-Trees zur schnelleren Berechnung von nächsten Nachbarn.",
    "Dies reduziert die Anzahl der Distanzberechnungen pro Iteration."
  ],
  "answerJa": [
    "kd-Treeを使うことで、近傍探索が高速化される。",
    "これにより、各イテレーションでの距離計算の数を削減できる。"
  ],
  "explanationDe": [
    "Das klassische k-means Verfahren berechnet in jeder Iteration die Distanz zwischen jedem Punkt und jedem Clusterzentrum.",
    "Dies ist ineffizient bei großen Datenmengen.",
    "kd-Trees ermöglichen es, die Punkte effizient nach ihrer Nähe zu Clustermittelpunkten zu durchsuchen, wodurch sich die Laufzeit signifikant verbessert."
  ],
  "explanationJa": [
    "従来のk-meansアルゴリズムでは、すべての点とクラスタ中心の間の距離を毎回計算する必要があり、大規模データでは非効率です。",
    "kd-Treeを用いることで、空間を効率的に分割して近い点の検索が高速化され、距離計算が大幅に減らせます。",
    "その結果、k-meansの処理時間が大幅に短縮されます。"
  ],
  "originalSlideText": "Clustering\nk-means kann man beschleunigen durch\n✔ kd-Trees",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 17,
  "questionDe": "(s17) Nach welchem Kriterium wird das Split-Attribut in einem Entscheidungsbaum ausgewählt?",
  "questionJa": "決定木で分割に使う属性は、どのような基準で選ばれるか？",
  "answerDe": [
    "Es wird das Attribut mit dem höchsten Information Gain gewählt.",
    "Dieses trennt die Daten am besten in Bezug auf Zielklassen."
  ],
  "answerJa": [
    "最もInformation Gain（情報利得）が高い属性が選ばれる。",
    "これは、目的変数に対してデータを最もよく分割できる属性である。"
  ],
  "explanationDe": [
    "Information Gain misst die Verringerung der Entropie nach einer Aufteilung.",
    "Ein Attribut mit hohem Information Gain trennt die Daten in möglichst reine Gruppen.",
    "Beispiel: Wenn die Aufteilung nach dem Attribut 'Farbe' fast nur noch eine Klasse pro Gruppe ergibt, ist der Information Gain hoch."
  ],
  "explanationJa": [
    "Information Gainは、ある属性でデータを分割したときに、どれだけクラスの不純度（エントロピー）が減るかを測る指標です。",
    "値が高いほど、その属性で分けたときにクラスがきれいに分かれることを意味します。",
    "例：'色'という属性で分けると、それぞれのグループがほぼ1つのクラスになるなら、その属性のInformation Gainは高いといえます。"
  ],
  "originalSlideText": "Decision Trees\nBei der Berechnung eines Splitattributes wird das Attribut mit dem [...] Information Gain gewählt\n✔ höchsten",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 18,
  "questionDe": "(s18) Warum gelten Entscheidungsbäume als Whitebox-Verfahren?",
  "questionJa": "なぜDecision TreeはWhitebox（ホワイトボックス）手法とされるのか？",
  "answerDe": [
    "Weil ihre Entscheidungsregeln transparent und nachvollziehbar sind.",
    "Man kann den gesamten Entscheidungsweg vom Wurzelknoten bis zum Blatt verfolgen."
  ],
  "answerJa": [
    "決定ルールが明確で追跡可能だから。",
    "ルートから葉に至るまでの判断プロセスをすべて把握できるため。"
  ],
  "explanationDe": [
    "Whitebox-Verfahren zeichnen sich dadurch aus, dass ihre internen Entscheidungsprozesse nachvollziehbar und erklärbar sind.",
    "Ein Decision Tree besteht aus klaren Wenn-Dann-Regeln, die man leicht interpretieren kann.",
    "Im Gegensatz dazu liefern Blackbox-Modelle wie neuronale Netze schwer verständliche Ergebnisse."
  ],
  "explanationJa": [
    "Whitebox手法とは、アルゴリズム内部の判断過程が明確に理解できる手法のことです。",
    "Decision Treeでは、「○○がAより大きいなら→左」などの明確なルールに従って分岐するため、判断過程が説明しやすくなります。",
    "一方、ニューラルネットワークのようなBlackboxモデルは、内部の処理が複雑で理解しにくいです。"
  ],
  "originalSlideText": "Decision Trees sind ein\n✔ Whitebox Verfahren",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 19,
  "questionDe": "(s19) Wie ordnet ein Decision Tree die Aussagekraft von Attributen entlang des Pfades an?",
  "questionJa": "Decision Treeは、ルートから葉にかけて、属性の情報価値をどのように配置していくか？",
  "answerDe": [
    "Von Attributen mit hoher Aussagekraft zu solchen mit geringerer Aussagekraft.",
    "Attribute nahe der Wurzel tragen am meisten zur Klassifikation bei."
  ],
  "answerJa": [
    "情報価値の高い属性から低い属性へと順に並べられる。",
    "ルートに近い属性ほど分類への影響が大きい。"
  ],
  "explanationDe": [
    "Ein Decision Tree wählt Attribute auf Basis des höchsten Information Gain aus.",
    "Die Wurzel enthält das wichtigste Attribut für die Klassifikation.",
    "Nach unten hin im Baum werden Attribute verwendet, die kleinere Unterscheidungen vornehmen – also eine geringere Aussagekraft haben."
  ],
  "explanationJa": [
    "Decision Treeでは、最もInformation Gain（情報利得）が大きい属性を上位に持ってきて分類を行います。",
    "ルートノードには最も分類に寄与する属性が配置され、その後、分岐ごとにやや重要性の低い属性が使われます。",
    "このようにして、木の上から下に向かって属性の重要度は低くなっていくのが一般的です。"
  ],
  "originalSlideText": "Ein Decision Tree ordnet die Aussagekraft von Attributen im Pfad von der Wurzel zu den Blättern von\n✔ hoch nach gering",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 20,
  "questionDe": "(s20) Welche typischen Probleme können bei der Verwendung von Decision Trees auftreten?",
  "questionJa": "Decision Treeを使う際に起こりやすい典型的な問題には何があるか？",
  "answerDe": [
    "Overfitting – der Baum passt sich zu stark an das Trainingsset an.",
    "Lokale Minima – durch greedy Splits können global bessere Lösungen verpasst werden."
  ],
  "answerJa": [
    "過学習（Overfitting）– トレーニングデータに過剰に適合してしまう。",
    "局所解（ローカルミニマ）– 貪欲法により全体最適を逃す可能性がある。"
  ],
  "explanationDe": [
    "Overfitting tritt auf, wenn ein Baum zu tief ist und jedes Trainingsbeispiel exakt abbildet – dies reduziert die Generalisierbarkeit.",
    "Da Decision Trees rekursiv und lokal optimale Splits wählen (greedy), kann es passieren, dass der globale beste Baum nicht gefunden wird – man spricht von lokalen Minima.",
    "Maßnahmen dagegen: Pruning, Begrenzung der Baumtiefe, Ensemble-Methoden (Random Forests)."
  ],
  "explanationJa": [
    "Decision Treeは深くなりすぎると、訓練データに対して完全に一致するように構築されてしまい、新しいデータへの汎化能力が落ちてしまいます（Overfitting）。",
    "また、各分割で最適な属性を局所的に選んでいくため、全体として最良の構造（グローバル最適）にならないことがあります（ローカルミニマ）。",
    "対策としては、剪定（pruning）や深さの制限、ランダムフォレストなどのアンサンブル法が挙げられます。"
  ],
  "originalSlideText": "Decision Trees\nProblem(e) mit Decision Trees\n✔ Overfitting\n✔ Lokale Minima",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 21,
  "questionDe": "(s21) Was passiert mit der Entropie in den Teilmengen nach einem Split in einem Entscheidungsbaum?",
  "questionJa": "決定木で属性による分割を行ったとき、部分集合内のエントロピーはどうなるか？",
  "answerDe": [
    "Die Entropie sinkt, weil die Daten nach dem Split homogener werden.",
    "Ein guter Split erzeugt möglichst reine Teilmengen."
  ],
  "answerJa": [
    "エントロピーは減少する。分割後のデータはより純粋になるため。",
    "良い分割では、クラスが偏った部分集合ができ、エントロピーが小さくなる。"
  ],
  "explanationDe": [
    "Die Entropie ist ein Maß für die Unreinheit (Unordnung) einer Datenmenge.",
    "Ein Split mit einem geeigneten Attribut teilt die Daten so auf, dass die resultierenden Gruppen homogener sind.",
    "Das Ziel im Decision Tree ist es, die Entropie nach jedem Split zu minimieren, was die Klassifikation erleichtert."
  ],
  "explanationJa": [
    "エントロピーは、データの不純度（ばらつき）を表す指標です。",
    "良い属性で分割すると、それぞれのグループが1つのクラスに偏った構成になり、不純度が減ってエントロピーが小さくなります。",
    "このように、分割後にエントロピーが下がることが、決定木において分類がうまく進んでいることを示します。"
  ],
  "originalSlideText": "Decision Trees\nSetze ein: Die Entropie in den Teilmenge [...] nach einem Split mit einem Attribut\n✔ Sinkt",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 22,
  "questionDe": "(s22) Was passiert mit den Attributen bei der Dimensionsreduktion?",
  "questionJa": "次元削減では、属性（特徴量）はどのように変化するか？",
  "answerDe": [
    "Die ursprünglichen Attribute werden durch eine deutlich kleinere Menge neuer Attribute ersetzt.",
    "Diese neuen Attribute (z. B. Hauptkomponenten) erfassen die wichtigsten Informationsrichtungen der Daten."
  ],
  "answerJa": [
    "元の属性は、より少ない数の新しい属性に置き換えられる。",
    "この新しい属性（例：主成分）は、データの中で重要な情報を保持している方向を表している。"
  ],
  "explanationDe": [
    "Bei der Dimensionsreduktion (z. B. durch PCA) werden viele ursprüngliche Merkmale zu wenigen neuen Merkmalen zusammengefasst.",
    "Diese neuen Dimensionen enthalten möglichst viel Varianz und reduzieren gleichzeitig Redundanz und Rauschen.",
    "Ziel ist es, Daten zu vereinfachen, ohne viel Information zu verlieren."
  ],
  "explanationJa": [
    "次元削減では、元の多数の特徴量を、情報をなるべく保持した少数の新しい特徴に変換します。",
    "たとえばPCAでは、データの分散を最もよく説明する軸（主成分）を抽出して、それらを新たな次元とします。",
    "これにより、情報を失わずに次元数を減らし、計算効率や可視化のしやすさを向上させます。"
  ],
  "originalSlideText": "Dimensionsreduktion\nBei der Dimensionsreduktion\n✔ werden Attribute durch eine deutlich kleinere Menge an Attributen ersetzt",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 23,
  "questionDe": "(s23) Wozu kann die 'Measure of Sampling Adequacy' (MSA) in der Dimensionsreduktion verwendet werden?",
  "questionJa": "『Measure of Sampling Adequacy (MSA)』は次元削減でどのように利用されるか？",
  "answerDe": [
    "Sie kann zur Berechnung relevanter Faktoren in der Faktorenanalyse genutzt werden.",
    "MSA hilft zu bewerten, ob Variablen für eine Faktoranalyse geeignet sind."
  ],
  "answerJa": [
    "因子分析で意味のある因子を決める際に使われる。",
    "MSAは、各変数が因子分析に適しているかどうかを判断する指標となる。"
  ],
  "explanationDe": [
    "Die MSA bewertet, ob eine Variable ausreichend mit anderen korreliert, um in eine Faktoranalyse aufgenommen zu werden.",
    "Ein hoher MSA-Wert (nahe 1) deutet darauf hin, dass die Variable gut zur gemeinsamen Faktorenstruktur passt.",
    "Zusammen mit dem KMO-Kriterium wird MSA genutzt, um die Eignung der Daten für Dimensionsreduktion zu prüfen."
  ],
  "explanationJa": [
    "MSA（Measure of Sampling Adequacy）は、ある変数が他の変数とどれだけ関連しているかを評価し、それが因子分析に適しているかを判断します。",
    "MSAが1に近いほど、その変数は因子分析に向いているとされます。",
    "KMO指標と併用され、次元削減に適したデータかどうかの判断に使われます。"
  ],
  "originalSlideText": "Dimensionsreduktion\nDie Methode \"Measure of Sampling Adequacy\" kann man\n✔ kann zur Berechnung relevanter Faktoren genutzt werden",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 24,
  "questionDe": "(s24) Was ist ein mögliches Problem bei der Projektion von Datenpunkten in der Dimensionsreduktion?",
  "questionJa": "次元削減でデータ点をプロットする際に生じる問題は何か？",
  "answerDe": [
    "Die Projektion kann zu Overplotting führen, wenn viele Datenpunkte auf ähnliche Koordinaten fallen.",
    "Besonders bei Reduktion auf 2D/3D können Informationen verloren gehen."
  ],
  "answerJa": [
    "多くのデータ点が同じ座標に集まり、重なって表示される（Overplotting）ことがある。",
    "特に2次元や3次元に次元を下げた場合、情報の損失が原因で視認性が下がることがある。"
  ],
  "explanationDe": [
    "Bei der Projektion von hochdimensionalen Daten in 2D oder 3D kommt es häufig vor, dass sich Datenpunkte stark überlagern.",
    "Dies wird als Overplotting bezeichnet und kann die Interpretation erschweren.",
    "Maßnahmen dagegen: Transparenz erhöhen, Jittering, alternative Projektionstechniken (z. B. t-SNE)."
  ],
  "explanationJa": [
    "高次元のデータを2次元や3次元に射影すると、多くのデータ点が同じような位置に集まり、重なってしまうことがあります。",
    "この現象をOverplotting（オーバープロット）と呼び、可視化の判読性を低下させます。",
    "対処法としては、点の透明度を上げる、Jitter（微小なずらし）を加える、t-SNEのような別の手法を使うなどがあります。"
  ],
  "originalSlideText": "Dimensionsreduktion\nDie Projektion von Datenpunkten\n✔ führt zu overplotting",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 25,
  "questionDe": "(s25) Was ist das Ziel der Typ Q Faktorenanalyse bei der Dimensionsreduktion?",
  "questionJa": "次元削減において、Q型因子分析（Typ Q Faktorenanalyse）は何を目的としているか？",
  "answerDe": [
    "Sie gruppiert Attribute, die sich in ihrem Muster ähneln.",
    "Die Analyse fokussiert sich auf die Ähnlichkeit von Variablen (nicht nur auf Korrelation)."
  ],
  "answerJa": [
    "パターンが似ている属性（変数）をグループ化する。",
    "単なる相関だけでなく、属性の全体的な傾向の類似性に注目する。"
  ],
  "explanationDe": [
    "Im Gegensatz zur R-Typ-Faktoranalyse, bei der Personen oder Objekte verglichen werden, vergleicht die Q-Typ-Faktoranalyse Variablen bzw. Attribute miteinander.",
    "Ziel ist es, Gruppen von Attributen zu identifizieren, die ein ähnliches Antwort- oder Verhaltensmuster aufweisen.",
    "Dabei steht nicht unbedingt die statistische Korrelation im Vordergrund, sondern die Ähnlichkeit im Gesamtverlauf der Werteprofile."
  ],
  "explanationJa": [
    "Q型因子分析（Typ Q）は、R型（個人間の因子構造）と異なり、変数（属性）間のパターンの類似性を分析します。",
    "目的は、似たような振る舞いや傾向を示す属性をグループ化することにあります。",
    "このとき重視されるのは単なる数値的な相関よりも、変数の値の全体的な傾向（パターン）です。"
  ],
  "originalSlideText": "Dimensionsreduktion\nDie Typ Q Faktorenanalyse gruppiert Attribute\n✔ die ähnlich sind",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 26,
  "questionDe": "(s26) Welcher Aspekt steht im Fokus bei der Dimensionsreduktion durch Multi-Dimensional Scaling (MDS)?",
  "questionJa": "Multi-Dimensional Scaling（MDS）による次元削減では、どの点が重視されるか？",
  "answerDe": [
    "Die Erhaltung der lokalen Nähe der Datenpunkte.",
    "Das Ziel ist es, relative Distanzen in eine niedrigere Dimension zu übertragen."
  ],
  "answerJa": [
    "データ点同士の局所的な近さ（距離関係）を保つこと。",
    "元の空間の相対的な距離構造を低次元に保ったまま写すことが目的である。"
  ],
  "explanationDe": [
    "MDS versucht, hochdimensionale Daten so in niedrigere Dimensionen abzubilden, dass die paarweisen Abstände möglichst gut erhalten bleiben.",
    "Der Fokus liegt besonders auf der lokalen Struktur – also den Nachbarschaften zwischen Punkten.",
    "Im Gegensatz zur PCA berücksichtigt MDS keine Varianz, sondern rein die Ähnlichkeits- oder Distanzmatrix."
  ],
  "explanationJa": [
    "MDS（多次元尺度構成法）は、データ点間の距離（近さ）をできるだけ保ったまま、次元を下げる手法です。",
    "特に、近い点同士の局所的な関係性を保つことに重点が置かれます。",
    "PCAのように分散の大きさではなく、距離（あるいは類似度）そのものを入力として処理する点が特徴です。"
  ],
  "originalSlideText": "Dimensionsreduktion\nMulti-dimensional Scaling reduziert die Dimensionen unter Beachtung\n✔ lokalen Nähe der Datenpunkte",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 27,
  "questionDe": "(s27) Worauf basiert die Dimensionsreduktion bei der Hauptkomponentenanalyse (PCA)?",
  "questionJa": "主成分分析（PCA）は、どのような基準に基づいて次元削減を行うか？",
  "answerDe": [
    "Auf den Richtungen mit der größten Varianz in den Daten.",
    "Diese Hauptkomponenten fassen die meiste Information der ursprünglichen Merkmale zusammen."
  ],
  "answerJa": [
    "データ内の分散が最大となる方向（主成分）に基づいて行う。",
    "主成分は、元の特徴量に含まれる情報を最もよく保持している軸である。"
  ],
  "explanationDe": [
    "PCA sucht die Achsen (Hauptkomponenten), entlang derer die Daten die größte Streuung (Varianz) aufweisen.",
    "Diese Achsen sind orthogonal zueinander und bilden ein neues Koordinatensystem.",
    "Durch Auswahl der ersten k Hauptkomponenten mit größter Varianz kann die Dimensionalität reduziert werden."
  ],
  "explanationJa": [
    "PCAは、データのばらつき（分散）が最も大きくなる方向を見つけて、新しい軸（主成分）を定義します。",
    "これらの主成分は互いに直交しており、新しい座標系を形成します。",
    "情報の多くは最初のいくつかの主成分に集約されているため、それらだけを残すことで次元を削減できます。"
  ],
  "originalSlideText": "Dimensionsreduktion\nPCA berechnet eine Dimensionsreduktion basierend auf den\n✔ größten Varianzrichtungen",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 28,
  "questionDe": "(s28) In welcher Reihenfolge werden die Hauptkomponenten bei PCA sortiert?",
  "questionJa": "PCAでは主成分はどのような順序で並べられるか？",
  "answerDe": [
    "Von groß nach klein – nach der erklärten Varianz.",
    "Die erste Komponente erklärt die meiste Varianz, die letzte die wenigste."
  ],
  "answerJa": [
    "分散の大きい順（大→小）に並べられる。",
    "第1主成分は最も情報を含み、以降の主成分はそれに比べて少ない情報を表す。"
  ],
  "explanationDe": [
    "Nach der Berechnung der Hauptkomponenten werden diese nach dem Anteil der erklärten Gesamtvarianz sortiert.",
    "So erhält man eine Rangfolge, bei der man gezielt die ersten k Komponenten für eine reduzierte Darstellung auswählen kann.",
    "Beispiel: Wenn die ersten 2 Komponenten 90 % der Varianz erklären, kann man mit ihnen die Daten gut abbilden."
  ],
  "explanationJa": [
    "PCAで求めた主成分は、それぞれがどれだけ分散を説明するか（情報量）によって並べ替えられます。",
    "この順序により、情報の多い上位k個の成分だけを残すことで、データの次元を効果的に縮小できます。",
    "例：最初の2つの主成分で全体の90％の分散が説明できるなら、それだけで元の構造を十分表現できると考えられます。"
  ],
  "originalSlideText": "Dimensionsreduktion\nPCA berechnet eine Dimensionsreduktion basierend auf den\n✔ sortiert von groß nach klein",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 29,
  "questionDe": "(s29) Mit welcher Verteilung platziert t-SNE die Punkte im Zielraum?",
  "questionJa": "t-SNEでは、データ点を配置する際にどの分布を用いるか？",
  "answerDe": [
    "Mit einer t-Verteilung (Student-t-Verteilung).",
    "Diese hilft, den Crowding-Effekt zu reduzieren."
  ],
  "answerJa": [
    "t分布（スチューデントのt分布）を用いる。",
    "これはクラウディング効果（近接点の過密化）を抑えるために使われる。"
  ],
  "explanationDe": [
    "t-SNE (t-distributed Stochastic Neighbor Embedding) ist ein Verfahren zur Dimensionsreduktion, das besonders gut für die Visualisierung von hochdimensionalen Daten geeignet ist.",
    "Im hochdimensionalen Raum berechnet t-SNE die Ähnlichkeit zwischen Datenpunkten mit Hilfe einer Gaußschen Verteilung.",
    "Im Zielraum (typischerweise 2D oder 3D) wird jedoch eine t-Verteilung verwendet. Diese hat schwerere Ränder (\"fat tails\") als die Normalverteilung.",
    "Durch die Verwendung der t-Verteilung wird verhindert, dass unterschiedliche Cluster zu nah aneinander dargestellt werden (Crowding-Effekt)."
  ],
  "explanationJa": [
    "t-SNE（t-分布型確率的近傍埋め込み）は、高次元のデータを2次元や3次元に可視化するための手法です。",
    "高次元空間では、データ点間の近さをガウス分布を使って確率的に表現します。",
    "一方、低次元（可視化）空間では、t分布を使って点を配置します。t分布は正規分布よりも裾が広く、遠くの点の影響もある程度考慮されます。",
    "この工夫により、本来離れているはずのクラスタが押しつぶされて重なってしまう『クラウディング効果』を緩和できます。"
  ],
  "originalSlideText": "Dimensionsreduktion\n✔ TNSE platziert die Punkte in der Ebene\n✔ mit einer t-Verteilung",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 30,
  "questionDe": "(s30) Worauf basiert die Einbettung bei t-SNE?",
  "questionJa": "t-SNEは、どのような情報に基づいてデータを低次元に埋め込むか？",
  "answerDe": [
    "Auf Wahrscheinlichkeiten, dass Punkt a in der Nähe von Punkt b liegt.",
    "Diese Wahrscheinlichkeiten werden in beiden Räumen verglichen und die Differenz minimiert."
  ],
  "answerJa": [
    "ある点aが別の点bの近くにある確率に基づく。",
    "高次元空間と低次元空間での確率分布の差（KLダイバージェンス）を最小化することで埋め込みを行う。"
  ],
  "explanationDe": [
    "t-SNE stellt die Nähe zwischen zwei Datenpunkten durch Wahrscheinlichkeiten dar.",
    "Im Originalraum berechnet es für jeden Punkt a die Wahrscheinlichkeit, dass Punkt b ein Nachbar von a ist.",
    "Im niedrigdimensionalen Raum wird versucht, diese Wahrscheinlichkeiten durch die Platzierung der Punkte möglichst gut nachzubilden.",
    "Das Maß für den Unterschied dieser Verteilungen ist die sogenannte Kullback-Leibler-Divergenz, die minimiert wird.",
    "So bleibt die Struktur (Nachbarschaft) der Daten erhalten, auch wenn man sie in 2D oder 3D darstellt."
  ],
  "explanationJa": [
    "t-SNEは、各データ点同士がどれくらい『近いか』を確率として定義します。",
    "具体的には、ある点aの周りに点bが現れる確率を計算し、それを全ての点の組み合わせについて求めます。",
    "そして、それらの確率が低次元空間でも同じようになるように、データ点を配置していきます。",
    "このとき、2つの確率分布の違いを測るためにKLダイバージェンスという指標を使い、それを最小にするように配置を調整します。",
    "これにより、元のデータの『近さ』を保ったまま、2次元などの視覚化可能な形に変換できます。"
  ],
  "originalSlideText": "Dimensionsreduktion\nTSNE nutzt zur Einbettung:\n✔ Wahrscheinlichkeiten, dass Punkt a in der Nähe von Punkt b liegt",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 31,
  "questionDe": "(s31) Wie funktioniert Bootstrapping im Kontext der Evaluation?",
  "questionJa": "評価においてブートストラップ法はどのように機能するか？",
  "answerDe": [
    "Beim Bootstrapping werden zufällig Datenpunkte mit Zurücklegen aus dem Original-Datensatz gezogen.",
    "So entstehen neue Stichproben, die die Verteilung der Originaldaten nachahmen.",
    "Dabei können einzelne Punkte mehrfach erscheinen, andere gar nicht."
  ],
  "answerJa": [
    "ブートストラップ法では、元のデータセットからデータ点を『復元抽出（戻しありのランダムサンプリング）』します。",
    "その結果、一部のデータは複数回選ばれ、別のデータは一度も選ばれない可能性もあります。",
    "この方法により、元のデータ分布を再現した疑似的なデータセットを多数生成することができます。"
  ],
  "explanationDe": [
    "Bootstrapping ist eine statistische Methode zur Schätzung der Genauigkeit (z. B. Konfidenzintervalle) eines Modells.",
    "Dabei wird eine neue Stichprobe aus dem Trainingsdatensatz gebildet, indem man zufällig Punkte auswählt – mit Zurücklegen.",
    "Das bedeutet: Ein und derselbe Datenpunkt kann mehrfach in der neuen Stichprobe erscheinen.",
    "Diese Technik eignet sich besonders gut, wenn man nur eine begrenzte Datenmenge zur Verfügung hat."
  ],
  "explanationJa": [
    "ブートストラップ法とは、データの信頼区間やモデルの性能を評価するための統計手法です。",
    "この方法では、元のデータセットからランダムにデータ点を選びますが、『復元抽出（戻しあり）』で行います。",
    "つまり、選んだデータを元に戻してまた選び直せるため、同じデータが何回も選ばれることがあります。",
    "これにより、新しい擬似データセットを多数生成でき、モデルの頑健性やばらつきを評価できるのです。",
    "注意点として、元データを『ランダム値で置き換える』わけではないので、この選択肢は誤りです。"
  ],
  "originalSlideText": "Evaluation\nBootstrapping\n❌ generiert zufällige Werte als Ersatz für einen Datenpunkt\n✔ dupliziert andere Werte als Ersatz",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 32,
  "questionDe": "(s31) Was bezeichnet man als Resubstitutionsfehler?",
  "questionJa": "Resubstitutionエラーとは何を指すか？",
  "answerDe": [
    "Es ist die Fehlerrate, die ein Modell auf denselben Daten hat, mit denen es trainiert wurde."
  ],
  "answerJa": [
    "Resubstitutionエラーとは、モデルが学習に用いた訓練データに対して出す誤り率のこと。"
  ],
  "explanationDe": [
    "Der Resubstitutionsfehler misst, wie gut ein Modell auf den Trainingsdaten abschneidet.",
    "Dabei wird das Modell mit denselben Daten getestet, die auch zum Trainieren verwendet wurden.",
    "Ein niedriger Resubstitutionsfehler bedeutet nicht automatisch gute Generalisierung, da das Modell überangepasst (overfitted) sein könnte."
  ],
  "explanationJa": [
    "Resubstitutionエラーは、モデルを訓練に使ったデータ（訓練データ）に対して評価したときの誤り率です。",
    "つまり、モデルが『自分の学習に使ったデータ』をどれくらい正しく予測できるかを表します。",
    "このエラーは過学習（オーバーフィッティング）を見抜くには不十分であり、テストデータでの評価も必須です。"
  ],
  "originalSlideText": "Evaluation\nDer Resubstitutionsfehler\n✔ ist die Fehlerrate im Trainingsdatensatz",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 33,
  "questionDe": "(s32) Was berechnet die quadratische Verlustfunktion?",
  "questionJa": "二乗損失関数（Quadratische Verlustfunktion）は何を計算するか？",
  "answerDe": [
    "Sie berechnet den Fehler zwischen vorhergesagten und tatsächlichen Werten.",
    "Oft genutzt bei Regressionsproblemen, nicht zur Minimierung der Fehlerrate direkt."
  ],
  "answerJa": [
    "予測値と実際の値との差（残差）を二乗して足し合わせたものを計算する。",
    "これは回帰問題でよく使われる損失関数であり、分類のように『誤りの回数』を直接最小化するものではない。"
  ],
  "explanationDe": [
    "Die quadratische Verlustfunktion (MSE = Mean Squared Error) misst die durchschnittlichen quadratischen Abweichungen zwischen Prognose und Realität.",
    "Sie wird verwendet, um kontinuierliche Zielgrößen in Regressionsmodellen zu bewerten.",
    "Die Minimierung dieser Funktion führt zu möglichst kleinen Fehlern, aber nicht zwingend zu einer niedrigen Fehlerrate bei Klassifikation."
  ],
  "explanationJa": [
    "二乗損失関数（MSE: Mean Squared Error）は、予測値と正解値の差を二乗して平均をとったもので、連続値を予測する回帰モデルで使われます。",
    "この損失関数は予測の『ずれの大きさ』に敏感であり、大きな誤差を強く罰します。",
    "ただし、『誤りの回数』を意味する分類タスクの『エラー率』とは直接関係ありません。"
  ],
  "originalSlideText": "Evaluation\nDie Quadratische Verlustfunktion\n✔ berechnet die Fehlerrate des Trainingsdatensatzes",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 34,
  "questionDe": "(s33) Was gibt die Spezifität an?",
  "questionJa": "特異度（Spezifität）は何を表すか？",
  "answerDe": [
    "Sie gibt an, wie viele negative Objekte korrekt als negativ klassifiziert wurden.",
    "Spezifität = TN / (TN + FP)"
  ],
  "answerJa": [
    "特異度とは、本来ネガティブであるデータを正しくネガティブと分類できた割合。",
    "具体的には「真の陰性 / (真の陰性 + 偽陽性)」として計算される。"
  ],
  "explanationDe": [
    "Spezifität ist ein Maß für die Genauigkeit bei der Erkennung negativer Klassen.",
    "Sie gibt an, wie gut ein Modell die tatsächlichen Negativfälle erkennt.",
    "Eine hohe Spezifität bedeutet, dass nur wenige falsche Positive gemacht wurden – wichtig z. B. bei medizinischen Tests zur Vermeidung unnötiger Behandlungen."
  ],
  "explanationJa": [
    "特異度（Spezifität）は、ネガティブな事例をどれだけ正確にネガティブと判断できたかを示します。",
    "『偽陽性』（本当は陰性なのに陽性と誤判定された）を避けたい場面で特に重要です。",
    "例：がんのスクリーニング検査では、健康な人を陽性と誤診すると不安や追加検査のリスクが生じます。そのため特異度が高いことが望まれます。"
  ],
  "originalSlideText": "Evaluation\nDie Spezifität gibt an\n✔ wie viele Objekte korrekt negativ klassifiziert wurden",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 35,
  "questionDe": "(s35) Wofür kann eine Kostenmatrix im Rahmen der Evaluation verwendet werden?",
  "questionJa": "Evaluationにおいて、コストマトリクスはどのような目的で使用されるか？",
  "answerDe": [
    "zur Vorhersage mit Hinblick auf die Fehlerkosten genutzt werden",
    "zur Verbesserung des Trainingsprozesses genutzt werden",
    "zur Analyse bei der Evaluierung genutzt werden"
  ],
  "answerJa": [
    "誤分類のコストを考慮した予測に使われる。",
    "学習プロセスの改善に使われる。",
    "評価時の分析に使われる。"
  ],
  "explanationDe": [
    "Eine Kostenmatrix zeigt auf, welche Kosten mit verschiedenen Fehlklassifikationen verbunden sind.",
    "Sie wird häufig bei unbalancierten Datensätzen oder in kritischen Anwendungen (z. B. Medizin, Betrugserkennung) eingesetzt.",
    "Beispiel: Eine falsche Negativentscheidung bei einem Kreditrisiko-Modell kann teurere Folgen haben als eine falsche Positiventscheidung.",
    "Die Matrix kann dabei helfen, die Gewichtung im Training zu beeinflussen oder die Evaluation besser anwendungsorientiert zu interpretieren."
  ],
  "explanationJa": [
    "コストマトリクスは、誤分類が発生した際の『損失の大きさ』を示す表です。",
    "分類ミスによるコストの大小を反映させることで、モデルの予測をより現実に即したものに調整できます。",
    "例えば、医療診断では『本当は病気なのに見逃す』誤りは非常に大きな損失を伴うため、そちらのコストを大きく設定します。",
    "これにより、学習時の重み付けの調整や評価指標の選択にも活用されます。"
  ],
  "originalSlideText": "Evaluation\nEine Kostenmatrix kann\n✔ zur Vorhersage mit Hinblick auf die Fehlerkosten genutzt werden\n✔ zur Verbesserung des Trainingsprozesses genutzt werden\n✔ zur Analyse bei der Evaluierung genutzt werden",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 36,
  "questionDe": "(s36) Was sind False Positives im Rahmen der Evaluation?",
  "questionJa": "EvaluationにおけるFalse Positiveとは何か？",
  "answerDe": [
    "Werte, die tatsächlich falsch sind, aber als wahr klassifiziert wurden."
  ],
  "answerJa": [
    "実際には陰性であるデータを、陽性と誤って分類したもの。"
  ],
  "explanationDe": [
    "False Positives (FP) entstehen, wenn ein Modell ein negatives Beispiel fälschlich als positiv einstuft.",
    "Das bedeutet, dass ein Fall als 'wahr' vorhergesagt wurde, obwohl er in Wirklichkeit 'falsch' ist.",
    "Beispiel: Ein medizinischer Test zeigt eine Krankheit an (positiv), obwohl die getestete Person gesund ist (tatsächlich negativ)."
  ],
  "explanationJa": [
    "False Positive（偽陽性）は、本来陰性であるべきデータを陽性と誤判定することを意味します。",
    "つまり、実際は『違う』のに、モデルが『正しい』と判断してしまったケースです。",
    "例：健康な人を病気と誤診してしまうような場合がこれにあたります。"
  ],
  "originalSlideText": "Evaluation\nFalse Positives sind\n✔ Werte, die tatsächlich falsch sind, aber als wahr klassifiziert wurden",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 37,
  "questionDe": "(s37) Welche Eigenschaft trifft auf die Holdout-Methode im Rahmen der Evaluation zu?",
  "questionJa": "Evaluationにおいて、Holdout法にはどのような特徴があるか？",
  "answerDe": [
    "ist anfällig für nicht-repräsentative Aufteilungen"
  ],
  "answerJa": [
    "データの分割が代表性に欠ける場合がある（バラつきに弱い）。"
  ],
  "explanationDe": [
    "Die Holdout-Methode teilt den Datensatz einmalig in Trainings- und Testdaten auf.",
    "Wenn diese Aufteilung nicht repräsentativ ist – z. B. durch zufällige Zufallsschwankungen oder seltene Klassen – kann die Bewertung des Modells verzerrt sein.",
    "Daher wird oft Cross-Validation bevorzugt, da sie stabilere Schätzungen liefert."
  ],
  "explanationJa": [
    "Holdout法とは、データセットを一度だけ訓練用とテスト用に分けてモデルを評価する方法です。",
    "ただし、この1回の分割が偏っていると、モデルの性能が過小評価または過大評価されるおそれがあります。",
    "そのため、複数回分割して平均を取る交差検証（Cross-Validation）の方が信頼性の高い評価を得られます。"
  ],
  "originalSlideText": "Evaluation\nHoldout\n❌ ist eine Validierungsmethode\n✔ ist anfällig für nicht-repräsentative Aufteilungen",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 38,
  "questionDe": "(s38) Wie sollte die ideale Verteilung der Werte in einer Confusion Matrix aussehen?",
  "questionJa": "Confusion Matrixでは、理想的にはどのような値の分布になるべきか？",
  "answerDe": [
    "große Zahlen auf der Diagonalen, kleine außerhalb"
  ],
  "answerJa": [
    "対角線上に大きな値、それ以外には小さな値がある状態が理想。"
  ],
  "explanationDe": [
    "Die Confusion Matrix zeigt die Leistung eines Klassifikationsmodells.",
    "Die Diagonale enthält die korrekt klassifizierten Beispiele (True Positives und True Negatives).",
    "Hohe Werte auf der Diagonalen bedeuten viele korrekte Vorhersagen.",
    "Außerhalb der Diagonalen stehen Fehlklassifikationen – idealerweise sollen diese möglichst gering sein."
  ],
  "explanationJa": [
    "Confusion Matrix（混同行列）は、分類モデルの予測結果を実際の正解と比較して表にしたものです。",
    "対角線上には正しく分類されたデータが入り、その他のセルには誤分類が記録されます。",
    "したがって、理想的には対角線の値が大きく、その他の場所（誤り）の値が小さい状態が望ましいです。"
  ],
  "originalSlideText": "Evaluation\nIn der Confusion Matrix sollten im Idealfall\n✔ große Zahlen auf der Diagonalen, kleine außerhalb",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 39,
  "questionDe": "(s39) Was passiert bei der Kreuzvalidierung mit den vorhandenen Daten?",
  "questionJa": "Kreuzvalidierungでは、データはどのように扱われるか？",
  "answerDe": [
    "teilt die Daten in mehrere Partitionen auf"
  ],
  "answerJa": [
    "データを複数の分割（パーティション）に分ける。"
  ],
  "explanationDe": [
    "Bei der Kreuzvalidierung wird der Datensatz in k gleich große Teilmengen (Folds) aufgeteilt.",
    "In jedem Durchlauf dient jeweils ein Fold als Testmenge, während die anderen als Trainingsmenge verwendet werden.",
    "Das Verfahren wird k-mal wiederholt, sodass jedes Teilstück genau einmal als Testdaten dient.",
    "Dies sorgt für eine stabilere und zuverlässigere Evaluation des Modells."
  ],
  "explanationJa": [
    "交差検証（Kreuzvalidierung）では、データセットをk個のグループ（フォールド）に分けます。",
    "各ステップで、1つのグループをテスト用に、残りを訓練用に使い、この手順をk回繰り返します。",
    "すべてのデータが1回ずつテストに使われるため、より信頼性の高い評価ができます。"
  ],
  "originalSlideText": "Evaluation\nKreuzvalidierung\n✔ teilt die Daten in mehrere Partitionen auf",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 40,
  "questionDe": "(s40) Welche Aussage über Testdaten ist korrekt?",
  "questionJa": "テストデータについて正しい説明はどれか？",
  "answerDe": [
    "dürfen nicht für das Training benutzt werden"
  ],
  "answerJa": [
    "テストデータは訓練に使ってはいけない。"
  ],
  "explanationDe": [
    "Testdaten dienen der objektiven Beurteilung der Modellleistung.",
    "Sie dürfen während des Trainingsprozesses nicht verwendet werden, um eine Verzerrung zu vermeiden.",
    "Nur so lässt sich sicherstellen, dass das Modell generalisiert und nicht einfach die Trainingsdaten auswendig gelernt hat."
  ],
  "explanationJa": [
    "テストデータは、モデルの性能を客観的に評価するために使います。",
    "もし訓練中にテストデータを使ってしまうと、モデルがそのデータに最適化されてしまい、本来の汎化性能を正しく測れなくなります。",
    "そのため、テストデータはモデルの訓練後にのみ使用されるべきです。"
  ],
  "originalSlideText": "Evaluation\nTestdaten\n✔ dürfen nicht für das Training benutzt werden",
  "explanationImage": "",
  "questionImage": ""
}
,
{
  "id": 42,
  "questionDe": "(s42) Wozu dienen Autoencoder in neuronalen Netzen?",
  "questionJa": "ニューラルネットワークにおいて、オートエンコーダは何のために用いられるか？",
  "answerDe": [
    "sind Netze zum Extrahieren von wichtigen Features aus den Eingabedaten"
  ],
  "answerJa": [
    "入力データから重要な特徴（特徴量）を抽出するためのネットワークである。"
  ],
  "explanationDe": [
    "Ein Autoencoder ist ein neuronales Netz, das lernt, Eingabedaten zu komprimieren (Encoding) und anschließend zu rekonstruieren (Decoding).",
    "Dabei wird eine niedrigdimensionale Repräsentation der Daten gelernt, die wichtige Merkmale beibehält.",
    "Das Netz eignet sich gut zur Merkmalsextraktion, Dimensionsreduktion oder Anomalieerkennung."
  ],
  "explanationJa": [
    "オートエンコーダは、入力データをいったん圧縮（エンコード）し、再び復元（デコード）することを学習するニューラルネットワークです。",
    "この過程で、元のデータの中から最も重要な特徴を抽出し、低次元の表現として表します。",
    "特徴量抽出や次元削減、異常検知などに広く用いられます。"
  ],
  "originalSlideText": "Neuronale Netze\nAutoencoder\n✔ sind Netze zum Extrahieren von wichtigen Features aus den Eingabedaten",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 43,
  "questionDe": "(s43) Was bedeutet der Schwellenwert bei der Delta-Regel in neuronalen Netzen?",
  "questionJa": "ニューラルネットにおけるDeltaルールでは、しきい値（Schwellenwert）はどのように扱われるか？",
  "answerDe": [
    "äquivalent zu einem Gewicht eines Inputs, welches immer aktiv ist"
  ],
  "answerJa": [
    "常に活性な入力に対する重みとして扱うことができる。"
  ],
  "explanationDe": [
    "Bei der Delta-Regel kann der Schwellenwert als ein Bias betrachtet werden.",
    "Technisch gesehen entspricht er einem zusätzlichen Input mit konstantem Wert 1, dessen Gewicht (Bias) trainiert wird.",
    "Damit kann der Schwellenwert direkt ins Training integriert werden – anstelle einer festen Schwelle."
  ],
  "explanationJa": [
    "Deltaルールにおけるしきい値（Schwellenwert）は、バイアス項（常に1の入力に対する重み）として表現できます。",
    "これはニューラルネットにおける一般的な手法で、学習可能なパラメータとして扱われます。",
    "つまり、しきい値を直接扱うのではなく、バイアスとして組み込んで学習するのが一般的です。"
  ],
  "originalSlideText": "Neuronale Netze\nBei der Delta Regel ist der Schwellenwert\n✔ äquivalent zu einem Gewicht eines Inputs, welches immer aktiv ist",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 44,
  "questionDe": "(s44) Wie verändert sich der Einfluss eines Fehlers bei einer Lernrate < 1 in einem tiefen Netzwerk?",
  "questionJa": "深いニューラルネットにおいて、学習率が1未満のとき誤差の影響はどのように変化するか？",
  "answerDe": [
    "nimmt von hinten nach vorne in den Layern ab"
  ],
  "answerJa": [
    "誤差の影響は、後ろの層から前の層に向かうにつれて小さくなっていく。"
  ],
  "explanationDe": [
    "In tiefen neuronalen Netzen nimmt der Fehlergradient beim Backpropagation-Verfahren häufig mit zunehmender Tiefe ab.",
    "Das nennt man 'Vanishing Gradient'-Problem – besonders bei kleinen Lernraten und ungeeigneten Aktivierungsfunktionen.",
    "Das bedeutet: Frühere Schichten lernen langsamer, da sie weniger 'Signal' vom Fehler erhalten."
  ],
  "explanationJa": [
    "深層ニューラルネットワークでは、誤差逆伝播法において、前の層にいくほど誤差の影響が減衰してしまうことがあります。",
    "これは『勾配消失問題』と呼ばれ、特に学習率が小さい場合や活性化関数が適していない場合に顕著です。",
    "その結果、入力層に近い層の学習が進みにくくなることがあります。"
  ],
  "originalSlideText": "Neuronale Netze\nDer Einfluss eines Fehlers bei einer Lernrate < 1\n✔ nimmt von hinten nach vorne in den Layern ab",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 45,
  "questionDe": "(s45) Wann wird die Hierarchie in künstlichen neuronalen Netzen festgelegt?",
  "questionJa": "人工ニューラルネットワークにおける階層構造はいつ決定されるか？",
  "answerDe": [
    "vor dem Training festgelegt"
  ],
  "answerJa": [
    "学習の前に設計される。"
  ],
  "explanationDe": [
    "Die Hierarchie eines neuronalen Netzes beschreibt die Anordnung und Anzahl der Schichten und Neuronen.",
    "Diese Struktur muss vor dem Trainingsprozess definiert werden, z. B. wie viele versteckte Schichten es gibt oder wie viele Neuronen pro Schicht verwendet werden.",
    "Während des Trainings werden lediglich die Gewichte angepasst – nicht die Architektur."
  ],
  "explanationJa": [
    "ニューラルネットワークの階層構造とは、層の数や各層のニューロン数などのモデル構造を指します。",
    "この構造は、学習が始まる前に設計されており、訓練中に変化することは基本的にありません。",
    "訓練によって変化するのは重み（パラメータ）のみであり、構造は固定されています。"
  ],
  "originalSlideText": "Neuronale Netze\nDie Hierarchie in künstlichen neuronalen Netzen\n✔ vor dem Training festgelegt",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 46,
  "questionDe": "(s46) Was bestimmt die Schwellenwertfunktion in neuronalen Netzen?",
  "questionJa": "ニューラルネットにおいて、しきい値関数は何を定義するか？",
  "answerDe": [
    "die Art der Berechnung in der Aktivierungsfunktion"
  ],
  "answerJa": [
    "活性化関数における計算の仕方を決定する。"
  ],
  "explanationDe": [
    "Die Schwellenwertfunktion (auch Aktivierungsfunktion genannt) entscheidet, ob ein Neuron 'aktiviert' wird.",
    "Sie wird auf die gewichtete Summe der Eingaben angewendet und bestimmt die Ausgabe des Neurons.",
    "Typische Schwellenwertfunktionen sind Sigmoid, ReLU oder tanh."
  ],
  "explanationJa": [
    "しきい値関数（活性化関数）は、あるニューロンが活性化されるかどうかを決定する関数です。",
    "入力値の重み付き和に対して適用され、その出力を変換します。",
    "例：シグモイド関数、ReLU、tanhなどが代表的な活性化関数です。"
  ],
  "originalSlideText": "Neuronale Netze\nDie Schwellenwertfunktion bestimmt\n✔ die Art der Berechnung in der Aktivierungsfunktion",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 47,
  "questionDe": "(s47) Warum können einfache Perceptrons keine nicht linear trennbaren Daten lernen?",
  "questionJa": "単純パーセプトロンは、なぜ非線形分離可能なデータを学習できないのか？",
  "answerDe": [
    "nicht lineare Daten zu rechenaufwendig wären"
  ],
  "answerJa": [
    "非線形なデータは計算量が多すぎるため。"
  ],
  "explanationDe": [
    "Die Aussage ist falsch.",
    "Einfaches Perceptron kann keine nicht-linear trennbaren Daten lernen, weil es nur lineare Entscheidungsgrenzen berechnet.",
    "Das hat nichts mit Rechenaufwand zu tun, sondern mit der begrenzten Modellkapazität.",
    "Nicht-lineare Daten erfordern Netzwerke mit mehreren Schichten oder nicht-linearen Aktivierungsfunktionen."
  ],
  "explanationJa": [
    "この選択肢は誤りです。",
    "単純パーセプトロンが非線形分離可能なデータを学習できないのは、『計算量の多さ』が理由ではなく、モデル自体が線形な決定境界しか表現できないためです。",
    "この制限を克服するには、多層パーセプトロンや非線形の活性化関数が必要です。"
  ],
  "originalSlideText": "Neuronale Netze\nEinfache Perceptrons können nicht lineare trennbare Daten nicht lernen,\n✔ nicht lineare Daten zu rechenaufwendig wären",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 48,
  "questionDe": "(s48) Wie werden Informationen in künstlichen neuronalen Netzen gespeichert?",
  "questionJa": "人工ニューラルネットワークでは、情報はどのように保存されるか？",
  "answerDe": [
    "werden Informationen verteilt gespeichert"
  ],
  "answerJa": [
    "情報はネットワーク全体に分散して保存される。"
  ],
  "explanationDe": [
    "In künstlichen neuronalen Netzen erfolgt die Informationsspeicherung nicht zentral, sondern verteilt über viele Gewichte.",
    "Das bedeutet: Einzelne Informationen sind nicht an ein bestimmtes Neuron gebunden, sondern durch das Zusammenspiel vieler Parameter kodiert.",
    "Diese verteilte Repräsentation macht neuronale Netze robust gegenüber kleinen Veränderungen."
  ],
  "explanationJa": [
    "人工ニューラルネットでは、情報は特定の1か所に集中的に記憶されるのではなく、ネットワーク全体に分散して保存されます。",
    "つまり、1つのニューロンが特定の情報を完全に担当するのではなく、複数の重みの組み合わせによって表現されています。",
    "このような分散表現は、多少の誤差やノイズに対して頑健性を持たせることができます。"
  ],
  "originalSlideText": "Neuronale Netze\nIn künstlichen neuronalen Netzen\n✔ werden Informationen verteilt gespeichert",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 49,
  "questionDe": "(s49) Was ist die Funktion der Konvolution in neuronalen Netzen?",
  "questionJa": "ニューラルネットワークにおける畳み込み（Konvolution）の役割は何か？",
  "answerDe": [
    "ist ein Filterverfahren, um bestimmte Eigenschaften aus den Eingabedaten zu extrahieren"
  ],
  "answerJa": [
    "入力データから特定の特徴を抽出するためのフィルタ処理である。"
  ],
  "explanationDe": [
    "In Convolutional Neural Networks (CNNs) wird die Konvolution verwendet, um lokale Muster wie Kanten oder Formen zu erkennen.",
    "Ein Filter (Kernel) wird dabei über das Eingabebild geschoben, wobei an jeder Position eine gewichtete Summe gebildet wird.",
    "So lassen sich bestimmte Merkmale effizient und lageinvariant extrahieren."
  ],
  "explanationJa": [
    "CNN（畳み込みニューラルネットワーク）では、畳み込みは画像やデータから特徴を抽出する処理です。",
    "カーネルと呼ばれる小さなフィルタをデータ上でスライドさせながら局所的な情報を検出します。",
    "これにより、エッジや模様などの重要な特徴を捉えることができ、位置のずれにも比較的強くなります。"
  ],
  "originalSlideText": "Neuronale Netze\nKonvolution\n✔ ist ein Filterverfahren, um bestimmte Eigenschaften aus den Eingabedaten zu extrahieren",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 50,
  "questionDe": "(s50) Warum können neuronale Netze mit Hidden Layer auch nicht lineare Funktionen approximieren?",
  "questionJa": "なぜ隠れ層を持つニューラルネットワークは非線形な関数も近似できるのか？",
  "answerDe": [
    "Weil jedes Neuron eine Hyperebene berechnet und diese sich dann ergänzen"
  ],
  "answerJa": [
    "各ニューロンが線形な超平面を計算し、それらを組み合わせることで非線形な境界が作られるから。"
  ],
  "explanationDe": [
    "Jedes Neuron im Hidden Layer berechnet eine lineare Funktion.",
    "Durch Kombination mehrerer solcher Neuronen mit Aktivierungsfunktionen entsteht eine nichtlineare Abbildung.",
    "Diese Zusammensetzung ermöglicht es neuronalen Netzen, sehr komplexe, nichtlineare Funktionen zu approximieren."
  ],
  "explanationJa": [
    "隠れ層にある各ニューロンは、基本的に線形な計算（重み付き和）を行います。",
    "しかし、それらの出力に活性化関数（非線形関数）を組み合わせることで、全体として非線形なマッピングが実現されます。",
    "その結果、非線形な関数や境界も表現・近似できるようになります。"
  ],
  "originalSlideText": "Neuronale Netze\nWarum können neuronale Netze mit hidden Layer auch nicht lineare Funktionen approximieren?\n✔ Weil jedes Neuron eine Hyperebene berechnet und diese sich dann ergänzen",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 51,
  "questionDe": "(s51) Warum werden die Gewichte in neuronalen Netzen mittels Backpropagation trainiert?",
  "questionJa": "なぜニューラルネットワークでは、重みの学習にBackpropagation（誤差逆伝播法）が用いられるのか？",
  "answerDe": [
    "Weil die Korrektur der Gewichte nur in Rückwärtsrichtung möglich ist."
  ],
  "answerJa": [
    "重みの修正は逆方向（出力層から入力層へ）でしか行えないため。"
  ],
  "explanationDe": [
    "Backpropagation ist ein Algorithmus zur Anpassung der Gewichte in einem neuronalen Netz.",
    "Dabei wird der Fehler vom Ausgabeneuron schrittweise rückwärts durch das Netz propagiert, um den Beitrag jeder Verbindung zum Gesamtfehler zu bestimmen.",
    "Nur auf diesem Weg kann der Fehler korrekt einzelnen Gewichten zugeordnet und diese effizient angepasst werden."
  ],
  "explanationJa": [
    "誤差逆伝播法（Backpropagation）は、ニューラルネットワークの重みを学習するための基本的なアルゴリズムです。",
    "出力層で計算された誤差を入力層方向へと逆に伝播させることで、どの重みがどれだけ誤差に影響したかを計算します。",
    "この逆方向の処理を通じて、各重みを効率的に調整することが可能になります。"
  ],
  "originalSlideText": "Neuronale Netze\nWarum werden die Gewichte in neuronalen Netzen mittels Backpropagation trainiert?\n✔ Weil die Korrektur der Gewichte nur in Rückwärtsrichtung möglich ist.",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 52,
  "questionDe": "(s52) Was passiert mit dem Abstand der Hyperebene zu den Daten bei der SVM?",
  "questionJa": "SVMにおいて、ハイパープレーン（分類平面）とデータの距離はどうなるか？",
  "answerDe": [
    "maximiert"
  ],
  "answerJa": [
    "最大化される。"
  ],
  "explanationDe": [
    "Support Vector Machines (SVM) suchen nach der optimalen Trennlinie, die den Abstand (Margin) zu den nächsten Datenpunkten beider Klassen maximiert.",
    "Ein größerer Abstand bedeutet eine robustere Trennung und bessere Generalisierungsfähigkeit des Modells.",
    "Diese Eigenschaft macht die SVM besonders effektiv bei klar trennbaren Klassen."
  ],
  "explanationJa": [
    "SVM（サポートベクターマシン）は、2つのクラスを分離する境界線（ハイパープレーン）を学習する手法です。",
    "このとき、ハイパープレーンと最も近いデータ点（サポートベクター）との距離＝マージンを最大化するように設計されています。",
    "マージンが広いほど分類の信頼性が高まり、未知のデータにも強くなります。"
  ],
  "originalSlideText": "SVM\nDer Abstand der Hyperebene zu den Daten wird\n✔ maximiert",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 53,
  "questionDe": "(s53) Worauf basiert der Kerneltrick in SVMs?",
  "questionJa": "SVMにおけるカーネルトリックは何に基づいているか？",
  "answerDe": [
    "dem Berechnen des Skalarproduktes im Ursprungsdatenraum"
  ],
  "answerJa": [
    "元の入力空間における内積（スカラー積）の計算に基づいている。"
  ],
  "explanationDe": [
    "Der Kerneltrick ermöglicht es, Daten implizit in einen höherdimensionalen Raum zu transformieren, ohne die Transformation explizit durchzuführen.",
    "Stattdessen wird das Skalarprodukt der transformierten Daten durch eine Kernel-Funktion im Ursprungsraum berechnet.",
    "Dadurch kann die SVM auch nichtlineare Entscheidungsgrenzen finden."
  ],
  "explanationJa": [
    "カーネルトリックとは、データを高次元空間に写像せずに、高次元空間での内積を直接計算する技術です。",
    "このとき、変換後のデータ同士の内積を、元の空間のデータだけを用いた関数（カーネル関数）で計算します。",
    "これにより、非線形な分類も効率よく扱えるようになります。"
  ],
  "originalSlideText": "SVM\nDer Kerneltrick in SVMs basiert auf\n✔ dem Berechnen des Skalarproduktes im Ursprungsdatenraum",
  "explanationImage": "",
  "questionImage": ""
},
{
  "id": 54,
  "questionDe": "(s54) Wie werden nicht lineare Daten in der SVM klassifiziert?",
  "questionJa": "SVMでは、非線形なデータはどのように分類されるか？",
  "answerDe": [
    "indem der Datenraum durch eine nichtlineare Transformation modifiziert wird"
  ],
  "answerJa": [
    "データ空間を非線形な変換によって修正することで分類される。"
  ],
  "explanationDe": [
    "SVMs nutzen eine Transformation, um nichtlineare Daten in einen Raum zu überführen, in dem sie linear trennbar sind.",
    "Dies geschieht z. B. durch eine Kernel-Funktion, die Daten implizit in einen höherdimensionalen Raum projiziert.",
    "Dort kann dann eine lineare Trennung stattfinden."
  ],
  "explanationJa": [
    "SVMでは、非線形なデータを扱うために、元の特徴空間を非線形に変換します。",
    "この変換により、もともと線形に分離できなかったデータも、新しい空間では直線（ハイパープレーン）で分類できるようになります。",
    "このとき使われる変換には、カーネル関数が用いられます。"
  ],
  "originalSlideText": "SVM\nNicht lineare Daten werden in der SVM klassifiziert\n✔ indem der Datenraum durch eine nichtlineare Transformation modifiziert wird",
  "explanationImage": "",
  "questionImage": ""
},

{
  "id": 54,
  "questionDe": "(s54) Wie werden nicht lineare Daten in der SVM klassifiziert?",
  "questionJa": "SVMでは、非線形なデータはどのように分類されるか？",
  "answerDe": [
    "indem der Datenraum durch eine nichtlineare Transformation modifiziert wird"
  ],
  "answerJa": [
    "データ空間を非線形な変換によって修正することで分類される。"
  ],
  "explanationDe": [
    "SVMs nutzen eine Transformation, um nichtlineare Daten in einen Raum zu überführen, in dem sie linear trennbar sind.",
    "Dies geschieht z. B. durch eine Kernel-Funktion, die Daten implizit in einen höherdimensionalen Raum projiziert.",
    "Dort kann dann eine lineare Trennung stattfinden."
  ],
  "explanationJa": [
    "SVMでは、非線形なデータを扱うために、元の特徴空間を非線形に変換します。",
    "この変換により、もともと線形に分離できなかったデータも、新しい空間では直線（ハイパープレーン）で分類できるようになります。",
    "このとき使われる変換には、カーネル関数が用いられます。"
  ],
  "originalSlideText": "SVM\nNicht lineare Daten werden in der SVM klassifiziert\n✔ indem der Datenraum durch eine nichtlineare Transformation modifiziert wird",
  "explanationImage": "",
  "questionImage": ""
},

{
  "id": 55,
  "questionDe": "(s55) Welche Datenpunkte identifiziert eine SVM als Stützvektoren?",
  "questionJa": "SVMはどのようなデータ点をサポートベクター（Stützvektoren）として特定するか？",
  "answerDe": [
    "Punkte aus den Gruppen, die möglichst nah an der anderen Gruppe liegen"
  ],
  "answerJa": [
    "別のクラスのデータに最も近い位置にある各グループのデータ点。"
  ],
  "explanationDe": [
    "Stützvektoren sind die Datenpunkte, die am nächsten an der Trennhyperbene liegen.",
    "Sie bestimmen die Lage der optimalen Trennlinie (bzw. -fläche) und sind entscheidend für das Modell.",
    "Nur diese Punkte beeinflussen das endgültige Modell – alle anderen haben keinen direkten Einfluss."
  ],
  "explanationJa": [
    "サポートベクター（Stützvektoren）は、分類境界（ハイパープレーン）に最も近いデータ点です。",
    "これらの点が境界線の位置を決定し、モデルの学習に直接的な影響を与えます。",
    "他の点は境界から遠いため、分類結果に影響を与えません。"
  ],
  "originalSlideText": "SVM\nSupport Vector Machines identifizieren als Stützvektoren\n✔ Punkte aus den Gruppen, die möglichst nah an der anderen Gruppe liegen",
  "explanationImage": "",
  "questionImage": ""
}







































]