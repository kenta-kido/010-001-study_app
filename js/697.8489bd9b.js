"use strict";(self["webpackChunkstudy_site"]=self["webpackChunkstudy_site"]||[]).push([[697],{495:function(e,n,i){i.d(n,{A:function(){return E}});var t=i(6768),r=i(4232),a=i(144);const s={class:"card mb-4 shadow-sm"},l={class:"card-body"},u={class:"card-title"},o={class:"text-muted fst-italic"},d={key:0},h=["src"],g={key:1,class:"mt-3"},m={class:"alert alert-success"},c={key:0},p={key:1},k={class:"alert alert-info mt-2"},b={key:0},D={key:1},w={class:"mt-3"},A={key:0},f={key:1},z={key:2},v={key:3},y={key:4},S=["src"],x={class:"mt-4"},M={class:"border rounded p-3 bg-white text-secondary",style:{"white-space":"pre-wrap","font-family":"inherit"}};var V={__name:"QuestionItem",props:{question:Object},setup(e){const n=(0,a.KR)(!1);return(i,a)=>((0,t.uX)(),(0,t.CE)("div",s,[(0,t.Lk)("div",l,[(0,t.Lk)("h5",u,"Q"+(0,r.v_)(e.question.id)+": "+(0,r.v_)(e.question.questionJa),1),(0,t.Lk)("p",o,"("+(0,r.v_)(e.question.questionDe)+")",1),e.question.questionImage?((0,t.uX)(),(0,t.CE)("div",d,[(0,t.Lk)("img",{src:`./images/${e.question.questionImage}`,class:"img-fluid rounded my-2 border",alt:"question image"},null,8,h)])):(0,t.Q3)("",!0),(0,t.Lk)("button",{class:"btn btn-outline-primary mt-2",onClick:a[0]||(a[0]=e=>n.value=!n.value)},(0,r.v_)(n.value?"Hide Answer":"Check Answer"),1),n.value?((0,t.uX)(),(0,t.CE)("div",g,[(0,t.Lk)("div",m,[a[1]||(a[1]=(0,t.Lk)("strong",null,"Antwort (De):",-1)),a[2]||(a[2]=(0,t.Lk)("br",null,null,-1)),Array.isArray(e.question.answerDe)?((0,t.uX)(),(0,t.CE)("ul",c,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.answerDe,(e,n)=>((0,t.uX)(),(0,t.CE)("li",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",p,(0,r.v_)(e.question.answerDe),1))]),(0,t.Lk)("div",k,[a[3]||(a[3]=(0,t.Lk)("strong",null,"Übersetzung (Ja):",-1)),a[4]||(a[4]=(0,t.Lk)("br",null,null,-1)),Array.isArray(e.question.answerJa)?((0,t.uX)(),(0,t.CE)("ul",b,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.answerJa,(e,n)=>((0,t.uX)(),(0,t.CE)("li",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",D,(0,r.v_)(e.question.answerJa),1))]),(0,t.Lk)("div",w,[a[6]||(a[6]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"Erklärung (De):",-1)),Array.isArray(e.question.explanationDe)?((0,t.uX)(),(0,t.CE)("div",A,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.explanationDe,(e,n)=>((0,t.uX)(),(0,t.CE)("p",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",f,(0,r.v_)(e.question.explanationDe),1)),a[7]||(a[7]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"解説 (Ja):",-1)),Array.isArray(e.question.explanationJa)?((0,t.uX)(),(0,t.CE)("div",z,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.explanationJa,(e,n)=>((0,t.uX)(),(0,t.CE)("p",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",v,(0,r.v_)(e.question.explanationJa),1)),e.question.explanationImage?((0,t.uX)(),(0,t.CE)("div",y,[(0,t.Lk)("img",{src:`./images/${e.question.explanationImage}`,class:"img-fluid rounded my-2 border",alt:"explanation image"},null,8,S)])):(0,t.Q3)("",!0),(0,t.Lk)("div",x,[a[5]||(a[5]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"原文（スライド抜粋）:",-1)),(0,t.Lk)("div",M,(0,r.v_)(e.question.originalSlideText),1)])])])):(0,t.Q3)("",!0)])]))}};const q=V;var E=q},7697:function(e,n,i){i.r(n),i.d(n,{default:function(){return b}});i(8111),i(116);var t=i(6768),r=i(4232),a=i(144),s=i(1387),l=i(495),u=i(3529),o=JSON.parse('[{"id":1,"questionDe":"Was umfasst das Thema „Dimensionsreduktion“ in dieser Vorlesung?","questionJa":"この講義における「次元削減」はどのような内容を含んでいますか？","answerDe":["Einleitung zum Thema","Faktorenanalyse","Hauptkomponentenanalyse (PCA)","Multi-Dimensional Scaling (MDS)","t-distributed Stochastic Neighbor Embedding (t-SNE)","Uniform Manifold Approximation and Projection (UMAP)"],"answerJa":["次元削減の導入（概要）","因子分析（Factor Analysis）","主成分分析（PCA）","多次元尺度構成法（MDS）","t-SNE（t分布型確率的近傍埋め込み）","UMAP（一様多様体近似と射影）"],"explanationDe":["Das Thema „Dimensionsreduktion“ behandelt Methoden, um hochdimensionale Daten in eine niedrigere Dimension zu überführen – oft zur Visualisierung oder zur Vorverarbeitung für maschinelles Lernen.","In der Vorlesung werden sowohl klassische lineare Verfahren (z. B. PCA, Factor Analysis) als auch moderne, nichtlineare Techniken (z. B. t-SNE, UMAP) vorgestellt.","Diese Methoden helfen, die Struktur der Daten besser zu erkennen, Überfitting zu vermeiden oder Rechenaufwand zu reduzieren.","Beispielsweise erlaubt PCA eine Interpretation der Hauptachsen der Varianz, während t-SNE oder UMAP eher die lokalen Nachbarschaften im Datenraum erhalten."],"explanationJa":["「次元削減」とは、高次元データをより少ない次元に変換する手法の総称で、データの可視化や機械学習の前処理によく使われます。","この講義では、線形な古典的手法（例：PCAや因子分析）と、非線形で近年注目されている手法（例：t-SNEやUMAP）の両方が取り上げられます。","次元削減は、データの本質的な構造を理解しやすくしたり、過学習を防いだり、計算量を削減したりするのに役立ちます。","例えば、PCAは分散の大きい方向を基に軸を定めるのに対し、t-SNEやUMAPはデータの局所的な関係性（近さ）を保ちつつ、2次元や3次元に落とし込むのが得意です。"],"originalSlideText":"Dimensionsreduktion\\n- Einleitung\\n- Factor Analysis\\n- Principal Component Analysis (PCA)\\n- Multi-Dimensional Scaling (MDS)\\n- T-distributed stochastic neighborhood embedding (t-SNE)\\n- Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)","explanationImage":"","questionImage":""},{"id":2,"questionDe":"(s.3) Welches Problem entsteht bei vielen Attributen und Datenpunkten?","questionJa":"多数の属性やデータ点があると、どのような問題が生じますか？","answerDe":["Bei einer Tabelle mit vielen Datenpunkten und Attributen entsteht das Problem, dass mehr Daten vorhanden sind als Darstellungsfläche."],"answerJa":["多数のデータ点や属性を持つ表では、表示できる領域よりもデータ量のほうが多くなってしまうという問題が発生します。"],"explanationDe":["In datengetriebenen Anwendungen, etwa bei Sensor- oder Bilddaten, entstehen häufig sehr große Tabellen mit vielen Spalten (Attributen) und Zeilen (Datenpunkten).","Ein zentrales Problem besteht darin, dass die vorhandene Darstellungsfläche – zum Beispiel ein Bildschirm oder Diagramm – nicht ausreicht, um die gesamte Datenstruktur verständlich darzustellen.","Dies erschwert die Analyse, insbesondere die visuelle Interpretation oder die Anwendung von Algorithmen, die in niedriger Dimensionalität effektiver arbeiten."],"explanationJa":["センサーや画像データなどの実用的なデータでは、多くの列（属性）と行（データ点）からなる大規模な表が生成されます。","このとき、「画面」や「図」といった表示領域が足りず、すべてのデータ構造を視覚的に表現するのが難しくなります。","この問題により、特に可視化や、低次元で効果的に働くアルゴリズムの適用が困難になります。"],"originalSlideText":"– Ausgangssituation\\n  – Tabelle mit n Datenpunkten und m Attributen\\n– Problem\\n  – Mehr Daten als Darstellungsfläche","explanationImage":"","questionImage":"lecture01/lecture06_q01.png"},{"id":3,"questionDe":"(s.3) Welche Lösungsansätze gibt es zur Datenreduktion?","questionJa":"データ量の削減にはどのような解決法がありますか？","answerDe":["Zur Lösung kann man entweder eine Datenauswahl durch Einschränkung der Attribute oder Attributwerte treffen oder eine Projektion zur Dimensionsreduktion durchführen."],"answerJa":["この問題への対処法としては、属性や属性値の制限によるデータの選択、または次元削減を目的とした射影（プロジェクション）があります。"],"explanationDe":["Ein Ansatz zur Datenreduktion ist die Datenauswahl. Dabei werden entweder nur bestimmte Attribute berücksichtigt (z. B. jene mit hoher Relevanz) oder Attributwerte eingeschränkt (z. B. durch Filterung).","Ein anderer Ansatz ist die Projektion: Hierbei werden die Daten mathematisch auf einen niedrigeren dimensionalen Raum abgebildet, wobei wichtige Strukturen erhalten bleiben.","Die Dimensionsreduktion (engl. Dimensionality Reduction) ist ein Spezialfall dieser Projektion und wird häufig eingesetzt, um Daten visuell darzustellen oder Rechenaufwand zu reduzieren.","Ein Beispiel: Bei der PCA werden die Hauptachsen der größten Varianz genutzt, um die Dimension der Daten zu verringern."],"explanationJa":["データ量を削減する方法のひとつは「データ選択」で、重要な属性だけを選んだり、属性値の範囲を制限したりします（例：フィルター処理）。","もうひとつは「射影（プロジェクション）」で、データを数学的に低次元の空間に写し、重要な構造を保ちながら次元を減らします。","この射影の一種が「次元削減（Dimensionality Reduction）」であり、視覚化のためや、計算負荷の軽減などによく用いられます。","例としてPCA（主成分分析）では、最も分散の大きな軸に沿ってデータを写すことで、次元を縮小します。"],"originalSlideText":"– Lösung\\n  – Datenauswahl\\n    – Einschränkung der Attribute\\n    – Einschränkung der Attributwerte\\n  – Projektion / Datenreduktion\\n    – Dimensionsreduktion („Dimensionality Reduction“)\\n  – Clustering","explanationImage":"","questionImage":""},{"id":4,"questionDe":"(s.4) Was ist unter Projektion im Kontext der Datenvisualisierung zu verstehen und welche Probleme entstehen dabei?","questionJa":"データの可視化におけるプロジェクションとは何ですか？また、どのような問題がありますか？","answerDe":["Bei der Projektion wählt man typischerweise 2 oder 3 Attribute aus, um Daten visuell darzustellen.","Dies kann zu vielen gleichen Datenpunkten führen (Overplotting).","Es stellt sich die Frage, welche Attribute ausgewählt werden und ob die Verteilung der Werte berücksichtigt wird."],"answerJa":["プロジェクションでは、通常2つか3つの属性を選び、データを視覚的に表示します。","しかし、多くのデータ点が同じ位置に重なってしまう（オーバープロッティング）という問題が発生します。","また、どの属性を選ぶべきか、値の分布が考慮されているかなどの課題もあります。"],"explanationDe":["In der Datenvisualisierung beschränkt man sich oft auf 2 oder 3 Dimensionen, da man diese einfach auf einem Bildschirm oder Plot darstellen kann.","Dazu wählt man einige Attribute aus, z. B. Alter und Einkommen, und stellt die Datenpunkte in einem 2D-Scatterplot dar.","Ein Problem ist das sogenannte Overplotting: Viele Datenpunkte haben in den ausgewählten Dimensionen identische Werte und überdecken sich visuell.","Ein weiteres Problem ist die Attributwahl: Wenn man wichtige Merkmale nicht berücksichtigt, geht relevante Information verloren.","Außerdem werden bei dieser einfachen Auswahl keine Korrelationen oder Verteilungen der Daten beachtet – man nutzt also nur einen sehr begrenzten Ausschnitt der gesamten Datenstruktur."],"explanationJa":["データの可視化では、通常2次元または3次元で表示する必要があるため、いくつかの属性を選び出して描画します。","たとえば、「年齢」と「収入」などをx軸とy軸にして散布図を描くといった形です。","しかし、この方法では同じ属性値を持つデータ点が重なりやすく、視覚的に区別できなくなる『オーバープロッティング』が発生します。","また、どの属性を選ぶかによって表現される情報が大きく変わり、重要な特徴を見逃してしまう可能性もあります。","さらに、選ばれた属性間の相関や、データの分布が考慮されていないため、元のデータ構造の一部しか反映されません。"],"originalSlideText":"– Projektion\\n  – Wähle d ∈ {2,3} Attribute\\n  – Führt in der Regel zu vielen „gleichen“ Datenpunkten → Overplotting\\n  – Probleme\\n    – Welche Attribute werden betrachtet?\\n    – Berücksichtigt die Werte und die Verteilung der Werte der Datenpunkte nicht","explanationImage":"","questionImage":""},{"id":5,"questionDe":"(s.4) Was versteht man unter Dimensionsreduktion und welche Varianten gibt es?","questionJa":"次元削減とは何ですか？また、その代表的な種類にはどのようなものがありますか？","answerDe":["Bei der Dimensionsreduktion ersetzt man die ursprünglichen Attribute durch eine kleinere Menge neuer Attribute.","Oft gilt: d ≪ m und d ∈ {2, 3}.","Es gibt verschiedene Varianten: Projektion (D ⊆ A), Berechnung neuer Attribute (D ∩ A = ∅), oder eine Kombination."],"answerJa":["次元削減では、もとの属性をより少ない数の新しい属性に置き換えます。","通常、d ≪ m で、d は2や3など小さい値が多いです。","方法には、プロジェクション（D ⊆ A）、新しい属性の生成（D ∩ A = ∅）、およびその両者の組み合わせがあります。"],"explanationDe":["Dimensionsreduktion ist eine Methode, um die Anzahl der Merkmale (Attribute) eines Datensatzes zu verringern, ohne die wesentliche Struktur der Daten zu verlieren.","Anstatt einfach einige existierende Attribute zu wählen (wie bei Projektion), berechnet man oft neue Merkmale, die die ursprüngliche Information möglichst gut zusammenfassen.","Beispiel: Bei der Hauptkomponentenanalyse (PCA) werden neue Achsen (Hauptkomponenten) so gewählt, dass sie die Varianz der Daten maximieren.","Man unterscheidet zwischen drei Varianten:\\n1. Projektion: man wählt eine Teilmenge der Originalattribute (D ⊆ A).\\n2. Transformation: man bildet völlig neue Attribute, berechnet aus den alten (D ∩ A = ∅).\\n3. Kombination: man mischt beide Ansätze.","Diese Methoden helfen, Daten übersichtlicher darzustellen und Overfitting oder Rechenlast zu reduzieren."],"explanationJa":["次元削減は、データの重要な構造を保ちつつ、属性（特徴量）の数を減らす方法です。","単に元の属性の一部を選ぶのではなく、元の情報を集約して、新たな属性を計算することが多いです。","たとえば主成分分析（PCA）では、データの分散を最もよく表す軸（主成分）を見つけ、それを新しい特徴量として使います。","方法には次の3つがあります。\\n1. プロジェクション：元の属性の一部をそのまま使う（D ⊆ A）。\\n2. 変換：元の属性から全く新しい属性を作る（D ∩ A = ∅）。\\n3. 両者の組み合わせ。","このような方法は、データの可視化や処理の効率化、過学習の防止に役立ちます。"],"originalSlideText":"– Dimensionsreduktion\\n  – Ersetze gegebene Attribute A = {A₁, ..., Aₘ} durch eine Menge von weniger Attributen D = {D₁, ..., D_d}, d ≪ m\\n    – Meist: d ∈ {2,3}\\n  – Alternativen:\\n    – D ⊆ A: entspricht Projektion\\n    – D ∩ A = ∅\\n      – Die neuen Attribute werden aus den alten berechnet\\n    – Kombination aus beidem","explanationImage":"lecture01/lecture06_ex01.png","questionImage":""},{"id":6,"questionDe":"(s.5) Welche Verfahren zur Dimensionsreduktion werden vorgestellt?","questionJa":"どのような次元削減の手法が紹介されていますか？","answerDe":["Vorgestellt werden: Factor Analysis, Principal Component Analysis (PCA), Multi-Dimensional Scaling (MDS).","Weitere Möglichkeiten: Self-Organizing Maps, visuelle Metaphern zur Anordnung von Dimensionen."],"answerJa":["紹介された手法は以下の通りです：因子分析（Factor Analysis）、主成分分析（PCA）、多次元尺度構成法（MDS）","その他の方法として、自己組織化マップ（SOM）や視覚的なメタファーによる次元の配置などもあります。"],"explanationDe":["In der Dimensionsreduktion gibt es verschiedene Ansätze, um die Anzahl der Merkmale zu reduzieren, ohne zu viel Information zu verlieren.","Klassische Verfahren sind:\\n- **Factor Analysis**: Modelliert beobachtete Variablen als lineare Kombinationen latenter (nicht direkt beobachtbarer) Faktoren.\\n- **PCA** (Principal Component Analysis): Findet neue Achsen (Hauptkomponenten), die die meiste Varianz erklären.\\n- **MDS** (Multi-Dimensional Scaling): Platziert Objekte so im Raum, dass die Distanzen möglichst den gegebenen Unterschieden entsprechen.","Zusätzliche Optionen sind:\\n- **Self-Organizing Maps (SOMs)**: Neuronale Netze, die eine nichtlineare Projektion liefern.\\n- **Visuelle Metaphern**: Grafische Ansätze, die dem Nutzer helfen, die Struktur der Daten intuitiv zu erfassen."],"explanationJa":["次元削減にはさまざまなアプローチがあり、情報をなるべく失わずにデータの特徴量を減らすことを目的とします。","代表的な手法は次のとおりです：\\n- **因子分析（Factor Analysis）**：観測された変数を、観測できない「因子」の線形結合としてモデル化します。\\n- **主成分分析（PCA）**：データの分散を最も説明する新しい軸（主成分）を見つけ出します。\\n- **多次元尺度構成法（MDS）**：データ間の距離関係を保つように、低次元空間に配置します。","その他の方法には：\\n- **自己組織化マップ（SOM）**：ニューラルネットワークを用いた非線形なマッピング手法です。\\n- **視覚的メタファー**：データの構造を視覚的に理解しやすくするための図示的アプローチです。"],"originalSlideText":"– Verfahren:\\n  – Factor Analysis\\n  – Principal Component Analysis (PCA)\\n  – Multi-Dimensional Scaling (MDS)\\n– Andere Möglichkeiten\\n  – Self-Organizing Maps\\n  – Visuelle Metaphern, um die Dimensionen anzuordnen","explanationImage":"","questionImage":""},{"id":7,"questionDe":"(s.5) Welche allgemeinen Hinweise gelten für alle Verfahren der Dimensionsreduktion?","questionJa":"次元削減のすべての手法に共通する注意点は何ですか？","answerDe":["Eine Analyse kann keine Informationen hinzufügen, die nicht vorhanden sind.","Es gilt das Prinzip: „Garbage in, garbage out“.","Faktoren werden immer gefunden – daher ist Domänenwissen zur Bewertung notwendig."],"answerJa":["分析では、元々存在しない情報を新たに加えることはできません。","『Garbage in, garbage out（ゴミを入れればゴミが出る）』という原則が当てはまります。","手法は常に何らかの『因子』を見つけますが、それが有効かどうかはドメイン知識で判断する必要があります。"],"explanationDe":["Ein wichtiger Hinweis bei allen Verfahren zur Dimensionsreduktion: Sie arbeiten nur mit den vorliegenden Daten.","Das bedeutet: Wenn z. B. ein wichtiges Merkmal nicht erhoben wurde, kann es auch durch keine mathematische Methode ersetzt oder ergänzt werden.","Der Spruch „Garbage in, garbage out“ warnt davor, dass schlechte oder irrelevante Daten zwangsläufig zu schlechten Ergebnissen führen.","Zudem identifizieren Verfahren wie PCA oder Factor Analysis stets bestimmte Strukturen oder Faktoren – selbst wenn diese möglicherweise keinen echten Informationsgehalt besitzen.","Deshalb ist es entscheidend, zusätzliches Wissen über den Anwendungsbereich (Domänenwissen) zu haben, um zu beurteilen, ob die gefundenen Muster sinnvoll sind."],"explanationJa":["次元削減を含むすべての分析手法は、与えられたデータの範囲内でしか処理を行えません。","つまり、もし重要な情報（例えばある属性）がそもそも収集されていなければ、それを後からどんな分析を使っても補うことはできません。","このような状況を表す言葉が「Garbage in, garbage out（ゴミを入れればゴミが出る）」です。","また、PCAや因子分析などの手法は、必ず何らかの構造や因子を抽出しますが、それが本当に意味あるものかどうかは自動的には判断できません。","したがって、分析結果の妥当性を判断するためには、データの背景知識や専門的な知見（ドメイン知識）が不可欠です。"],"originalSlideText":"– Achtung!\\n– Für alle Verfahren gilt:\\n  – Information, die nicht erhoben wurde, wird durch eine Analyse nicht hinzugefügt\\n  – Beware of „garbage in, garbage out“\\n    Vorsicht vor „Müll hinein, Müll heraus“:\\n    – Das Verfahren findet immer Faktoren\\n    – Hintergrund-/Domänen-Wissen ist notwendig, um zu entscheiden, ob die Faktoren einen Wert haben","explanationImage":"","questionImage":""},{"id":7,"questionDe":"(s.5) Nennen Sie vier wichtige Hinweise („Achtung!“), die für alle Verfahren der Dimensionsreduktion gelten.","questionJa":"次元削減のすべての手法に共通する重要な注意点を4つ挙げなさい。","answerDe":["1. Eine Analyse kann keine Informationen hinzufügen, die nicht erhoben wurden.","2. Das Prinzip „Garbage in, garbage out“ gilt: Schlechte Daten führen zu schlechten Ergebnissen.","3. Jedes Verfahren liefert mathematisch immer Faktoren, auch wenn diese bedeutungslos sein können.","4. Um die gefundenen Faktoren sinnvoll zu interpretieren, ist Domänenwissen erforderlich."],"answerJa":["1. 収集されていない情報は、分析であとから追加することはできない。","2. 「Garbage in, garbage out（ゴミを入れればゴミが出る）」の原則が当てはまる：質の低いデータでは、分析結果も信用できない。","3. 数理的な処理として、分析手法は必ず因子（パターン）を見つけ出すが、それが意味のあるものとは限らない。","4. 見つかった因子を正しく解釈するには、対象分野の専門知識（ドメイン知識）が必要である。"],"explanationDe":["**1. Keine Informationen hinzufügbar:** Wenn bestimmte Merkmale (wie Alter oder Einkommen) gar nicht erhoben wurden, können sie durch Analyseverfahren auch nicht ersetzt oder erschlossen werden.","**2. Garbage in, garbage out:** Wenn z. B. fehlerhafte, unvollständige oder unskalierte Daten verwendet werden, wird das Ergebnis entsprechend unzuverlässig. Beispiel: Bei uneinheitlichen Maßeinheiten (cm vs. m) liefert PCA irreführende Ergebnisse.","**3. Verfahren liefern immer Faktoren:** Algorithmen wie PCA oder Factor Analysis produzieren zwangsläufig mathematische Faktoren – auch dann, wenn keine sinnvolle Struktur in den Daten steckt. Dies kann zu falscher Interpretation führen, wenn man den Ergebnissen zu viel Bedeutung beimisst.","**4. Bedeutung durch Domänenwissen:** Nur mit Fachwissen lässt sich entscheiden, ob ein Faktor tatsächlich etwas wie \'Kundenzufriedenheit\' widerspiegelt oder nur ein statistisches Artefakt ist."],"explanationJa":["**1. 情報は後から足せない：** たとえば「年齢」や「収入」といった属性が最初からデータに含まれていない場合、分析手法でそれを補うことはできません。","**2. Garbage in, garbage out（ゴミを入れればゴミが出る）：** 測定ミスや不完全なデータ、単位が統一されていないデータを用いると、主成分分析なども意味のある結果を返しません。例：身長をcmとmで混在させたままPCAを行うと、主成分の方向が歪みます。","**3. 手法は必ず「因子」を出力する：** PCAや因子分析などの数理手法は、たとえデータに意味のある構造がなくても、必ず数学的に「因子」を作り出します。これが過信につながる危険があります。","**4. 専門知識が必要：** たとえば「第1主成分が顧客満足度に対応している」かどうかを判断するには、統計ではなく業務や対象領域の知識が不可欠です。そうでなければ、意味のない構造をあたかも意味があるように見なしてしまいます。"],"originalSlideText":"- Achtung!\\n- Für alle Verfahren gilt:\\n  – Information, die nicht erhoben wurde, wird durch eine Analyse nicht hinzugefügt\\n  – Beware of „garbage in, garbage out“\\n    Vorsicht vor „Müll hinein, Müll heraus“:\\n    – Das Verfahren findet immer Faktoren\\n    – Hintergrund-/Domänen-Wissen ist notwendig, um zu entscheiden, ob die Faktoren einen Wert haben","explanationImage":"","questionImage":""},{"id":8,"questionDe":"(s.6) Erklären Sie die zwei Typen der Faktorenanalyse und benennen Sie ihre jeweiligen Merkmale.","questionJa":"因子分析の2つのタイプ（Typ Q と Typ R）について説明し、それぞれの特徴を挙げなさい。","answerDe":["Typ Q fasst Datenpunkte zusammen, ist rechenaufwändig und wird oft durch Clustering ersetzt.","Typ R identifiziert verborgene Dimensionen, also Gruppen korrelierter Variablen."],"answerJa":["Typ Q はデータポイントをまとめる手法で、計算コストが高く、代わりにクラスタリングが使われることが多い。","Typ R は相関のある変数をまとめ、隠れた次元（因子）を見つける手法である。"],"explanationDe":["Die Faktorenanalyse wird in zwei Varianten unterteilt:","**Typ Q** betrachtet die Zeilen der Datenmatrix, also einzelne Datenpunkte. Ziel ist es, Gruppen von ähnlichen Individuen zu bilden. Da dies jedoch oft rechenintensiv ist, wird stattdessen häufig Clustering verwendet.","**Typ R** hingegen analysiert die Spalten der Datenmatrix, also die Variablen. Ziel ist es, sogenannte \'verborgene Dimensionen\' (Faktoren) zu finden, die mehrere korrelierte Variablen zusammenfassen. Zum Beispiel könnte ein \'sozialer Faktor\' die Variablen „Freundschaft“, „Geselligkeit“ und „Kommunikation“ zusammenfassen.","Typ R ist der klassische Anwendungsfall der Faktorenanalyse in der Psychologie, Soziologie und Marktforschung."],"explanationJa":["因子分析（Factor Analysis）には2つのタイプがあります：","**Typ Q** はデータ行（つまり個々のデータ点）を対象とし、似た個体（データポイント）をまとめようとします。たとえば、似たような購買行動をする顧客をグループ化するような場合です。ただし、計算量が大きいため、実務では代わりにクラスタリングが使われることが多いです。","**Typ R** は変数（列）を対象とし、相関のある変数をまとめて、より少ない“因子（潜在変数）”に置き換えます。たとえば、「友人の数」「SNSでのやりとり」「会話の頻度」などがすべて「社交性」という1つの因子で説明できる場合です。","Typ R は心理学やマーケティング調査などでよく使われる方法です。"],"originalSlideText":"FACTOR ANALYSIS\\n– Arten\\n  – Typ Q\\n    – Zusammenfassung der Datenpunkte\\n    – Aufwändige Berechnung\\n    – Meist wird stattdessen Clustering verwendet\\n  – Typ R\\n    – Finde „verborgene Dimensionen“ (Gruppen von korrelierten Variablen)\\n– Ziele:\\n  – Identifikation von Strukturen\\n  – Zusammenfassung der Daten\\n  – Reduktion der Daten\\n– Eigenschaften:\\n  – Gruppen sind disjunkt\\n  – Gruppen können von ihren Mitgliedern repräsentiert werden","explanationImage":"","questionImage":""},{"id":9,"questionDe":"(s.6) Nennen Sie drei typische Ziele der Faktorenanalyse.","questionJa":"因子分析の代表的な目的を3つ挙げなさい。","answerDe":["Identifikation von Strukturen","Zusammenfassung der Daten","Reduktion der Daten"],"answerJa":["構造（パターン）の識別","データの要約","データの次元削減"],"explanationDe":["Ein zentrales Ziel der Faktorenanalyse ist es, in den Daten zugrunde liegende Strukturen zu erkennen – etwa, ob bestimmte Variablen in Gruppen zusammengehören.","Sie dient außerdem dazu, große Datenmengen zu verdichten und durch wenige aussagekräftige Faktoren zu beschreiben.","Dies führt gleichzeitig zu einer Reduktion der Anzahl an Variablen und damit zu besserer Interpretierbarkeit und Effizienz bei der Analyse."],"explanationJa":["因子分析の主な目的は、観測された変数の背後にある構造（パターン）を明らかにすることです。たとえば、変数同士がどのように関係し、どのグループに属するかなどを探ります。","また、情報の要約手法としても使われ、多くの変数を少数の因子に集約することで、データの解釈が容易になります。","その結果、次元の削減が可能となり、計算の効率化や視覚的な理解の向上にもつながります。"],"originalSlideText":"– Ziele:\\n  – Identifikation von Strukturen\\n  – Zusammenfassung der Daten\\n  – Reduktion der Daten"},{"id":10,"questionDe":"(s.6) Welche zwei Eigenschaften zeichnen Gruppen in der Faktorenanalyse aus?","questionJa":"因子分析におけるグループの特徴を2つ挙げなさい。","answerDe":["Gruppen sind disjunkt.","Gruppen können von ihren Mitgliedern repräsentiert werden."],"answerJa":["グループは互いに重複しない（排他的）","グループはその構成要素によって代表される"],"explanationDe":["Disjunkt bedeutet, dass jede Variable oder jedes Objekt nur zu einer Gruppe gehört – es gibt keine Überschneidungen.","Zudem lassen sich die Gruppen durch typische Merkmalsausprägungen ihrer Mitglieder charakterisieren, was die Interpretation erleichtert."],"explanationJa":["『排他的（disjunkt）』とは、1つの変数やオブジェクトが複数のグループにまたがることがないことを意味します。","また、グループはその構成メンバー（変数やデータ点）の性質によって代表されるため、解釈しやすくなるという特徴もあります。"],"originalSlideText":"– Eigenschaften:\\n  – Gruppen sind disjunkt\\n  – Gruppen können von ihren Mitgliedern repräsentiert werden"},{"id":11,"questionDe":"(s.7) Vergleichen Sie anhand der dargestellten Grafik die Gruppierungsergebnisse von Typ Q Faktor-Analyse und Clustering.","questionJa":"図をもとに、Typ Q 因子分析とクラスタリングによるグループ分けの違いを比較しなさい。","answerDe":["Bei der Typ Q Faktor-Analyse gehören A und C sowie B und D zur gleichen Gruppe.","Beim Clustering gehören A und B sowie C und D zusammen."],"answerJa":["Typ Q 因子分析では、A と C、B と D がそれぞれ同じグループに属している。","一方、クラスタリングでは、A と B、C と D が同じグループに分類されている。"],"explanationDe":["Die Grafik zeigt vier Objekte (A–D) mit drei Variablen (V1–V3). Die Liniengrafik verdeutlicht die Ähnlichkeit der Wertverläufe.","Bei der Typ Q Faktor-Analyse werden Objekte gruppiert, die über alle Variablen hinweg ähnliche Verläufe zeigen. A und C folgen einem ähnlichen Muster, obwohl ihre absoluten Werte unterschiedlich sind – deshalb werden sie gruppiert. Ebenso verhalten sich B und D ähnlich in ihrer Kurvenform.","Beim Clustering hingegen werden Objekte eher nach absoluter Nähe im Merkmalsraum gruppiert. A und B haben ähnliche hohe Werte, daher werden sie zusammen gruppiert. C und D liegen beide in einem niedrigeren Wertebereich.","Die beiden Methoden erfassen also unterschiedliche Arten von Ähnlichkeit: Faktor-Analyse betrachtet Formmuster, Clustering absolute Nähe."],"explanationJa":["この図には、3つの変数 (V1〜V3) に対して4つのデータ（A〜D）の値が示されており、線グラフがそれぞれの傾向を視覚化しています。","Typ Q の因子分析では、全体の形（変数間の推移の傾向）に注目します。たとえば、AとCは全体の数値は異なるものの、増減のパターンが似ており、同じ因子とみなされます。同様に、BとDも同じ形状の傾向を持つため、一緒に分類されます。","一方クラスタリングは、値の絶対的な近さに基づいてグループ化を行います。AとBは3変数すべての値が高く、CとDは低いため、前者と後者でグループが分かれます。","つまり、因子分析（Typ Q）は『傾向の類似性』、クラスタリングは『数値の近さ』をもとにしているという違いがあります。"],"originalSlideText":"FACTOR ANALYSIS\\n– Typ Q Faktor-Analyse – Clustering\\n– Faktor-Analyse\\n  – Gruppe 1: A, C\\n  – Gruppe 2: B, D\\n– Clustering\\n  – Gruppe 1: A, B\\n  – Gruppe 2: C, D","explanationImage":"","questionImage":"lecture01/lecture06_q02.png"},{"id":12,"questionDe":"(s.8) Welche Voraussetzungen müssen für die Anwendung der Faktorenanalyse erfüllt sein? Nennen Sie mindestens 3.","questionJa":"因子分析を適用するために必要な前提条件を3つ以上挙げて説明しなさい。","answerDe":["Die Anzahl der Datenpunkte sollte deutlich größer sein als die Anzahl der Variablen.","Es sollten möglichst wenige Variablen verwendet werden.","Es sollten möglichst viele Datenpunkte verwendet werden.","Die Eignung der Daten kann mit Bartletts Test auf Sphärizität überprüft werden."],"answerJa":["データ点の数が変数の数よりも十分に多いこと。","使用する変数の数はできるだけ少なくすること。","できるだけ多くのデータ点を使用すること。","バートレットの球面性検定（Bartlett\'s Test）により、因子分析に適しているかどうかを確認すること。"],"explanationDe":["Für eine stabile Faktorenanalyse muss die Anzahl der Datenpunkte deutlich größer als die der Variablen sein – mindestens 50–100 Datenpunkte werden empfohlen.","In der Praxis wird oft die Regel verwendet: 5–10 Datenpunkte pro Variable oder sogar 20 Datenpunkte pro Variable.","Bartlett\'s Test auf Sphärizität prüft, ob die Korrelationsmatrix signifikant von der Einheitsmatrix abweicht.","Die Nullhypothese besagt, dass die Korrelationsmatrix gleich der Einheitsmatrix ist – d.h. keine Korrelationen zwischen den Variablen.","Wenn die Nullhypothese bei p < 0,05 abgelehnt wird, ist eine Faktorenanalyse sinnvoll, da es ausreichende Korrelationen gibt.","Voraussetzung für den Test ist multivariate Normalverteilung."],"explanationJa":["因子分析を正しく行うためには、いくつかの前提条件を満たす必要があります。","まず、データ点（サンプル）の数が変数の数よりも十分に多くなければなりません。一般には50〜100件のサンプル、または1変数あたり5〜10件（理想的には20件）のデータが必要とされます。","また、使用する変数の数はできるだけ絞るべきです。変数が多いと結果が不安定になる可能性があります。","バートレットの球面性検定（Bartlett\'s Test）は、変数間に有意な相関があるか（因子分析が妥当か）を判断するための統計的検定です。","この検定の帰無仮説は『相関行列が単位行列と同じである』というもので、p値が0.05未満なら帰無仮説は棄却され、因子分析を行う意義があるとされます。","この検定を使うには、多変量正規分布しているという前提も必要です。"],"originalSlideText":"FACTOR ANALYSIS\\n– Anzahl der Datenpunkte ≫ Anzahl der Variablen\\n  – Minimum 50 – 100 Datenpunkte\\n  – #Datenpunkte = 5–10 · #Variablen\\n  – #Datenpunkte = 20 · #Variablen\\n– Verwendung von möglichst wenig Variablen\\n– Verwendung von möglichst vielen Datenpunkten\\n– Anwendbarkeit\\n  – Bartlett‘s Test auf Sphärizität\\n    – Nullhypothese: die Korrelationsmatrix ist gleich der Einheitsmatrix\\n    – Signifikanz: p < 0,05\\n    – Nullhypothese wird abgelehnt → Faktorenanalyse möglich\\n    – Voraussetzung: multivariate Normalverteilung\\n    – χ² = − (n − 1 − (2·m+5)/6) · log(det(R))\\n      R: Korrelationsmatrix","explanationImage":"","questionImage":""},{"id":13,"questionDe":"Was prüft der Bartlett-Test auf Sphärizität und wann ist eine Faktorenanalyse zulässig?","questionJa":"Bartlettの球面性検定では何を検定しますか？また、どのような場合に因子分析を行うことができますか？","answerDe":["Der Bartlett-Test prüft, ob die Korrelationsmatrix einer Einheitsmatrix entspricht.","Wenn die Nullhypothese abgelehnt wird (p < 0,05), kann eine Faktorenanalyse durchgeführt werden."],"answerJa":["Bartlett検定は、相関行列が単位行列に等しいかどうかを検定します。","p値が0.05未満で帰無仮説が棄却された場合、因子分析を行うことができます。"],"explanationDe":["In der Faktorenanalyse ist es wichtig, dass zwischen den Variablen genügend Korrelationen bestehen, damit gemeinsame Faktoren identifiziert werden können.","Der Bartlett-Test auf Sphärizität prüft genau diese Voraussetzung. Er testet die Nullhypothese, dass die Korrelationsmatrix einer Einheitsmatrix entspricht – also keine Korrelationen zwischen den Variablen vorliegen.","Ein signifikantes Testergebnis (p < 0,05) bedeutet, dass die Korrelationen ausreichend stark sind, um eine Faktorenanalyse durchzuführen.","Die Teststatistik basiert auf der Determinante der Korrelationsmatrix und folgt einer Chi-Quadrat-Verteilung. Voraussetzung ist eine multivariate Normalverteilung der Daten.","Beispiel: Wenn viele Variablen in einem Fragebogen ähnlich beantwortet werden, deutet dies auf gemeinsame latente Faktoren hin – der Bartlett-Test kann zeigen, ob diese Annahme gerechtfertigt ist."],"explanationJa":["因子分析では、観測された変数群の背後に共通する因子（潜在変数）があると仮定します。そのため、変数同士にある程度の相関が存在することが前提条件です。","Bartlettの球面性検定は、この前提が成り立っているかを確認するための検定で、相関行列が単位行列（すべての変数が無相関）と等しいかを検証します。","p値が0.05未満であれば、帰無仮説は棄却され、変数間に有意な相関があると判断され、因子分析が可能になります。","この検定は相関行列の行列式（determinant）を利用し、カイ二乗分布に従う検定統計量を用います。前提条件として、データが多変量正規分布に従う必要があります。","例：アンケート調査で「価格」「価値」「満足度」などの質問項目の回答に似た傾向があれば、それらに共通する『購買意欲』のような因子があると仮定され、この検定で因子分析の適用が妥当かを判断できます。"],"originalSlideText":"Bartlett‘s Test auf Sphärizität\\n- Nullhypothese: die Korrelationsmatrix ist gleich der Einheitsmatrix\\n- Signifikanz: p < 0,05\\n- Nullhypothese wird abgelehnt → Faktorenanalyse möglich\\n- Voraussetzung: multivariate Normalverteilung\\n- χ² = − (n − 1 − 2·m+5 / 6) log(det(R))\\n- R: Korrelationsmatrix","explanationImage":"lecture01/lecture06_ex02.png","questionImage":""},{"id":14,"questionDe":"Was misst das Measure of Sampling Adequacy (MSA) und wie wird es interpretiert?","questionJa":"MSA（サンプリング適性尺度）とは何を測定する指標で、どのように評価されますか？","answerDe":["MSA misst, ob eine Variable für die Faktorenanalyse geeignet ist.","Ein Wert kleiner als 0,5 bedeutet: ungeeignet.","Ein Wert zwischen 0,6 und 0,8: brauchbar.","Ein Wert größer als 0,8: gut geeignet."],"answerJa":["MSAは変数が因子分析に適しているかどうかを示す指標です。","MSAが0.5未満の場合：因子分析に不適切。","0.6から0.8の間：使用可能な変数。","0.8より大きい：因子分析に非常に適した変数。"],"explanationDe":["Das Measure of Sampling Adequacy (MSA) zeigt, wie stark eine Variable mit anderen Variablen zusammenhängt und ob sie für eine gemeinsame Faktorenstruktur geeignet ist.","Eine hohe MSA bedeutet, dass die Variable stark mit anderen korreliert und nur geringe partielle Korrelationen aufweist.","Die MSA für eine Variable j wird folgendermaßen berechnet:","MSA_j = Summe der quadrierten Korrelationen r_jk mit anderen Variablen geteilt durch (Summe der quadrierten Korrelationen r_jk plus Summe der quadrierten partiellen Korrelationen p_jk).","Sind die partiellen Korrelationen hoch, deutet das auf eine geringe Eignung für Faktorenanalyse hin.","Beispiel: Eine Variable, die mit vielen anderen stark korreliert, aber nach Herausrechnen der restlichen Einflüsse (durch partielle Korrelation) weiterhin starke Zusammenhänge zeigt, erhält eine hohe MSA und ist gut geeignet."],"explanationJa":["MSA（サンプリング適性尺度）は、ある変数が因子分析に適しているかどうかを判断するための指標です。","この値は、変数が他の変数とどれだけ強く相関しており、かつ偏相関（他の変数の影響を除いた後の相関）が小さいかどうかを見ます。","MSAの計算式は、相関係数r_jkの二乗の合計を、相関係数の二乗の合計と偏相関係数p_jkの二乗の合計の和で割ったものです。","つまり、MSA_j = r^2の合計 / (r^2の合計 + p^2の合計)","偏相関が大きい（他の変数の影響が大きい）場合、MSAは低くなり、因子分析には適さないとされます。","例えば、ある設問が他の設問と強く関連していて、さらに他の変数の影響を除いてもその関係が残る場合、その設問はMSAが高く、因子分析に適しています。"],"originalSlideText":"Anwendbarkeit: Measure of Sampling Adequacy (MSA)\\n- Gesamt und für jede der m Variablen\\n- Variablen mit kleineren Werten werden von der Faktorenanalyse ausgenommen\\n- ∀1 ≤ j ≤ m: MSA_j := ∑_{k≠j} r^2_{jk} / (∑_{k≠j} r^2_{jk} + ∑_{k≠j} p^2_{jk})\\n- r_jk: Korrelation zwischen j und k\\n- p_jk: partielle Korrelation zwischen j und k\\n- Auswertung\\n  - < 0,5: Variable ungeeignet\\n  - 0,6 < MSA_j ≤ 0,8: Variable brauchbar\\n  - > 0,8: Variable gut geeignet\\n- Partielle Korrelation: p_jk,l = (r_jk - r_jl r_kl) / sqrt((1 - r^2_jl)(1 - r^2_kl))","explanationImage":"lecture01/lecture06_ex03.png","questionImage":""}]');const d={class:"container py-4"},h={class:"text-center mb-5"},g={class:"display-5 fw-bold text-primary"},m={class:"fs-5 text-muted"},c={class:"text-dark"};var p={__name:"Lecture06Page",setup(e){const n=(0,s.lq)(),i=(0,a.KR)(""),p=(0,a.KR)(""),k=(0,a.KR)(""),b=(0,a.KR)([]);return(0,t.sV)(()=>{const e="lecture01",t=parseInt(n.name.split("_")[1]),r=u[e];i.value=r.title,k.value=t.toString().padStart(2,"0");const a=r.lectures.find(e=>e.number===t);p.value=a?a.title:"",b.value=o}),(e,n)=>((0,t.uX)(),(0,t.CE)("div",d,[(0,t.Lk)("div",h,[(0,t.Lk)("h1",g,(0,r.v_)(i.value),1),(0,t.Lk)("p",m,[(0,t.eW)(" Lecture "+(0,r.v_)(k.value)+": ",1),(0,t.Lk)("span",c,(0,r.v_)(p.value),1)]),n[0]||(n[0]=(0,t.Lk)("hr",{class:"w-25 mx-auto border-primary"},null,-1))]),((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(b.value,e=>((0,t.uX)(),(0,t.Wv)(l.A,{key:e.id,question:e},null,8,["question"]))),128))]))}};const k=p;var b=k}}]);
//# sourceMappingURL=697.8489bd9b.js.map