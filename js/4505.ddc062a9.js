"use strict";(self["webpackChunkstudy_site"]=self["webpackChunkstudy_site"]||[]).push([[4505],{495:function(e,n,i){i.d(n,{A:function(){return M}});var r=i(6768),t=i(4232),a=i(144);const s={class:"card mb-4 shadow-sm"},l={class:"card-body"},o={class:"card-title"},d={class:"text-muted fst-italic"},u={key:0},h=["src"],g={key:1,class:"mt-3"},c={class:"alert alert-success"},m={key:0},w={key:1},b={class:"alert alert-info mt-2"},k={key:0},p={key:1},z={class:"mt-3"},f={key:0},v={key:1},D={key:2},N={key:3},x={key:4},A=["src"],E={class:"mt-4"},S={class:"border rounded p-3 bg-white text-secondary",style:{"white-space":"pre-wrap","font-family":"inherit"}};var B={__name:"QuestionItem",props:{question:Object},setup(e){const n=(0,a.KR)(!1);return(i,a)=>((0,r.uX)(),(0,r.CE)("div",s,[(0,r.Lk)("div",l,[(0,r.Lk)("h5",o,"Q"+(0,t.v_)(e.question.id)+": "+(0,t.v_)(e.question.questionJa),1),(0,r.Lk)("p",d,"("+(0,t.v_)(e.question.questionDe)+")",1),e.question.questionImage?((0,r.uX)(),(0,r.CE)("div",u,[(0,r.Lk)("img",{src:`./images/${e.question.questionImage}`,class:"img-fluid rounded my-2 border",alt:"question image"},null,8,h)])):(0,r.Q3)("",!0),(0,r.Lk)("button",{class:"btn btn-outline-primary mt-2",onClick:a[0]||(a[0]=e=>n.value=!n.value)},(0,t.v_)(n.value?"Hide Answer":"Check Answer"),1),n.value?((0,r.uX)(),(0,r.CE)("div",g,[(0,r.Lk)("div",c,[a[1]||(a[1]=(0,r.Lk)("strong",null,"Antwort (De):",-1)),a[2]||(a[2]=(0,r.Lk)("br",null,null,-1)),Array.isArray(e.question.answerDe)?((0,r.uX)(),(0,r.CE)("ul",m,[((0,r.uX)(!0),(0,r.CE)(r.FK,null,(0,r.pI)(e.question.answerDe,(e,n)=>((0,r.uX)(),(0,r.CE)("li",{key:n},(0,t.v_)(e),1))),128))])):((0,r.uX)(),(0,r.CE)("p",w,(0,t.v_)(e.question.answerDe),1))]),(0,r.Lk)("div",b,[a[3]||(a[3]=(0,r.Lk)("strong",null,"Übersetzung (Ja):",-1)),a[4]||(a[4]=(0,r.Lk)("br",null,null,-1)),Array.isArray(e.question.answerJa)?((0,r.uX)(),(0,r.CE)("ul",k,[((0,r.uX)(!0),(0,r.CE)(r.FK,null,(0,r.pI)(e.question.answerJa,(e,n)=>((0,r.uX)(),(0,r.CE)("li",{key:n},(0,t.v_)(e),1))),128))])):((0,r.uX)(),(0,r.CE)("p",p,(0,t.v_)(e.question.answerJa),1))]),(0,r.Lk)("div",z,[a[6]||(a[6]=(0,r.Lk)("p",{class:"fw-bold mb-1"},"Erklärung (De):",-1)),Array.isArray(e.question.explanationDe)?((0,r.uX)(),(0,r.CE)("div",f,[((0,r.uX)(!0),(0,r.CE)(r.FK,null,(0,r.pI)(e.question.explanationDe,(e,n)=>((0,r.uX)(),(0,r.CE)("p",{key:n},(0,t.v_)(e),1))),128))])):((0,r.uX)(),(0,r.CE)("p",v,(0,t.v_)(e.question.explanationDe),1)),a[7]||(a[7]=(0,r.Lk)("p",{class:"fw-bold mb-1"},"解説 (Ja):",-1)),Array.isArray(e.question.explanationJa)?((0,r.uX)(),(0,r.CE)("div",D,[((0,r.uX)(!0),(0,r.CE)(r.FK,null,(0,r.pI)(e.question.explanationJa,(e,n)=>((0,r.uX)(),(0,r.CE)("p",{key:n},(0,t.v_)(e),1))),128))])):((0,r.uX)(),(0,r.CE)("p",N,(0,t.v_)(e.question.explanationJa),1)),e.question.explanationImage?((0,r.uX)(),(0,r.CE)("div",x,[(0,r.Lk)("img",{src:`./images/${e.question.explanationImage}`,class:"img-fluid rounded my-2 border",alt:"explanation image"},null,8,A)])):(0,r.Q3)("",!0),(0,r.Lk)("div",E,[a[5]||(a[5]=(0,r.Lk)("p",{class:"fw-bold mb-1"},"原文（スライド抜粋）:",-1)),(0,r.Lk)("div",S,(0,t.v_)(e.question.originalSlideText),1)])])])):(0,r.Q3)("",!0)])]))}};const T=B;var M=T},4505:function(e,n,i){i.r(n),i.d(n,{default:function(){return k}});i(8111),i(116);var r=i(6768),t=i(4232),a=i(144),s=i(1387),l=i(495),o=i(3529),d=JSON.parse('[{"id":1,"questionDe":"(s2–4) Warum sind mehrschichtige Perzeptrons notwendig, um das XOR-Problem zu lösen?","questionJa":"★XOR問題を解決するために、なぜ多層パーセプトロンが必要なのか？","answerDe":["XOR ist mit einfachen künstlichen Neuronen nicht linear trennbar","Einzelnes Perzeptron kann XOR nicht korrekt klassifizieren","Mehrschichtige Perzeptrons mit Hidden Layer ermöglichen nichtlineare Trennung"],"answerJa":["★XORは単純な人工ニューロンでは線形分離できない","★単一のパーセプトロンではXORを正しく分類できない","★中間層を持つ多層パーセプトロンにより非線形な分離が可能になる"],"explanationDe":["Das XOR-Problem zeigt, dass einfache Perzeptrons nur linear trennbare Probleme lösen können.","In der Wahrheitstabelle von XOR erkennt man, dass keine lineare Trennlinie existiert, die alle Klassen korrekt trennt.","Dies führt zu einem logischen Widerspruch, wenn man versucht, eine Gewichtung für ein einzelnes Perzeptron zu finden.","1969 zeigten Minsky und Papert, dass einfache künstliche Neuronen damit an ihre Grenze stoßen.","Die Lösung des XOR-Problems wurde erst durch die Einführung von mehrschichtigen Perzeptrons möglich.","Diese besitzen versteckte Schichten (Hidden Layers), in denen Zwischenrepräsentationen erzeugt werden, um komplexere, nichtlineare Trennungen zu ermöglichen.","Ein Beispiel ist ein Netzwerk mit zwei Neuronen in der versteckten Schicht und einem Output-Neuron.","Solche Netze gehören zur Klasse der Feedforward-Netze, da die Information nur vorwärts weitergegeben wird."],"explanationJa":["XOR問題は、単純なパーセプトロンでは解決できないことを示す典型的な例です。","XORの真理値表を見ると、すべてのデータを直線で分けることができず、線形分離が不可能であることがわかります。","単一のパーセプトロンでは、重みを調整しても論理的に矛盾が生じ、正しく分類できません。","1969年にMinskyとPapertがこの限界を指摘し、人工ニューロンは行き詰まりであると結論づけました。","その後、約15年間ニューラルネット研究は停滞しました。","この問題を解決する鍵となったのが多層パーセプトロン（MLP）の導入です。","MLPは中間層（Hidden Layer）を持ち、複雑で非線形な分離を実現できます。","例えば、2つの中間ニューロンと1つの出力ニューロンからなる構造で、XORを正確に学習・分類することが可能です。","このようなネットワークは、情報を一方向に伝えるフィードフォワード型であることも特徴です。"],"originalSlideText":"Neuronale Netze\\n– XOR Problem\\n– Triviales Problem ist mit künstlichen Neuronen nicht lösbar!\\n– 1969 publiziert von Minsky und Papert\\n– Folgerung: künstliche Neuronen sind eine Sackgasse\\n– Forschung wurde daraufhin ca. 15 Jahre eingestellt\\n– Lösungen sind:\\n – Mehrschichtige Perzeptrons (Feedforward Netze)\\n – Nichtlineare Trennfunktionen","explanationImage":"","questionImage":""},{"id":2,"questionDe":"(s4) Wie funktionieren mehrschichtige Perzeptrons zur Lösung nichtlinearer Probleme wie XOR?","questionJa":"★XORのような非線形問題を解決するために、多層パーセプトロンはどのように機能するか？","answerDe":["Einführung von zusätzlichen Schichten (Hidden Layer)","Signal wird schrittweise vom Eingang zum Ausgang weitergeleitet","Struktur: Feedforward-Netzwerk"],"answerJa":["★中間層（Hidden Layer）の導入","入力から出力へと信号を段階的に伝える","★構造はフィードフォワード型ネットワーク"],"explanationDe":["Mehrschichtige Perzeptrons (MLPs) bestehen aus einer Eingabeschicht, einer oder mehreren versteckten Schichten (Hidden Layers) und einer Ausgabeschicht.","Die versteckten Schichten ermöglichen es dem Netzwerk, Zwischenberechnungen durchzuführen und somit auch nichtlineare Funktionen wie XOR zu modellieren.","Im Beispiel der Folie wird gezeigt, wie zwei Neuronen in der versteckten Schicht zusammenwirken, um die XOR-Funktion korrekt zu berechnen.","Dabei erhält jeder Knoten gewichtete Eingaben, summiert sie und gibt das Ergebnis weiter.","Die Netzstruktur ist feedforward, das heißt die Informationen fließen nur in eine Richtung: von den Eingaben über die Hidden Layer bis zur Ausgabe."],"explanationJa":["多層パーセプトロン（MLP）は、入力層・1つ以上の中間層（Hidden Layer）・出力層から構成されるニューラルネットワークです。","中間層を介することで、XORのような非線形な関数も学習することができます。","スライドの図では、中間層に2つのニューロンを配置することで、XORの条件を満たす出力を生成しています。","各ニューロンは入力を重み付けし、合計した後に次の層へ信号を伝えます。","このように、入力から出力まで信号が一方向に流れる構造はフィードフォワード型と呼ばれます。"],"originalSlideText":"Neuronale Netze\\n– Mehrschichtige Perceptrons\\n - Einführung von zusätzlichen Schichten\\n - Hidden layer\\n - Signal wird vom Eingang zu den Ausgängen weitergeleitet\\n - Feedforward Netze","explanationImage":"lecture01/lecture11_ex01.png","questionImage":""},{"id":3,"questionDe":"(s5) Welche Netzwerktopologien gibt es bei neuronalen Netzen und wie funktionieren sie?","questionJa":"★ニューラルネットワークにはどのようなトポロジー（接続構造）があり、それぞれどのように機能するか？","answerDe":["Fully connected: Alle Neuronen einer Schicht sind mit allen Neuronen der nächsten Schicht verbunden","Short Cuts: Einige Neuronen sind mit weiter entfernten Schichten verbunden"],"answerJa":["★全結合（Fully connected）：ある層のすべてのニューロンが次の層のすべてのニューロンと接続されている","ショートカット（Short Cuts）：一部のニューロンが離れた層のニューロンと直接接続されている"],"explanationDe":["In neuronalen Netzen bezeichnet die Topologie die Art und Weise, wie die Neuronen über Schichten hinweg verbunden sind.","Bei der \'fully connected\'-Struktur ist jedes Neuron einer Schicht mit jedem Neuron der nächsten Schicht verbunden.","Diese Struktur ermöglicht eine maximale Informationsweitergabe, ist aber auch rechenintensiv.","Im dargestellten XOR-Netzwerk sind die Eingabeneuronen vollständig mit der versteckten Schicht und diese wiederum mit dem Ausgabeneuron verbunden.","Bei der \'Short Cut\'-Topologie gibt es zusätzliche Verbindungen zwischen nicht benachbarten Schichten.","Diese können z. B. verwendet werden, um Informationen schneller zu tieferen Schichten weiterzuleiten oder das Vanishing-Gradient-Problem in tiefen Netzen zu verringern."],"explanationJa":["ニューラルネットワークにおけるトポロジーとは、層をまたぐニューロン間の接続の仕方を意味します。","「全結合（Fully connected）」では、ある層のすべてのニューロンが、次の層のすべてのニューロンに接続されており、情報の伝達が最大限に行われます。","スライドに示されているXORネットワークも、入力層→中間層→出力層が全結合構造になっています。","一方「ショートカット（Short Cuts）」は、あるニューロンが離れた層のニューロンと直接つながっている構造です。","これにより、深いネットワークで情報をより効率的に伝えることができたり、勾配消失問題を緩和したりする効果があります。"],"originalSlideText":"Neuronale Netze\\n– Topologien\\n– Fully connected\\n - Alle Neuronen einer Schicht werden mit allen Neuronen der folgenden Schicht verbunden\\n– Short Cuts\\n - Einige Neuronen sind zudem mit Neuronen in weiter entfernten Schichten verbunden","explanationImage":"","questionImage":""},{"id":4,"questionDe":"(s6) Warum reicht das Delta-Regel-Verfahren bei mehrschichtigen Netzen nicht aus?","questionJa":"★多層ネットワークにおいて、デルタ則だけでは学習に不十分なのはなぜか？","answerDe":["Delta-Regel berechnet nur den Gradienten auf einer Schicht","Fehler verteilt sich jedoch über mehrere Schichten","Gewichte in allen Layern müssen angepasst werden"],"answerJa":["★デルタ則は単一層における勾配しか計算できない","★実際には誤差は複数の層にまたがって広がる","すべての層の重みを調整する必要がある"],"explanationDe":["Die Delta-Regel wurde ursprünglich für einfache Netzwerke mit nur einer Schicht entwickelt.","Sie berechnet den Gradienten basierend auf dem Fehler nur auf dieser einen Ausgabeschicht.","In einem mehrschichtigen Netzwerk hängt der endgültige Fehler jedoch auch von den versteckten Schichten ab.","Daher muss man die Fehlerinformation rückwärts durch das Netz leiten, um alle beteiligten Gewichte korrekt anpassen zu können.","Ein einfaches lokales Anpassen wie mit der Delta-Regel reicht hier nicht mehr aus."],"explanationJa":["デルタ則はもともと単層のネットワーク用に設計されており、出力層での誤差に基づいて重みの勾配を計算します。","しかし、多層ネットワークでは中間層（Hidden Layer）も出力に影響を与えるため、誤差は全層に分布します。","したがって、全層の重みを正しく更新するには、誤差の情報をネットワーク全体に逆方向に伝える必要があります。","このように、デルタ則だけでは不十分となり、より高度な手法が必要です。"],"originalSlideText":"– Lernverfahren\\n– Delta Regel berechnet Gradient auf einer Schicht\\n– Fehler verteilt sich aber auf mehrere Schichten\\n– Gewichte müssen nun über mehrere Layer angepasst werden\\n– Wie verteilt man den Einfluss des Fehlers auf die Gewichte?","explanationImage":"","questionImage":""},{"id":5,"questionDe":"(s6) Wie funktioniert das Backpropagation-Verfahren zur Gewichtsaktualisierung in neuronalen Netzen?","questionJa":"★ニューラルネットワークにおける重み更新のためのBackpropagation（誤差逆伝播法）はどのように機能するか？","answerDe":["Fehler wird von den Ausgängen zu den Eingängen rückwärts weitergegeben","Trainingsdaten werden vorwärts durch das Netz propagiert","Fehler werden rückwärts durch das Netz propagiert","Ermöglicht Anpassung der Gewichte in allen Schichten"],"answerJa":["★誤差は出力層から入力層へと逆向きに伝播される","★訓練データは順方向にネットワークを通して伝えられる","★その後、誤差がネットワークを逆方向に伝わっていく","★これにより全ての層の重みを適切に更新できる"],"explanationDe":["Beim Backpropagation-Verfahren wird ein Trainingsbeispiel zunächst durch das Netzwerk vorwärts propagiert, um eine Vorhersage zu erzeugen.","Anschließend wird der Fehler zwischen Vorhersage und Zielwert berechnet.","Dieser Fehler wird dann schrittweise rückwärts durch das Netzwerk geleitet.","Dabei wird mit Hilfe der Kettenregel der Einfluss jedes Gewichts auf den Gesamtfehler bestimmt.","So kann jedes Gewicht im Netzwerk entsprechend seines Beitrags zum Fehler angepasst werden."],"explanationJa":["Backpropagation（誤差逆伝播法）は、まず訓練データを順方向にネットワークに通し、出力を得ます。","次に、出力と正解ラベルとの誤差（損失）を計算します。","その後、その誤差を出力層から入力層へと逆方向に伝播させます。","★このとき、連鎖律（Chain Rule）を使って、各重みが誤差にどれだけ影響しているかを計算します。","★これにより、各重みを誤差の寄与に応じて適切に更新できます。"],"originalSlideText":"– Idee: Propagiere den Fehler durch das Netz\\n– Von den Ausgängen zu den Eingängen\\n– Trainingsdaten werden vorwärts durch das Netz propagiert\\n– Fehler werden rückwärts durch das Netz propagiert\\n– Backpropagation","explanationImage":"","questionImage":""},{"id":6,"questionDe":"(s7) Wie läuft der Backpropagation-Prozess in neuronalen Netzen konkret ab?","questionJa":"★（試験）ニューラルネットにおけるBackpropagation（誤差逆伝播）の具体的な手順は？","answerDe":["Trainingsdaten werden durch das Netz propagiert","Ausgabe wird evaluiert, Differenz wird als Fehler interpretiert","Fehler wird von der Ausgabe zum Eingang propagiert","Gewichte, die zum Fehler führten, werden angepasst"],"answerJa":["訓練データがネットワークを通って順方向に伝播される","出力と正解の差が誤差として評価される","★誤差が出力層から入力層へと逆方向に伝播される","★差に関与した重みが調整される"],"explanationDe":["Beim Backpropagation-Verfahren wird ein Datensatz zuerst durch das Netzwerk geleitet, um eine Vorhersage zu erzeugen.","Dann wird die Ausgabe mit dem Soll-Wert verglichen, und die Differenz wird als Fehler betrachtet.","Dieser Fehler wird rückwärts durch das Netzwerk propagiert.","Dabei wird überprüft, welche Gewichte den Fehler verursacht haben, und genau diese Gewichte werden angepasst.","Ziel ist es, bei zukünftigen Eingaben eine genauere Vorhersage zu erreichen."],"explanationJa":["★Backpropagation（誤差逆伝播法）では、まず訓練データを使ってネットワークに順方向の信号を流し、出力を得ます。","★その出力と正解との誤差を計算し、それをネットワーク内で誤差として扱います。","★この誤差を出力層から入力層に向かって逆方向に伝播させます。","★どの重みが誤差にどれだけ寄与したかを解析し、それに基づいて各重みを調整します。","この調整により、次の学習でより正確な出力が得られるようになります。"],"originalSlideText":"Backpropagation Ablauf:\\n- Trainingsdaten werden durch das Netz propagiert\\n- Ausgabe des Netzes wird evaluiert, die Differenz zwischen Soll und Ist wird als Fehler angenommen\\n- Der Fehler wird von der Ausgabe zum Eingang propagiert\\n- Die Gewichte der Verbindungen, die zu diesem Fehler geführt haben, werden angepasst","explanationImage":"","questionImage":""},{"id":7,"questionDe":"(s7) Worauf muss man bei der Anpassung des Netzes während des Lernens achten?","questionJa":"★ネットワークの学習時における重み調整で注意すべき点は？","answerDe":["Anpassung erfolgt auf den falsch klassifizierten Datensatz","Netz darf sich nicht zu stark an einzelne Datenpunkte anpassen","Lernrate steuert die Anpassung"],"answerJa":["★誤分類されたデータに対してネットワークが調整される","★特定のデータに過剰に適応（過学習）しないように注意が必要","★重みの調整には学習率（Lernrate）を用いて調整の度合いを制御する"],"explanationDe":["Wenn ein Datensatz falsch klassifiziert wurde, wird das Netz entsprechend angepasst.","Dabei besteht die Gefahr, dass sich das Netz zu stark auf einzelne Trainingsbeispiele fokussiert, was zu Overfitting führt.","Um das zu vermeiden, wird eine Lernrate verwendet.","Die Lernrate bestimmt, wie stark die Gewichte nach jedem Fehler angepasst werden.","Eine zu hohe Lernrate kann zu instabiler Konvergenz führen, eine zu niedrige zu langsamer Lernfortschritt."],"explanationJa":["★誤分類されたデータに対してネットワークの重みが調整されます。","★しかし、特定のデータに対して過剰に適応してしまうと、汎化能力が落ちる（過学習）リスクがあります。","このような過学習を防ぐために、学習率（Lernrate）を使って重みの調整幅を制御します。","★学習率が高すぎると収束が不安定になり、逆に低すぎると学習が遅くなるという問題があります。","適切な学習率の設定が、効率的かつ安定した学習に重要です。"],"originalSlideText":"– Anpassung des Netzes auf den falsch klassifizierten Datensatz\\n– Dabei muss verhindert werden, dass das Netz sich überspezifisch an einzelnen Datenpunkten orientiert\\n– Nutzung einer Lernrate, um die Anpassung der Gewichte zu steuern","explanationImage":"","questionImage":""},{"id":8,"questionDe":"(s8) Welche mathematische Grundlage hat das Backpropagation-Verfahren und wie funktioniert es?","questionJa":"Backpropagation（誤差逆伝播法）の数学的な基盤とその仕組みは？","answerDe":["Backpropagation ist ein Spezialfall des Gradientenverfahrens","Typisch ist der mittlere quadratische Fehler als Fehlerfunktion","Neuronen-Ausgabe basiert auf Gewichten und Eingaben","Die Fehlerfläche bildet ein elliptisches Paraboloid","Ziel ist es, den stärksten Abstieg im Gradientenverfahren zu finden"],"answerJa":["Backpropagationは勾配法の特殊な形式である","誤差関数には通常、平均二乗誤差が用いられる","ニューロンの出力は、重みと入力に基づいて決定される","誤差の地形は楕円放物面（パラボロイド）を形成する","勾配法における最も急な下降方向を探索するのが目的である"],"explanationDe":["Backpropagation ist ein mathematisches Verfahren zur Optimierung von Gewichten in neuronalen Netzen.","Es basiert auf dem Gradientenverfahren, wobei die Richtung des stärksten Fehlerrückgangs (Gradientenabstieg) bestimmt wird.","Dabei wird häufig der mittlere quadratische Fehler (MSE) als Fehlerfunktion verwendet.","Dieser Fehler ergibt sich aus der Differenz zwischen der Netzwerkausgabe und dem gewünschten Zielwert.","Die Ausgabe jedes Neurons wird durch die Eingaben und deren Gewichtungen bestimmt.","Wenn man alle möglichen Gewichtskombinationen visualisiert, ergibt sich bei linearen Neuronen eine Fehleroberfläche in Form eines elliptischen Paraboloids.","Backpropagation berechnet auf dieser Oberfläche die Richtung, in der der Fehler am schnellsten abnimmt, und aktualisiert die Gewichte entsprechend."],"explanationJa":["Backpropagation（誤差逆伝播法）は、ニューラルネットワークにおける重みを最適化するための数学的な手法です。","この手法は勾配法に基づいており、誤差を最も早く減少させる方向（勾配の負方向）を計算して重みを更新します。","誤差関数としては、出力と目標値の差を2乗して平均を取った「平均二乗誤差（MSE）」がよく使われます。","各ニューロンの出力は、入力値とそれに対応する重みに基づいて計算されます。","この重み空間上での誤差の分布は、楕円放物面（パラボロイド）のような形状になります。","Backpropagationではこの誤差面上で最も急に誤差が小さくなる方向を見つけ、その方向に重みを少しずつ更新していきます。","これによりネットワークの出力が目標に近づくように学習が進んでいきます。"],"originalSlideText":"– Backpropagation\\n– Spezialfall des Gradientenverfahrens\\n– Meist mittlerer quadratischer Fehler als Fehlerfunktion\\n– Ausgang der Neuronen basiert auf den Gewichten und Eingaben\\n– Bilden ein elliptisches Paraboloid\\n– Backpropagation sucht nach dem stärksten Abstieg im Gradientenverfahren","explanationImage":"","questionImage":""},{"id":9,"questionDe":"(s9) Wie wird beim Backpropagation-Verfahren der Einfluss eines einzelnen Gewichts auf den Gesamtfehler berechnet?","questionJa":"Backpropagation（誤差逆伝播法）では、個々の重みが全体の誤差に与える影響はどのように計算されるか？","answerDe":["Berechnung des Einflusses eines Gewichts w auf den Gesamtfehler","Beispiel: ∂E_total / ∂w5","Lies: „die partielle Ableitung von E_total nach w5“","Oder: „der Gradient in Bezug auf w5“","Verwendung der Kettenregel für die Berechnung"],"answerJa":["各重み w が全体誤差 E_total に与える影響を計算する","例：∂E_total / ∂w5","これは「E_total を w5 で偏微分したもの」と読む","または「w5 に関する勾配」とも言える","この計算には連鎖律（チェーンルール）を用いる"],"explanationDe":["Im Backpropagation-Verfahren geht es darum, herauszufinden, wie stark sich eine Änderung eines bestimmten Gewichts auf den Gesamtfehler auswirkt.","Dazu berechnet man die partielle Ableitung des Fehlers E_total nach dem Gewicht w – also ∂E_total / ∂w.","Am Beispiel w5 spricht man von der partiellen Ableitung ∂E_total / ∂w5.","Das bedeutet: Wie stark verändert sich der Fehler, wenn man w5 leicht ändert.","Zur Berechnung dieser Ableitung verwendet man die Kettenregel.","Das erlaubt, komplexe Ableitungen in einfachere Bestandteile zu zerlegen und so schrittweise zu berechnen."],"explanationJa":["Backpropagationでは、特定の重みがネットワーク全体の誤差にどれだけ影響しているかを調べます。","そのために「全体誤差 E_total を重み w で偏微分（∂E_total / ∂w）」して勾配を求めます。","例として、w5に対しては ∂E_total / ∂w5 を計算し、これが w5 に対する勾配（影響度）を示します。","これは「w5 を少し変えたときに誤差がどれだけ変化するか」を意味します。","このような計算では、複数の関数が連続しているため、連鎖律（チェーンルール）を使って複雑な微分を段階的に分解します。","これにより、ネットワークの全ての重みに対する誤差の影響を効率よく求めることができます。"],"originalSlideText":"– Backpropagation\\n– Berechnung des Einflusses eines Gewichtes w auf den Gesamtfehler\\n– Zum Beispiel w5:\\n∂E_total / ∂w5\\n– Lies: „die partielle Ableitung von E_total nach w_5“\\n– Oder: „Der Gradient in Bezug auf w_5“\\n– Partielle Ableitung können mit der Kettenregel berechnet werden:","explanationImage":"lecture01/lecture11_ex02.png","questionImage":""},{"id":10,"questionDe":"(s25) Nenne einen Vorteil von Backpropagation in Feedforward-Netzen.","questionJa":"★FeedforwardネットにおけるBackpropagationの利点を1つ挙げよ。","answerDe":["Auch nicht lineare Funktionen können approximiert werden"],"answerJa":["★非線形関数も近似できる"],"explanationDe":["Durch den Einsatz von Backpropagation in mehrschichtigen Feedforward-Netzen können auch komplexe, nicht linear trennbare Zusammenhänge gelernt werden.","Das bedeutet, dass solche Netze nicht nur einfache lineare Funktionen, sondern auch z. B. XOR-Logik oder komplizierte Muster erkennen und verarbeiten können."],"explanationJa":["Backpropagationを多層のFeedforwardネットに使うことで、線形に分離できないような複雑な関係も学習可能になります。","つまり、単純な直線だけでなく、XORのような非線形な論理関係や複雑なパターンも扱えるようになります。"],"originalSlideText":"– Durch Backpropagation in Feedforward Netzen können auch nicht lineare Funktionen approximiert werden","explanationImage":"","questionImage":""},{"id":11,"questionDe":"(s25) Nenne 3 Herausforderungen beim Einsatz von Backpropagation in Feedforward-Netzen.","questionJa":"★FeedforwardネットにおけるBackpropagationの課題を3つ挙げよ。","answerDe":["Lernerfolg ist nicht garantiert","Kann sehr lange Trainingszeiten benötigen","Topologie des Netzes hat Auswirkung auf den Lernerfolg"],"answerJa":["★学習が必ず成功するとは限らない","★学習に非常に長い時間がかかる場合がある","★ネットワークの構造が学習成功に影響する"],"explanationDe":["Obwohl Backpropagation in der Theorie stark ist, kann der tatsächliche Lernerfolg ausbleiben, besonders bei komplexen Problemen.","In manchen Fällen dauert das Training sehr lange, insbesondere wenn viele Parameter oder tiefe Netze beteiligt sind.","Die Struktur des Netzes – etwa wie viele Schichten oder Neuronen es hat – beeinflusst maßgeblich, ob und wie gut das Netzwerk lernt."],"explanationJa":["Backpropagationは理論上強力な学習手法ですが、現実の問題においては学習がうまくいかないこともあります。","特に複雑なネットワークやパラメータが多い場合、学習に非常に長い時間がかかることがあります。","ネットワークの構造（層の数やノード数など）は、学習が成功するかどうかに大きく影響します。"],"originalSlideText":"– Lernerfolg ist aber nicht garantiert\\n  – Kann sehr lange Trainingszeiten benötigen\\n  – Topologie des Netzes hat Auswirkung auf den Lernerfolg","explanationImage":"","questionImage":""},{"id":12,"questionDe":"(s25) Welche Vorteile bringen Pooling, Konvolution und komplexere Netztopologien beim Einsatz neuronaler Netze?","questionJa":"★ニューラルネットワークにおいて、Pooling・畳み込み・より複雑なネットワーク構造を使う利点は何か？","answerDe":["Pooling und Konvolution beschleunigen das Training","Sie verbessern die Ergebnisse","Komplexere Netztopologien führen zu besseren Ergebnissen"],"answerJa":["プーリングや畳み込みは学習を高速化する","また、精度の向上にもつながる","複雑なネットワーク構造はより良い結果を生む"],"explanationDe":["Mechanismen wie Pooling (z. B. Max-Pooling) helfen, die Eingabedaten zu komprimieren und unwichtige Details zu ignorieren.","Konvolution (z. B. mit CNNs) hilft, lokale Muster besser zu erkennen, was zu genaueren Vorhersagen führt.","Komplexere Netzwerke wie tiefe neuronale Netze oder ResNet können komplexere Zusammenhänge modellieren und dadurch genauere Ergebnisse liefern."],"explanationJa":["★プーリング（例：Max-Pooling）は入力データのサイズを縮小し、重要な情報だけを残して処理を効率化します。","★畳み込み（CNNなど）では、画像などの局所的な特徴を捉えることで、より正確な予測が可能になります。","★ResNetなどの複雑なネットワーク構造は、複雑な関係性をより良くモデル化でき、予測精度が向上します。"],"originalSlideText":"– Weitere Mechanismen wie Pooling und Konvolution helfen bei der Beschleunigung und verbessern die Ergebnisse\\n– Komplexere Netztopologien führen zu besseren Ergebnissen","explanationImage":"","questionImage":""},{"id":13,"questionDe":"(s26) Welche Herausforderungen gibt es bei der Verarbeitung von Bilddaten in neuronalen Netzen und wie hilft Konvolution dabei?","questionJa":"★ニューラルネットで画像データを処理する際の課題は何か？また、畳み込み（Konvolution）はそれにどう役立つか？","answerDe":["Kanten und Texturen sind wichtig, aber schwer direkt aus Pixeln zu erkennen","Strukturen erfordern viele Neuronen","Farbkombinationen sind komplex","Konvolution extrahiert relevante Merkmale wie Kanten und Texturen vorab"],"answerJa":["★エッジやテクスチャは重要だが、ピクセルから直接認識するのは難しい","★構造を認識するには多くのニューロンが必要","★色情報の組み合わせが複雑","★畳み込み処理によってエッジやテクスチャなどの重要な特徴を事前に抽出できる"],"explanationDe":["In Bilddaten sind Kanten und Texturen entscheidend, um Objekte zu erkennen, aber sie lassen sich aus einzelnen Pixeln nur schwer erfassen.","Um solche Strukturen aufzubauen, werden viele Neuronen benötigt, was die Netzarchitektur komplex macht.","Auch die Kombination von Farbwerten ist nicht trivial.","Konvolution hilft dabei, indem sie durch Filter bereits vor dem Training wichtige Merkmale wie Kanten oder Muster extrahiert.","Dies reduziert die Komplexität des Netzes und verbessert die Erkennungsleistung."],"explanationJa":["画像データにおいて、エッジ（輪郭）やテクスチャ（模様）は物体認識に重要ですが、単なるピクセルの集まりから直接捉えるのは困難です。","こうした構造を学習させるには多くのニューロンが必要になり、ネットワークが複雑になります。","さらに色情報の組み合わせも扱いが難しい要素です。","そこで畳み込み処理では、フィルターを使ってあらかじめエッジや模様などの特徴を抽出することで、ニューラルネットの負担を軽減し、認識精度を高めます。"],"originalSlideText":"– Konvolution\\n– Beispielsweise sind in Bilddaten Kanten und Texturen relevant für die Erkennung\\n– Zusammensetzen von Pixeln zu Strukturen in Netzen nur mit vielen Neuronen\\n– Zusammenspiel von Farbwerten ebenso kompliziert\\n– Idee: Vorverarbeitung des Bildes mittels Filter, sodass Kanten und Texturen vorab extrahiert werden","explanationImage":"lecture01/lecture11_ex03.png","questionImage":""},{"id":15,"questionDe":"(s27) Was ist ein Kernel in der Konvolution und wie wird er angewendet?","questionJa":"畳み込みにおけるカーネル（Kernel）とは何か？また、それはどのように使われるか？","answerDe":["Ein Kernel transformiert Datenwerte","In der Regel wird eine kleinere Matrix erzeugt","Verschiedene Kernel existieren, z. B. zur Kantenerkennung","Stammt aus der Bildverarbeitung"],"answerJa":["カーネルはデータの値を変換するための行列","通常、出力は元より小さい行列になる","エッジ検出など目的に応じたカーネルが使われる","画像処理の概念に基づいている"],"explanationDe":["Ein Kernel ist eine kleine Matrix, die über ein Bild gleitet und dabei Berechnungen wie Multiplikationen mit Bildausschnitten durchführt.","Ziel ist es, bestimmte Merkmale wie Kanten oder Texturen zu extrahieren.","Die Anwendung des Kernels reduziert die Größe der Eingabedaten und hebt relevante Strukturen hervor.","Zum Beispiel erkennt ein Kanten-Kernel horizontale oder vertikale Übergänge im Bild.","Dieses Verfahren stammt ursprünglich aus der klassischen Bildverarbeitung und wird in neuronalen Netzen zur Merkmalsextraktion genutzt."],"explanationJa":["カーネルとは、画像に対して滑らせながら使う小さな行列で、入力データの一部に対して数値的な演算（主に積と和）を行います。","この操作により、画像内のエッジや模様などの特徴を抽出することができます。","カーネルの適用によって、元の画像よりも小さい特徴マップ（convolved feature）が生成され、計算コストが減りつつ重要な情報を強調できます。","たとえば、縦や横の境界（エッジ）を検出するカーネルを使うと、輪郭が強調された画像が得られます。","このような手法は元々画像処理分野で使われており、ディープラーニングでも広く利用されています。"],"originalSlideText":"– Konvolution\\n– Nutzung eines Kernels, um die Datenwerte zu transformieren\\n  – In der Regel in eine kleinere Matrix\\n– Verschiedene Kernel möglich, z.B. um Kanten zu erkennen\\n– Konzept aus der Bildverarbeitung","explanationImage":"lecture01/lecture11_ex04.png","questionImage":""},{"id":16,"questionDe":"(s28) Welche Arten von Konvolution werden unterschieden und wofür sind sie geeignet?","questionJa":"どのような種類の畳み込み（コンボリューション）があり、それぞれ何に適しているか？","answerDe":["1D Konvolution: Transformation eines 1D-Datenbereichs auf einen anderen","– Geeignet für Zeitserien","3D Konvolution: Verarbeitung von 3-dimensionalen Bilddaten","– Besonders geeignet für MRT-Daten (Magnetresonanztomographie)"],"answerJa":["1次元畳み込み（1Dコンボリューション）：1次元データ領域を別の1次元データ領域に変換","– 時系列データに適用される","3次元畳み込み（3Dコンボリューション）：3次元の画像データに対して処理を行う","– 特にMRT（磁気共鳴画像）などの医療画像データに有効"],"explanationDe":["Bei der 1D-Konvolution wird ein eindimensionaler Filter über eine eindimensionale Eingabedatenreihe (z. B. ein Signal oder eine Zeitreihe) geschoben.","Dabei wird jeder kleine Abschnitt des Signals mit dem Filter multipliziert und die Ergebnisse summiert.","Diese Methode eignet sich besonders für Daten, bei denen die zeitliche Abfolge eine Rolle spielt, z. B. Sensorwerte oder Sprachdaten.","Die 3D-Konvolution ist eine Erweiterung der 2D-Konvolution auf dreidimensionale Daten.","Hierbei wird ein 3D-Kernel über 3D-Eingabedaten (z. B. MRT-Scans) geschoben, um Merkmale wie Volumenmuster zu extrahieren.","Sie ist besonders nützlich in der medizinischen Bildgebung, bei der das Datenvolumen aus mehreren Schichten besteht."],"explanationJa":["1次元畳み込みは、時系列などの1次元データに対してカーネル（フィルター）をスライドさせながら適用する手法です。","各スライド位置で入力データの一部とカーネルを掛け合わせ、和を取ることで新たな出力を得ます。","この方法は、音声信号やセンサーデータのように時間的な順序が重要な場合に適しています。","一方、3次元畳み込みは画像のような3次元データに対して行う手法です。","たとえばMRI（磁気共鳴画像）データなど、立体的な構造を持つ画像から特徴を抽出するのに使われます。","3Dカーネルを用いて奥行き方向も含めて処理するため、より複雑なパターン検出が可能です。"],"originalSlideText":"– Konvolution\\n– 1D Konvolution\\n  – Transformation eines Datenbereichs in 1D auf einen 1D Datenbereich\\n  – Z.B. für Zeitserien\\n– 3D Konvolution\\n  – Insbesondere für 3 dimensionale Bilddaten (MRT etc)","explanationImage":"lecture01/lecture11_ex05.png","questionImage":""},{"id":17,"questionDe":"(s29) Nenne 5 Vorteile der Verwendung von Konvolution in neuronalen Netzen.","questionJa":"★畳み込みをニューラルネットワークで使うことの利点を5つ挙げよ。","answerDe":["1. Datenbereiche werden durch spezialisierte Filter auf wichtige Eigenschaften reduziert.","2. Schnelleres Verarbeiten der Daten im Netzwerk.","3. Kernel werden als Neuronen im Netz dargestellt, was Parameter spart.","4. Gemeinsame Gewichte bei Neuronen sparen Ressourcen.","5. Robuster gegenüber Translation und Skalierung."],"answerJa":["★1. 特定のフィルターを用いてデータ領域を重要な特徴に要約できる。","★2. 要約されたデータによりネットワーク内での処理が高速になる。","★3. カーネルはネットワーク内でニューロンとして表現されるため、効率的な構造が可能。","★4. ニューロン間で重みを共有することでパラメータ数が削減される。","★5. 平行移動やスケーリングに対して頑健性がある。"],"explanationDe":["Konvolution fasst komplexe Bildbereiche mit Hilfe von Filtern zu Merkmalen zusammen, z. B. Kanten oder Texturen.","Diese Vorverarbeitung ermöglicht es dem Netz, sich auf relevante Informationen zu konzentrieren.","Durch gemeinsame Gewichte für alle Positionen (Gewichtsteilung) wird die Anzahl der zu lernenden Parameter stark reduziert.","Dies führt zu einer besseren Generalisierung und spart Rechenleistung.","Zudem ist das Netzwerk robuster gegen Verschiebungen (Translation) oder Größenänderungen (Skalierung) im Eingabebild, da die Merkmale unabhängig von ihrer Position erkannt werden."],"explanationJa":["畳み込みでは、画像内の局所的な情報（例えばエッジや模様）をフィルターで抽出して特徴量としてまとめます。","この処理により、入力データの重要な部分だけを抽出でき、ネットワーク全体の処理を効率化できます。","また、カーネル（重み行列）を全体の入力に対して共有することで、パラメータの数が大幅に削減され、学習の安定性も向上します。","抽出された特徴は画像のどこに現れても等しく重要と見なされるため、位置の違いに強くなります。","さらに、画像の拡大・縮小や平行移動があっても、認識精度が保たれるという頑健性もあります。"],"originalSlideText":"– Konvolution\\n– Datenbereiche werden durch spezifische Filter auf bestimmte Eigenschaften zusammengefasst\\n– Daten können dann im Netz schneller verarbeitet werden\\n– Kernel werden per Neuronen im Netz dargestellt\\n– Neuronen haben gemeinsame Gewichte\\n– Extrahierte Features sind an jeder Stelle gleich wichtig\\n– Deutlich weniger Kanten notwendig\\n– Robuster gegenüber Translation, Skalierung","explanationImage":"","questionImage":""},{"id":18,"questionDe":"(s30) Welche 4 Vorteile bietet Pooling in neuronalen Netzen?","questionJa":"★ニューラルネットワークにおいてプーリング層がもたらす4つの利点を挙げよ。","answerDe":["1. Reduktion der Eingabedaten für die weitere Verarbeitung.","2. Verringerung von Speicherbedarf und Rechenzeit.","3. Extraktion von dominanten Merkmalen (Features).","4. Invarianz gegenüber Translation und Positionsänderungen."],"answerJa":["★1. 次の処理のために入力データ量を削減する。","★2. メモリ使用量と処理時間を削減する。","★3. 重要な特徴（特徴量）を抽出する。","★4. 平行移動や位置の違いに対して出力が安定する（不変性）。"],"explanationDe":["Pooling ist ein Verfahren, das in Convolutional Neural Networks (CNNs) verwendet wird, um große Bilddatenmengen effizient zu verarbeiten.","1. Durch Pooling wird die Größe der Zwischenausgaben reduziert – das bedeutet, dass z. B. ein 4×4-Pixelbereich auf einen einzelnen Wert (z. B. das Maximum) zusammengefasst wird. Dadurch wird die Datenmenge kleiner.","2. Weniger Daten bedeuten, dass das Modell weniger Speicher benötigt und schneller rechnen kann. Besonders bei großen Bildern oder Videos ist dieser Effekt entscheidend.","3. Beim Pooling – zum Beispiel Max-Pooling – wird aus jedem Bereich der höchste Wert genommen. Dadurch bleiben dominante Merkmale wie scharfe Kanten oder helle Bereiche erhalten, während unwichtige Informationen unterdrückt werden.","4. Ein großer Vorteil ist die Robustheit gegenüber kleinen Verschiebungen im Bild: Wenn sich ein Objekt im Bild leicht nach rechts verschiebt, bleibt der Pooling-Ausgang oft gleich. Das hilft dem Modell, zuverlässige Vorhersagen zu machen, auch wenn das Eingabebild nicht exakt gleich ist."],"explanationJa":["プーリングは、畳み込みニューラルネットワーク（CNN）で使われる重要な処理手法で、大量の画像データを効率よく扱うために用いられます。","1. たとえば4×4のピクセル領域を1つの最大値にまとめるMaxプーリングでは、データ量を大きく圧縮できます。これはネットワークの後段への負荷を軽減します。","2. 入力データが少なくなることで、必要なメモリ容量や計算時間も大幅に減少し、大規模な画像や動画データでも高速に処理できます。","3. プーリングでは、特徴量の中でも特に重要な部分（たとえばエッジやコントラストが強い箇所）が残ります。これによりモデルは画像の本質的な情報を捉えやすくなります。","4. 画像が少し移動しても（たとえば人物の顔が中央から少し横にずれた場合など）、プーリングにより出力結果があまり変化しないため、分類や認識の精度が維持されます。"],"originalSlideText":"– Pooling\\n– Reduktion der Eingabedaten für die weitere Verarbeitung\\n  – Speicherbedarf & Rechenzeit\\n– Extraktion von dominanten Features\\n– Translations- und Positionsinvariant","explanationImage":"lecture01/lecture11_ex06.png","questionImage":""},{"id":19,"questionDe":"(s31) Welche zwei Arten von Pooling gibt es und wie unterscheiden sie sich?","questionJa":"★プーリングにはどのような2つの種類があり、それらはどのように異なるか？","answerDe":["1. Max Pooling: Der größte Wert in einem Datenbereich wird als Repräsentant übernommen.","2. Average Pooling: Der Mittelwert der Werte in einem Datenbereich wird als Repräsentant übernommen.","Max Pooling ist in der Regel performanter als Average Pooling."],"answerJa":["★1. Maxプーリング：データ領域の中で最大の値を代表値として採用する。","★2. 平均（Average）プーリング：データ領域の平均値を代表値として採用する。","★一般的に、Maxプーリングの方がAverageプーリングよりも高性能である（performanter）。"],"explanationDe":["Beim Pooling wird ein kleiner Bereich der Eingabedaten betrachtet – z. B. ein 2×2-Feld – und durch einen einzigen Wert ersetzt.","Beim Max Pooling wird aus diesem Bereich der größte Wert übernommen. Dies hilft, besonders ausgeprägte Merkmale wie Kanten oder Muster zu erhalten.","Beim Average Pooling hingegen wird der Durchschnitt aller Werte im Bereich berechnet. Dies führt zu einer geglätteten Datenrepräsentation.","Max Pooling ist laut Folie in der Regel performanter, da es markantere Informationen besser beibehält und zu effizienteren Modellen führen kann."],"explanationJa":["プーリングとは、入力データの一部（例えば2×2の領域）を1つの代表値に変換する処理です。","Maxプーリングでは、その領域の中で最大の値を取り出し、特徴が強く現れている部分（エッジなど）を保持します。","平均プーリングでは、領域内の値の平均を計算するため、データがなだらかに表現されます。","スライドでは、Maxプーリングの方が一般的に高性能（performanter）であり、より効果的な特徴抽出に寄与すると示されています。"],"originalSlideText":"– Pooling\\n– Berechnung für einen Ausschnitt der Daten einen Repräsentanten:\\n  – Max Pooling\\n    – Übernahme des größten Wertes als Repräsentanten\\n  – Average Pooling\\n    – Mittelwert als Repräsentant\\n\\nMax Pooling ist in der Regel performanter","questionImage":"","explanationImage":"lecture01/lecture11_ex07.png"},{"id":20,"questionDe":"(s32) Was ist ein (Deep) Feed Forward Network und wodurch entsteht dessen Tiefe?","questionJa":"(Deep) Feed Forward Network（フィードフォワードネットワーク）とは何か？またその「深さ」はどのように決まるか？","answerDe":["Ein Feed Forward Network (FF) ist ein neuronales Netz, bei dem die Informationsverarbeitung nur in eine Richtung – von den Eingabeknoten zu den Ausgabeknoten – erfolgt.","Ein einfaches FF-Netz besteht aus Perzeptrons, die direkt mit den Ausgabeknoten verbunden sind.","Ein Deep Feed Forward Network (DFF) enthält zusätzlich eine oder mehrere versteckte (Hidden) Schichten.","Die Tiefe eines DFF ergibt sich aus der Anzahl an Hidden Layern und Kanten zwischen den Neuronen."],"answerJa":["Feed Forward Network（FF）とは、情報が入力ノードから出力ノードへ一方向に伝わる構造のニューラルネットワークである。","単純なFFネットワークは、パーセプトロン（Perceptron）で構成され、隠れ層（Hidden Layer）を持たない。","Deep Feed Forward Network（DFF）は、1つ以上の隠れ層を持ち、複雑な処理が可能になる。","この「深さ」は、隠れ層の数とそれらの層を結ぶエッジ（接続）の数によって決まる。"],"explanationDe":["Bei einem Feed Forward Network wird die Information strikt von der Eingabe zur Ausgabe weitergeleitet, ohne Rückkopplungen.","Dies macht die Architektur einfach und berechenbar, aber limitiert die Fähigkeit zur Verarbeitung komplexer Daten.","Durch die Einführung zusätzlicher versteckter Schichten – wie bei Deep Feed Forward Networks – können nicht-lineare Zusammenhänge modelliert werden.","Die Tiefe eines solchen Netzes ist ein Maß für seine Komplexität und wird durch die Anzahl der Layer und der Verbindungen bestimmt.","Beispiel: Ein Netz mit 3 versteckten Schichten ist \'tiefer\' als eines mit nur einer."],"explanationJa":["Feed Forward Networkでは、情報は常に前方向に流れ、再帰的なつながり（ループ）は存在しない。","この構造により、アルゴリズムは理解しやすく、計算も比較的単純になるが、複雑なパターンの学習には限界がある。","そこで登場するのがDeep Feed Forward Network（DFF）で、複数の隠れ層を追加することで、より高度な特徴を抽出できる。","ネットワークの深さとは、これらの隠れ層と、ノード間の接続数（エッジ）の合計によって決まる。","例：2つの隠れ層しかないネットワークよりも、4つの隠れ層を持つネットワークの方が“深い”といえる。"],"originalSlideText":"– (Deep) Feed Forward Networks\\n– Propagation immer in Richtung der Ausgabeknoten\\n– Einfache Perceptrons\\n– Multilayer Perceptrons\\n– Tiefe wird über die Anzahl an Hidden Layer und Kanten erreicht","questionImage":"","explanationImage":"lecture01/lecture11_ex08.png"},{"id":21,"questionDe":"(s33) Wie funktioniert ein Deep Convolutional Network (DCN) und welche Schritte umfasst seine Architektur?","questionJa":"Deep Convolutional Network（DCN）はどのように機能し、その構成はどのようなステップで構成されているか？","answerDe":["Ein Deep Convolutional Network (DCN) besteht aus mehreren Schichten, typischerweise inklusive mindestens einer Convolution-Schicht.","In der Convolution-Schicht werden Merkmale (Features) mittels Faltung extrahiert.","Anschließend folgt meist eine Pooling-Schicht zur Reduktion der Dimension und Stabilisierung der Merkmale.","Die resultierenden Merkmale werden danach durch ein tiefes neuronales Netz weiterverarbeitet."],"answerJa":["Deep Convolutional Network（DCN）は複数の層で構成され、通常は少なくとも1つ以上の畳み込み（Convolution）層を含む。","畳み込み層では、画像などの入力データに対してフィルタ（カーネル）を適用し、特徴（Feature）を抽出する。","次に、特徴量を縮小・集約するためにプーリング（Pooling）層が続き、情報量を圧縮しつつ重要な情報を保持する。","抽出された特徴は、後続の深層ネットワーク（通常は全結合層など）で分類や識別のために処理される。"],"explanationDe":["DCNs sind besonders für Bilddaten geeignet, da sie lokal relevante Muster wie Kanten, Texturen oder Farben erkennen können.","Die Convolution-Schichten nutzen kleine Filter, die über das Eingabebild gleiten und lokale Muster detektieren.","Pooling reduziert die Auflösung, macht das Modell robuster gegen Verschiebung und spart Rechenleistung.","Nach Extraktion und Verdichtung der Informationen durch diese ersten Schichten übernimmt ein klassisches tiefes Netz (z. B. Feed Forward Network) die finale Verarbeitung – z. B. Klassifikation von Objekten im Bild."],"explanationJa":["DCN（ディープ・コンボリューショナル・ネットワーク）は、画像認識や視覚情報の処理に特化した構造で、CNN（畳み込みニューラルネットワーク）の一形態である。","畳み込み層では、フィルタを使って画像上のローカルなパターン（例えばエッジや角、模様など）を抽出する。","続くプーリング層では、最大値や平均値を取ることで空間的なサイズを縮小し、重要な情報のみを残す。","その後の深層ネットワークは、これらの抽出された特徴量を用いて分類や推論などの処理を行う。","例えば、画像中の「犬」や「猫」などを判別するタスクでは、この構成が非常に効果的である。"],"originalSlideText":"– Deep Convolutional Networks\\n– Eine oder mehrere Convolution Layer\\n– Feature Extraktion per Faltung\\n– Pooling\\n– Danach Verarbeitung durch ein tiefes Netz","questionImage":"","explanationImage":"lecture01/lecture11_ex09.png"},{"id":22,"questionDe":"(s34) Erklären Sie, wie ein Autoencoder funktioniert und welche Rolle er beim Unsupervised Learning spielt.","questionJa":"オートエンコーダはどのように機能し、教師なし学習においてどのような役割を果たすかを説明せよ。","answerDe":["Ein Autoencoder besteht aus drei Schichten: Eingabe-, Hidden- und Ausgabeschicht.","Die Eingabedaten werden in der Hidden Layer komprimiert.","Aus dieser komprimierten Repräsentation werden die Daten in der Ausgabeschicht rekonstruiert.","Der Autoencoder benötigt keine annotierten Daten – es handelt sich um Unsupervised Learning.","Er extrahiert wichtige Features oder reduziert die Dimensionalität der Daten."],"answerJa":["オートエンコーダは、入力層・中間層（隠れ層）・出力層の3層から構成される。","入力データは隠れ層で圧縮され、情報の本質的な特徴が抽出される。","その後、圧縮された情報をもとに、出力層で元のデータに近い形に再構成される。","この学習は教師なしで行われ、ラベル付きデータを必要としない。","オートエンコーダは、主に次元圧縮や特徴抽出の目的で使用される。"],"explanationDe":["Autoencoder sind neuronale Netzwerke, die lernen, Eingaben zu komprimieren und wiederherzustellen.","Die Kompression erfolgt im Hidden Layer (L2), der weniger Neuronen enthält als Eingabe (L1) und Ausgabe (L3).","Das Netzwerk versucht, die Ausgaben so zu rekonstruieren, dass sie möglichst ähnlich den Eingaben sind.","Dadurch lernt das Modell die wichtigsten Eigenschaften der Daten.","Ein Beispiel ist die Reduktion von Bilddaten auf wenige Hauptmerkmale (z. B. Gesichtserkennung)."],"explanationJa":["オートエンコーダは、与えられた入力データを一度圧縮してから再構成することで、データの本質的な特徴を学習するニューラルネットワークである。","中間層（L2）は、入力層（L1）よりも少ないユニット数で構成されており、ここで圧縮処理が行われる。","再構成された出力層（L3）のデータが、入力データとできるだけ一致するように重みが学習される。","これにより、モデルはデータの中で最も重要な特徴を抽出する。","例えば画像処理では、顔画像の輪郭や目・鼻などの重要なパーツだけを抽出することができる。"],"originalSlideText":"– Autoencoder\\n– Daten werden durch Hidden Layer komprimiert und dann durch weniger Ausgabeknoten rekonstruiert\\n– Approximation\\n– Benötigt keine annotierten Daten\\n– Unsupervised Learning\\n– Extrahiert wichtige Features oder Dimensionen aus den Daten","questionImage":"","explanationImage":"lecture01/lecture11_ex10.png"},{"id":23,"questionDe":"(s35) Nennen Sie 3 Merkmale von Recurrent Neural Networks (RNNs) und erläutern Sie ihren Nutzen bei der Verarbeitung von Sequenzen.","questionJa":"★再帰型ニューラルネットワーク（RNN）の特徴を3つ挙げ、それがシーケンス処理においてどのように役立つかを説明せよ。","answerDe":["RNNs erlauben Verbindungen zwischen Neuronen derselben oder vorherigen Schicht.","Dadurch entstehen Zyklen im Netzwerk, was eine Verarbeitung von zeitlichen Abfolgen ermöglicht.","Sie sind besonders hilfreich für sequenzielle Daten wie Bilderkennung oder maschinelle Übersetzung.","RNNs sind im Vergleich zu Feedforward-Netzen schwerer zu trainieren."],"answerJa":["★RNN（再帰型ニューラルネットワーク）は、同じ層または前の層のニューロン同士が接続される構造を持つ。","★この構造により、ネットワーク内にループ（循環）が生じ、時間的な情報を保持しながら処理できる。","RNNは、画像認識や機械翻訳など、時系列や文のようなシーケンス情報のあるデータに特に有効である。","ただし、通常のニューラルネットワークよりも学習が難しい傾向がある。"],"explanationDe":["RNNs verarbeiten Informationen nicht nur von aktuellen Eingaben, sondern behalten auch frühere Informationen im \'Gedächtnis\'.","Dies ist entscheidend für Aufgaben wie Sprachverarbeitung oder Zeitreihenanalyse, bei denen frühere Kontexte wichtig sind.","Zum Beispiel kann ein RNN bei der Übersetzung eines Satzes berücksichtigen, was bereits gesagt wurde.","Das Training ist allerdings aufwendig, da durch Rückkopplungen Gradientenprobleme wie \'vanishing gradients\' entstehen können."],"explanationJa":["RNNは、現在の入力だけでなく、過去の入力の情報をネットワーク内で記憶しながら処理することができる。","このような性質は、文章や音声など、過去の情報が意味に影響を与えるような時系列データの処理に不可欠である。","例えば、翻訳タスクにおいては、文の前半の意味を記憶しながら後半を訳す必要があるため、RNNが効果的である。","一方で、RNNは内部にループを持つため、勾配消失問題などの学習上の困難が生じやすいという課題がある。"],"originalSlideText":"- Recurrent Neural Networks\\n- Neuronen können nun auch mit Neuronen der gleichen oder vorherigen Schicht verbunden werden\\n- Zyklen\\n- Hilfreich bei Sequenzen\\n  - Bilderkennung\\n  - Sprachübersetzung\\n- Sind in Regel schwieriger zu trainieren","questionImage":"","explanationImage":"lecture01/lecture11_ex11.png"},{"id":24,"questionDe":"(s37) Was ist das Problem bei tiefen Netzen mit vielen Layern und wie hilft LSTM dieses Problem zu lösen?","questionJa":"★層の多い深いニューラルネットワークにおける問題点は何か、それに対してLSTMはどのように対処するのか説明せよ。","answerDe":["Bei tiefen Netzen wird der Fehler beim Backpropagation-Verfahren von hinten nach vorne immer weiter skaliert.","Dies führt dazu, dass der Fehler entweder verschwindet (vanishing gradient) oder explodiert (exploding gradient).","Durch dieses Problem können tiefe Netze nicht vollständig genutzt werden.","LSTM (Long Short Term Memory) ist eine Verbesserung, die dieses Skalierungsproblem adressiert und somit das Training tief verschachtelter Netze ermöglicht."],"answerJa":["★深い層を持つニューラルネットワークでは、誤差逆伝播の際に誤差が後ろから前にスケーリングされる。","★この過程で、誤差が極端に小さくなる（勾配消失）か極端に大きくなる（勾配爆発）問題が生じる。","★そのため、深層ネットワークは本来の能力を十分に発揮できない。","★LSTM（Long Short Term Memory）はこのスケーリング問題を解決し、深いネットワークでも効果的に学習できるように改善された手法である。"],"explanationDe":["Das sogenannte Vanishing-Gradient-Problem tritt auf, wenn in tiefen Netzen durch die wiederholte Ableitung der Fehlergradient mit jedem Layer kleiner wird.","Umgekehrt kann der Gradient durch wiederholte Multiplikation auch explodieren.","LSTM-Zellen beinhalten Mechanismen wie Speicher- und Vergessens-Gates, die es ermöglichen, wichtige Informationen über längere Zeiträume zu speichern und zu propagieren.","Dadurch kann LSTM effektiv mit langen Abhängigkeiten und tiefen Netzarchitekturen umgehen."],"explanationJa":["深層ニューラルネットワークでは、誤差を逆伝播する際に各層での勾配が小さくなりすぎてしまう勾配消失や、大きくなりすぎてしまう勾配爆発が問題となる。","このような現象が起きると、ネットワークはうまく学習できず、特に初期の層のパラメータが更新されないままになる。","LSTMは内部に記憶セルやゲート（入力ゲート、出力ゲート、忘却ゲート）を持つ構造で、重要な情報を選択的に保持・忘却・出力する仕組みがある。","これにより、時間的に離れた情報の依存関係も学習可能で、深層ネットでも誤差を安定的に伝播できるようになる。"],"originalSlideText":"- Long Short Term Memory\\n  - Verbesserung für Netze mit vielen Layern\\n  - Bei der Backpropagation wird der Fehler von hinten nach vorne immer weiter skaliert\\n    - Verschwindet oder Explodiert dadurch\\n  - Tiefe Netze werden dadurch nicht vollständig „genutzt“","questionImage":"","explanationImage":""},{"id":25,"questionDe":"(s38) Nenne die drei Hauptbestandteile (Gates) einer LSTM-Zelle und erläutere ihre jeweilige Funktion. Wie trägt dies zur besseren Lernbarkeit rekurrenter Netze bei?","questionJa":"LSTMセルの3つの主要なゲート（Input Gate, Forget Gate, Output Gate）とその役割を説明せよ。また、それが再帰型ネットワーク（RNN）の学習性向上にどう寄与するか述べよ。","answerDe":["Input Gate steuert den Effekt eines neuen Wertes auf die Zelle C.","Forget Gate bestimmt, wie stark ein Wert in der Zelle bleibt oder vergessen wird.","Output Gate kontrolliert, wie stark der Zellzustand auf das nächste Neuron wirkt.","Dadurch bleibt der Fehler über die Zeit konstanter erhalten.","Rekurrente Netze werden dadurch besser lernbar."],"answerJa":["Inputゲートは、新しい値がセル状態（C）に与える影響を制御する。","Forgetゲートは、セル内の情報をどの程度保持または忘却するかを決定する。","Outputゲートは、セル状態が次のニューロンへどの程度影響を与えるかを制御する。","これらにより、誤差が時間的により安定して保たれ、長期的依存関係も学習可能となる。","その結果、再帰型ニューラルネットワーク（RNN）は学習しやすくなる。"],"explanationDe":["In einer klassischen RNN-Zelle wird der Fehler bei der Rückpropagation oft instabil – er verschwindet oder explodiert.","Die LSTM-Zelle nutzt drei Arten von Gates, die jeweils mit einer Sigmoid-Aktivierungsfunktion arbeiten.","Das Input Gate entscheidet, welche Informationen in den Zellzustand aufgenommen werden sollen.","Das Forget Gate gibt an, welche bisherigen Informationen vergessen werden sollen.","Das Output Gate bestimmt, welcher Anteil des Zellzustands an das nächste Neuron weitergegeben wird.","Durch diese gezielte Informationssteuerung kann ein LSTM-Netzwerk längere Abhängigkeiten lernen."],"explanationJa":["通常のRNNでは、時間が長くなると誤差が減衰または爆発し、学習が困難になる問題がある。","LSTMはこの問題を3つのゲートを通じて解決する。","Inputゲートは、どの情報をセル状態に新たに取り込むかを制御する。","Forgetゲートは、どの情報を保持し、どの情報を忘れるかを決定する。","Outputゲートは、セル状態のどの部分を出力として次の層に伝えるかを決める。","これにより、情報の保持と伝播が長期にわたって安定し、長期依存関係のあるデータに対しても効果的に学習できるようになる。"],"originalSlideText":"- Input Gate steuert den Effekt, den ein neuer Wert auf die Zelle C hat\\n- Forget Gate steuert den Effekt, wie stark ein Wert in der Zelle bleibt oder vergessen wird\\n- Output Gate steuert den Effekt, wie stark dieser Effekt im nächsten Neuron wirken soll\\n- Hierdurch bleibt der Fehler konstanter erhalten\\n- Rekurrente Netze sind besser lernbar","questionImage":"","explanationImage":""},{"id":26,"questionDe":"(s39) Erkläre den Aufbau und das Trainingsprinzip von Generative Adversarial Networks (GANs). Welche Anwendungsmöglichkeiten gibt es?","questionJa":"★GAN（敵対的生成ネットワーク）の構造と学習原理を説明せよ。また、その応用例を述べよ。","answerDe":["GANs bestehen aus zwei Netzen: einem Generator und einem Erkenner (Discriminator).","Der Generator erzeugt Ausgaben aus Zufallsdaten.","Der Erkenner erhält sowohl echte als auch generierte Daten und soll diese unterscheiden.","Wenn der Erkenner echte Daten fälschlicherweise als generiert klassifiziert, wird er entsprechend trainiert.","Wenn er generierte Daten korrekt als generiert erkennt, wird der Generator trainiert, um bessere Daten zu erzeugen.","Beide Netze verbessern sich im Laufe der Trainings-Epochen.","GANs werden u.a. für die Bild-, Text- oder Spiele-KI-Erzeugung eingesetzt."],"answerJa":["★GANは生成器（Generator）と識別器（Discriminator）の2つのネットワークで構成される。","★生成器はランダムな入力からデータ（例：画像）を生成する。","★識別器は本物のデータと生成されたデータを見分けるよう訓練される。","★本物のデータを偽物と誤って分類した場合は、識別器が修正される。","★生成されたデータを偽物と正しく分類された場合は、生成器が改善されるよう学習する。","★この対抗的な関係により、両方のネットは繰り返し学習して精度を高める。","応用例には画像生成、テキスト生成、ゲームAIなどがある。"],"explanationDe":["Ein Generative Adversarial Network (GAN) ist ein neuronales Netzwerk-System aus zwei Komponenten: dem Generator und dem Diskriminator.","Der Generator erzeugt künstliche Daten aus Zufallswerten, z. B. Bilder, die wie reale Fotos aussehen sollen.","Der Diskriminator bewertet, ob ein gegebenes Beispiel real oder künstlich ist.","Das Besondere: Beide Netze lernen gleichzeitig – der Diskriminator wird besser im Erkennen, der Generator besser im Täuschen.","Fehlklassifikationen in beiden Richtungen führen zu Verbesserungen durch Backpropagation.","Diese Technik hat sich z. B. bei der Generierung realistischer Bilder, Deepfakes oder Spielinhalten bewährt."],"explanationJa":["GAN（敵対的生成ネットワーク）は、生成器と識別器という2つのネットワークからなるモデルである。","生成器は、例えばノイズから画像やテキストなどのデータを作り出す。","識別器は、それが本物のデータか生成された偽物かを分類する。","学習中、識別器が誤って本物を偽物と判断すると、それを修正するように学習される。","一方、生成器は識別器を欺けるようなよりリアルなデータを生成することを目指して訓練される。","このような対立する目的のもとで両者が交互に学習を繰り返すことで、最終的に非常に高品質なデータ生成が可能になる。","GANは画像生成（例：人物の顔）、文章生成（例：詩の自動生成）、ゲームのAI生成など多くの分野で利用されている。"],"originalSlideText":"- Kombination von Netzen\\n- Ein Netz generiert aus Zufallsdaten eine Ausgabe\\n- Ein zweites Netz bekommt „echte“ und generierte Daten präsentiert\\n- Backpropagation in beiden Netzen, je nach Fehlerart\\n- „Echter“ Datenpunkt wird als generiert klassifiziert\\n- Erkennungsnetz wird auf den Fehler trainiert\\n- Generierter Datensatz wird als generiert klassifiziert\\n- Generatornetz wird auf den Fehler trainiert\\n- Beide Netze werden durch die Epochen stetig besser\\n- Nutzung bei Bildern, Texten oder Spiel-KI","questionImage":"","explanationImage":""},{"id":40,"questionDe":"Welche Aspekte sind bei der Evaluierung von neuronalen Netzen zu beachten? Nenne mindestens 4 relevante Punkte.","questionJa":"★ニューラルネットワークの評価において注意すべき点を4つ以上挙げよ。","answerDe":["Ergebnisse können mit Hilfe von Trainings-, Evaluierungs- und Testdatensätzen überprüft werden.","Die Evaluierung orientiert sich am Kapitel zur allgemeinen Modellbewertung.","Die Qualität der Ergebnisse hängt stark von der Netzwerktopologie ab.","Die Auswahl der Topologie basiert oft auf Erfahrung.","In der Praxis werden häufig bewährte Topologien verwendet.","Auch die Verkettung mehrerer Netzwerke kann die Bewertung beeinflussen."],"answerJa":["★ニューラルネットワークの評価は、訓練データ、評価データ、テストデータを用いて行われる。","評価方法は、一般的なモデル評価の方法に準拠している。","評価結果はネットワークのトポロジー（構造）に大きく依存する。","トポロジーの選択は経験に基づくことが多い。","一般的によく知られたトポロジーが用いられることが多い。","複数の異なるネットワークの連結（例えば、CNNとRNNの組み合わせ）も評価結果に影響を与える。"],"explanationDe":["Die Evaluierung neuronaler Netze umfasst mehrere Phasen: Zunächst wird das Modell mit einem Trainingsdatensatz trainiert.","Anschließend wird es mit einem Evaluierungsdatensatz überprüft, um die Lernfortschritte und Überanpassung zu beobachten.","Schließlich erfolgt die abschließende Bewertung mit einem unabhängigen Testdatensatz.","Wichtig ist zudem die Netzstruktur: Unterschiedliche Architekturen führen zu unterschiedlichen Leistungen.","Die Wahl einer geeigneten Topologie erfordert oft Erfahrungswerte oder orientiert sich an etablierten Designs wie z. B. CNNs oder LSTMs.","Manchmal wird die Leistung durch das Kombinieren mehrerer Netztypen verbessert (z. B. Autoencoder + Klassifikator)."],"explanationJa":["ニューラルネットワークの評価は、学習データ、検証データ、テストデータの3種類のデータセットを用いて段階的に行われる。","学習データで学習を行い、検証データで途中経過の性能や過学習の有無を確認し、最終的にテストデータで一般化性能を測定する。","また、ネットワークの構造（トポロジー）によって性能が大きく変わることにも注意が必要である。","例えば、単純な全結合ネットワークよりもCNN（畳み込みニューラルネットワーク）の方が画像処理に適している。","このような構造の選択には経験が必要で、実際には既存のよく知られたネットワーク構造が多く使われる。","さらに、異なる種類のネットワークを組み合わせる（例：畳み込み層とリカレント層）ことでより高い性能が得られる場合もある。"],"originalSlideText":"- Evaluierung von neuronalen Netzen\\n- Ergebnisse können wie im Kapitel Evaluierung geprüft werden\\n  - Trainingsdatensatz\\n  - Evaluierungsdatensatz\\n  - Testdatensatz\\n- Ergebnisse sind aber abhängig von der Topologie des Netzes\\n  - Auswahl ist Erfahrung\\n  - Meistens werden bekannte Topologien genutzt\\n  - Verkettung von verschiedenen Netzen","questionImage":"","explanationImage":""},{"id":41,"questionDe":"Nenne 4 Herausforderungen bei der Evaluierung und Nachvollziehbarkeit neuronaler Netze.","questionJa":"★ニューラルネットワークの評価や説明可能性に関する課題を4つ挙げよ。","answerDe":["Die Extraktion wichtiger Attribute und Eigenschaften ist schwierig.","Konvolution und Übertragungsfunktionen fassen mehrere Attribute zusammen, sodass deren Einfluss nicht direkt sichtbar ist.","Neuronale Netze modellieren komplexe Zusammenhänge, deren Offenlegung für Forschung und Praxis relevant ist.","Es bestehen Sicherheitsaspekte: Wer haftet für Fehlentscheidungen? Sind die Ergebnisse nachvollziehbar?"],"answerJa":["★重要な属性や特徴の抽出が難しい。","★畳み込みや伝達関数が複数の属性をまとめて扱うため、個別の影響が直接には読み取れない。","★ニューラルネットワークは複雑な関係をモデル化するため、その構造の公開は研究や実用上重要である。","★安全性の観点からの課題もあり、誤った判断に対して誰が責任を負うのか、結果が説明可能かどうかが問われる。"],"explanationDe":["Neuronale Netze sind zwar leistungsfähig, jedoch ist ihre Funktionsweise schwer nachvollziehbar.","Das liegt daran, dass einzelne Eingaben oft durch mehrere Transformationen wie Konvolution und Aktivierungsfunktionen kombiniert werden.","Daher ist es schwierig, den Einfluss eines bestimmten Merkmals auf die finale Entscheidung zu erkennen.","Dies wirft ethische und rechtliche Fragen auf – insbesondere im Hinblick auf Sicherheit, Verantwortung und Nachvollziehbarkeit von Entscheidungen."],"explanationJa":["ニューラルネットワークは非常に強力な予測モデルだが、その内部の処理は非常に複雑で、理解が難しい。","一つの属性がネットワーク内で多数の計算処理（例：畳み込み、活性化関数）を通ることで、最終的な出力への影響が分かりづらくなる。","このため、何が最終判断にどのように影響したのかを明示するのが困難であり、説明責任や法的責任が曖昧になるリスクがある。","そのため、AIの倫理や安全性に関する議論では、説明可能性（Explainability）や透明性（Transparency）が重要なテーマとなっている。"],"originalSlideText":"- Evaluierung\\n- Extraktion der wichtigen Attribute und Eigenschaften hingegen schwierig\\n  - Konvolution und Übertragungsfunktionen fassen Attribute zusammen\\n  - Einfluss eines einzelnen Attributes lässt sich nicht direkt ablesen\\n- Aber: Neuronale Netze modellieren komplexe Zusammenhänge\\n  - Offenlegung dieser für weitere Forschung/Arbeit interessant\\n- Sicherheitsaspekte\\n  - Wer haftet für Fehlentscheidungen?\\n  - Sind die Ergebnisse nachvollziehbar?","questionImage":"","explanationImage":""},{"id":42,"questionDe":"Nenne 3 zentrale Anliegen der Explainable Artificial Intelligence (XAI) und erläutere deren Bedeutung.","questionJa":"★説明可能な人工知能（XAI）の3つの主要な目的を挙げ、それぞれの重要性を説明せよ。","answerDe":["Erstellung von erklärbaren Modellen mit gleichzeitig hoher Genauigkeit.","Zugriffsmöglichkeiten im Modell für den Nutzer (Human in the Loop).","Transparenz der genutzten Datenverarbeitung."],"answerJa":["★高い精度を保ちながらも説明可能なモデルの構築。","★ユーザーがモデルの処理に関与できるような仕組み（Human in the Loop）。","★データ処理の透明性を確保すること。"],"explanationDe":["Explainable AI (XAI) ist ein relativ neues Forschungsfeld, das seit 2004 entwickelt wird.","Es zielt darauf ab, die sogenannte \'Blackbox\' von komplexen Modellen zu öffnen und nachvollziehbar zu machen – etwa durch sogenannte \'Glassbox\'-Verfahren.","Gerade bei nichtlinearen Lernverfahren ist es wichtig, Erklärbarkeit mit hoher Modellgenauigkeit zu kombinieren.","Durch XAI können Nutzer (z.B. Ärzte oder Richter) aktiv in den Entscheidungsprozess eingebunden werden.","Auch gesetzliche Anforderungen wie die DSGVO verlangen Transparenz und Nachvollziehbarkeit von Entscheidungen automatisierter Systeme."],"explanationJa":["XAI（説明可能なAI）は、2004年以降に発展してきた比較的新しい研究分野である。","その目的は、複雑で不透明な“ブラックボックス”モデルの内部を可視化し、説明可能にすることである（“グラスボックス”手法など）。","特に非線形の学習モデルでは、精度と説明性の両立が求められる。","XAIにより、医師や裁判官などのユーザーがAIの判断に関与し、最終決定に責任を持てるようになる（Human in the Loop）。","さらに、EU一般データ保護規則（GDPR/DSGVO）などの法律により、透明性と説明責任がAIに求められている。"],"originalSlideText":"- Explainable Artificial Intelligence (XAI)\\n- Relativ neues Forschungsfeld seit 2004\\n- Öffnen des Blackbox Verfahrens\\n- Sog. „Glassbox“ Verfahren\\n- Insbesondere für nicht lineare Lernverfahren\\n- Mittlerweile auch im Fokus der Gesetzgebung (z.b. DSGVO)\\n- XAI beinhaltet Themen wie:\\n  - Erstellung von erklärbaren Modellen, welche aber auch eine hohe Genauigkeit haben\\n  - Zugriffsmöglichkeiten im Modell für den Nutzer (human in the loop)\\n  - Transparenz der genutzten Datenverarbeitung","questionImage":"","explanationImage":""},{"id":43,"questionDe":"(s47)Welche Herausforderungen und Vorteile bietet die visuelle Darstellung neuronaler Netze? Nenne mindestens 3 Aspekte.","questionJa":"★ニューラルネットワークの視覚的な表現にはどのような課題と利点があるか。少なくとも3点を挙げよ。","answerDe":["Darstellung als Graph mit Layern ermöglicht ein strukturelles Verständnis des Netzes.","Kantengewichte können visuell kodiert werden (z. B. farblich nach Stärke), was hilft, starke oder schwache Pfade zu erkennen.","Skalierungsprobleme entstehen bei sehr tiefen oder großen Netzwerken, da die Visualisierung unübersichtlich wird."],"answerJa":["★層構造を持つグラフとして可視化することで、ネットワークの構造的理解が可能となる。","★エッジ（結合）の重みを強さによって色分けするなど、視覚的に表現できるため、強い経路や弱い経路を把握しやすい。","★ネットワークが深くなったり大規模になると、視覚化が複雑になり、スケーリングの課題が生じる。"],"explanationDe":["Neuronale Netze bestehen aus vielen verbundenen Einheiten (Neuronen), die typischerweise in Schichten (Layern) organisiert sind.","Eine visuelle Darstellung dieser Architektur hilft dabei, die Verbindungen und Gewichtungen zwischen den Einheiten zu analysieren.","Besonders wichtig ist es, starke oder schwache Pfade im Netzwerk zu identifizieren, um z. B. Engpässe oder überdominante Verbindungen zu erkennen.","Allerdings stößt man bei großen Netzwerken auf Skalierungsprobleme – die Darstellung wird unübersichtlich und schwer interpretierbar."],"explanationJa":["ニューラルネットワークは通常、複数の層（レイヤー）に分かれた多数のニューロンによって構成されている。","その構造をグラフとして可視化することで、各ニューロン間の接続や重みを把握することができる。","特に、どの経路が情報伝達において強く影響を及ぼしているか（強い経路）、またはあまり貢献していないか（弱い経路）を分析する手がかりになる。","しかし、層が増えたりノード数が多くなると、図が複雑化し、視覚的に把握しづらくなるスケーリングの課題がある。"],"originalSlideText":"- Darstellung des Netzes\\n- Visuelle Darstellung als Graph mit Layern\\n- Kantengewichte werden nach Ausprägung eingezeichnet\\n- Welche Pfade sind besonders schwach/stark?\\n- Skalierungsprobleme","questionImage":"","explanationImage":"lecture01/lecture11_ex13.png"},{"id":44,"questionDe":"Was ist eine Salience Map und welche Rolle spielt sie bei der Bildverarbeitung? Nenne mindestens 2 Funktionen.","questionJa":"Salience Map（サリエンスマップ）とは何か。また、画像処理においてどのような役割を果たすか。少なくとも2つの機能を挙げよ。","answerDe":["Eine Salience Map markiert die Bereiche in den Daten, die für die Klassifikation besonders wichtig waren.","Sie ist besonders in der Bildverarbeitung nützlich, um relevante Bildbereiche hervorzuheben.","Das Netz kann die Klassifikationsentscheidung rückrechnen, um zu zeigen, welche Neuronen besonders stark aktiviert wurden."],"answerJa":["Salience Map（サリエンスマップ）は、分類において特に重要だったデータの領域を強調して示すものである。","画像処理において有用で、分類の根拠となった画像の特徴的領域を視覚的に把握できる。","ネットワークは分類結果をもとに逆方向に計算を行い、特に強く活性化されたニューロンを特定することができる。"],"explanationDe":["Salience Maps sind Visualisierungstechniken in neuronalen Netzen, die hervorheben, welche Teile der Eingabedaten (z. B. ein Bild) besonders zur Entscheidungsfindung beigetragen haben.","In der Praxis wird dies verwendet, um zu verstehen, welche Bildbereiche das Netz zur Erkennung eines Objekts wie z. B. eines Frosches herangezogen hat.","Man kann so nachverfolgen, welche Neuronen in welcher Schicht besonders stark auf bestimmte Merkmale reagiert haben.","Dadurch trägt die Salience Map zur Interpretierbarkeit neuronaler Netzwerke bei."],"explanationJa":["Salience Map（サリエンスマップ）は、ニューラルネットワークが分類の際にどの入力データの部分に注目したのかを示す視覚化手法である。","たとえばカエルの画像を分類する場合、ネットワークがカエルのどの部分（輪郭や目など）を分類の手がかりにしたかを視覚的に示すことができる。","これはネットワーク内で特に強く反応したニューロンの位置を示すことで実現される。","そのため、ネットワークの判断根拠を明示することで、モデルの解釈性を向上させる役割がある。"],"originalSlideText":"- Salience Map\\n- Markierung der Bereiche in den Daten, die besonders wichtig für die Klassifikation waren\\n- Insbesondere in der Bildverarbeitung nützlich\\n- Rückrechnung des Klassifikationsergebnisses durch das Netz\\n- Identifikation von Neuronen, die besonders stark aktiviert wurden","questionImage":"","explanationImage":""},{"id":45,"questionDe":"(s48) Wie funktioniert die Layer-wise Relevance Propagation (LRP) im Zusammenhang mit Salience Maps und welche Erkenntnisse lassen sich daraus gewinnen?","questionJa":"Salience MapにおけるLayer-wise Relevance Propagation（LRP）はどのように機能するか。また、そこからどのような情報が得られるか説明せよ。","answerDe":["Die Layer-wise Relevance Propagation (LRP) analysiert das Klassifikationsergebnis schichtweise rückwärts.","Dabei wird der Einfluss jedes einzelnen Neurons auf das Ergebnis berechnet und zurückverfolgt.","Eingangsneuronen erhalten einen Relevanzwert, der ihre Bedeutung für das Ergebnis zeigt.","Es kann ein \'Negativ\' der Eingabedaten erstellt werden, um zu visualisieren, welche Teile besonders wichtig waren."],"answerJa":["Layer-wise Relevance Propagation（LRP）は、分類結果を各層ごとに遡って解析する手法である。","ネットワークの各ニューロンが結果にどの程度貢献したかを数値化して算出し、それを前の層に伝播させていく。","最終的に入力層の各ニューロンに対して、分類における重要度（Relevanz）が割り当てられる。","入力データに対して“ネガ”のような視覚的マップを作成することで、どの部分がどのように影響を与えたかが比較可能になる。"],"explanationDe":["Layer-wise Relevance Propagation (LRP) ist eine Technik, um Entscheidungen neuronaler Netze interpretierbarer zu machen.","Das Verfahren analysiert schrittweise rückwärts, wie stark jede Neuronenaktivität zum Klassifikationsergebnis beigetragen hat.","Durch LRP kann man visuell darstellen, welche Eingabedaten (z. B. Pixel in einem Bild) entscheidend für die Klassifizierung waren.","Das ermöglicht ein \'Negativ\'-Bild, bei dem irrelevante Bereiche unterdrückt und relevante hervorgehoben werden."],"explanationJa":["Layer-wise Relevance Propagation（LRP）は、ニューラルネットワークの判断根拠を可視化するための手法である。","出力結果から順に、各ニューロンが結果にどれだけ貢献したかを逆伝播させながら計算する。","その結果、入力層の各ユニットに対して、その入力がどれほど分類結果に影響したかが「Relevance（関連度）」として与えられる。","たとえば画像認識であれば、画像のどのピクセルが重要であったかを可視化でき、元の画像との比較により、判断の説明性が高まる。"],"originalSlideText":"- Beispielsweise durch die „Layer-wise Relevance Propagation“\\n- Klassifikationsergebnis wird Layerweise auf den Einfluss der einzelnen Neuronen analysiert\\n- Eingangsneuronen erhalten damit einen Wert für ihre Bedeutung\\n- Erstellung eines „Negativs“ der Eingabedaten\\n- Vergleich der „Bilder“","questionImage":"","explanationImage":"lecture01/lecture11_ex14.png"},{"id":46,"questionDe":"(s49) Welche Herausforderungen bestehen bei der Anwendung von Salience Maps auf verschiedene Datentypen?","questionJa":"Salience Mapを異なるデータ型（例：画像、テキスト、高次元データ）に適用する際に、どのような課題があるか3つ挙げよ。","answerDe":["Salience Maps funktionieren gut mit Bilddaten, da die relevanten Bildbereiche visuell hervorgehoben werden können.","Bei Textdaten ist eine Anwendung zwar möglich, aber die erkannten Zusammenhänge sind oft schwer interpretierbar.","Die Auswertung von hochdimensionalen Daten bleibt problematisch, da Relevanzen schwer darstellbar und verständlich sind."],"answerJa":["Salience Mapは画像データとの相性が良く、分類に重要な領域を視覚的に強調することができる。","テキストデータにも適用可能ではあるが、重要性の関係性が曖昧で解釈が困難になりやすい。","高次元データにおいては、関連度を可視化・評価するのが難しく、解析に課題が残る。"],"explanationDe":["Salience Maps bieten eine Möglichkeit zur Erklärung von Klassifikationsergebnissen, indem sie wichtige Eingabebereiche markieren.","Bei Bildern ist dies besonders nützlich, da Pixelbereiche direkt visualisiert werden können.","In Textdaten ist jedoch schwer nachvollziehbar, warum ein bestimmtes Wort wichtig war, da der Zusammenhang oft nicht direkt sichtbar ist.","Noch schwieriger ist es bei hochdimensionalen Daten wie Vektorrauminformationen oder Sensordaten, da diese nicht intuitiv darstellbar sind."],"explanationJa":["Salience Mapは、分類モデルがどの部分を重視して判断したかを可視化する技術である。","画像データではピクセルごとの重要度を可視化できるため、有用性が高い。","しかし、テキストデータでは単語ごとの意味的関連が明示されず、なぜその語が重要とされたのかが見えにくい。","さらに高次元のデータでは、次元が多すぎて重要性を視覚的に表現するのが難しくなる。"],"originalSlideText":"- Salience Map\\n- Funktioniert gut mit Bilddaten\\n- Textdaten sind auch möglich, Zusammenhänge werden aber unklar\\n- Auswertung von hochdimensionalen Daten weiterhin problematisch\\n- https://lrpserver.hhi.fraunhofer.de/handwriting-classification\\n- https://lrpserver.hhi.fraunhofer.de/text-classification","questionImage":"","explanationImage":""},{"id":47,"questionDe":"(s51) Welche Möglichkeiten bietet die Occlusion Based Feature Extraction zur Interpretation neuronaler Netze? Nennen Sie 3 konkrete Anwendungsaspekte.","questionJa":"Occlusion Based Feature Extraction（オクルージョンによる特徴抽出）を用いてニューラルネットの判断根拠を可視化する際に得られる利点を3つ挙げよ。","answerDe":["Durch das Abdecken einzelner Bildbereiche kann identifiziert werden, welche Regionen für die Klassifikation besonders wichtig sind.","Unwichtige Merkmale können gezielt ausgeschlossen werden, um die Modellinterpretierbarkeit zu verbessern.","Die Methode erlaubt es, Fehlklassifikationen zu erkennen, etwa wenn ein Zug nur über die Gleise oder das Logo identifiziert wird."],"answerJa":["画像の一部を意図的に隠すことで、分類において重要な領域を特定できる。","重要でない特徴（ノイズなど）を除外することで、モデルの解釈性を高めることができる。","モデルが意図しない特徴（例：線路やロゴ）に依存して誤分類していることを発見するのに役立つ。"],"explanationDe":["Occlusion Based Feature Extraction bedeutet, dass man ein Bild systematisch mit einer Maske überdeckt und beobachtet, wie sich die Ausgabewahrscheinlichkeit des neuronalen Netzes verändert.","Sinkt die Vorhersagewahrscheinlichkeit stark beim Abdecken eines bestimmten Bereichs, ist dieser Bereich offensichtlich entscheidend für die Klassifikation.","Ein Zug wird fälschlicherweise über die Gleise erkannt, und ein Auto über ein Markenlogo – die durch Occlusion sichtbar gemachten Regionen zeigen, dass das Modell sich auf diese irrelevanten Teile verlässt.","Das rechte Bild im Slide zeigt ein Beispiel mit einer Röntgenaufnahme: Indem einzelne Bildbereiche (z. B. Lungenbereich) abgedeckt werden, lässt sich erkennen, welche Regionen das Modell zur Diagnose heranzieht. So kann überprüft werden, ob das Modell die relevanten medizinischen Merkmale tatsächlich erkennt.","Dadurch können problematische Entscheidungsgrundlagen entdeckt und Modelle gezielt verbessert werden."],"explanationJa":["Occlusion Based Feature Extractionとは、画像の一部を順番に隠してモデルの出力（分類スコア）がどのように変化するかを調べる方法である。","もし隠した部分が予測スコアの大幅な低下を引き起こした場合、その領域は分類にとって重要であると推定できる。","列車を分類する際に線路に注目してしまったケースや、自動車をブランドロゴによって誤認したケースがある。赤く示された領域がモデルが注目していた箇所である。","スライド右の画像はX線写真におけるオクルージョンの例で、重要な領域（肺など）を隠したときにモデルの信頼度が下がるかを可視化している。これにより、モデルが適切な医療的判断根拠を使っているかが確認できる。","このように、オクルージョンはモデルが注目すべきでない領域に依存していないかを検出し、性能向上や信頼性評価に貢献する。"],"originalSlideText":"- Occlusion Based Feature Extraction\\n- Abdecken einzelner Bildbereiche\\n- Identifikation wichtiger Regionen\\n- Ausschluss „unwichtiger“ Merkmale\\n- Zugidentifikation über die Gleise\\n- Autoidentifikation über das Markenlogo","questionImage":"","explanationImage":"lecture01/lecture11_ex15.png"},{"id":48,"questionDe":"(s53) Wie funktioniert das Class Activation Mapping (CAM) und welche Funktion übernimmt dabei das Global Average Pooling?","questionJa":"Class Activation Mapping（CAM）はどのように機能するか。また、Global Average Poolingはその中でどのような役割を果たすか。","answerDe":["Beim Class Activation Mapping (CAM) werden die Ausgaben eines Convolutional Neural Networks (CNN) mit einem Global Average Pooling (GAP) Layer kombiniert.","Der GAP-Layer reduziert jede Feature Map zu einem einzigen Wert (Durchschnitt), bevor diese Werte zur Klassifikation verwendet werden.","Durch Gewichtung und Addition dieser Mittelwerte kann eine klassenbezogene Salience Map erstellt werden, die relevante Bildbereiche lokalisiert."],"answerJa":["Class Activation Mapping（CAM）は、CNNの出力特徴マップに対してGlobal Average Pooling（GAP）を適用することで、分類に寄与した領域を特定する手法である。","GAPは各特徴マップを1つの平均値に変換し、これを分類層に渡す。","分類時に用いられる重みを各特徴マップに適用し、すべてを足し合わせることで、どの画像領域がどのクラスに影響したかを示すSalience Map（重要度マップ）が得られる。"],"explanationDe":["Class Activation Mapping (CAM) ermöglicht die visuelle Identifikation relevanter Bildbereiche für eine bestimmte Klassifikation.","Dazu wird vor der Klassifikation ein Global Average Pooling (GAP) Layer eingeführt, der die Ausgabe jeder Convolution-Map auf einen Mittelwert reduziert.","Diese Mittelwerte werden dann mit den Gewichten der Zielklasse multipliziert und aufsummiert, was zu einer Karte führt, die die Aktivierung der relevanten Regionen im Bild zeigt."],"explanationJa":["CAMは、画像のどの部分があるクラスの分類に強く寄与したかを可視化する手法である。","その前提として、Global Average Pooling（GAP）により各特徴マップの平均が求められ、これが分類に使われる。","分類層の重みを使って、それぞれの特徴マップに寄与度を掛け合わせて足し合わせることで、最終的に画像の中で重要な領域が強調される。これは画像内での物体の局在化にも応用できる。"],"originalSlideText":"- Class Activation Mapping\\n- Nutzung eines Global Average Pooling Layers vor der Klassifikation\\n- Addition der einzelnen Pooling Ergebnisse, um klassenspezifische Salience Map zu erstellen\\n- Kann auch zur Lokalisierung im Bild genutzt werden","questionImage":"","explanationImage":""},{"id":49,"questionDe":"(s56) Welche zwei Herausforderungen ergeben sich bei der Auswertung hochdimensionaler Ausgaben neuronaler Netze in der Multiklassenklassifikation?","questionJa":"多クラス分類（Multiklassenklassifikation）において、ニューラルネットワークの出力が高次元になることによって生じる課題を2つ挙げよ。","answerDe":["1. Die Ausgaben von neuronalen Netzen bei der Bild- oder Textklassifikation können hochdimensional sein, was die Analyse erschwert.","2. Es ist schwierig zu erkennen, wie stark ein einzelnes Feature in den Daten enthalten ist."],"answerJa":["1. ニューラルネットの出力（特に画像やテキストの分類）は高次元になることが多く、分析や理解が難しい。","2. 出力ベクトル内の各特徴がデータにどの程度関係しているかを直接把握するのが困難である。"],"explanationDe":["In der Multiklassenklassifikation gibt das neuronale Netz oft einen Vektor mit vielen Werten aus – zum Beispiel für jede mögliche Klasse einen Wahrscheinlichkeitswert.","Solche Ausgaben sind besonders bei der Bildklassifikation und Textklassifikation üblich, da dort sehr viele Merkmale oder Kategorien beteiligt sein können.","Diese hohe Dimensionalität erschwert es, direkt zu erkennen, welche Features wirklich relevant sind oder warum eine bestimmte Entscheidung getroffen wurde."],"explanationJa":["多クラス分類では、ニューラルネットワークは通常、各クラスに対するスコア（確率など）を含んだベクトルを出力する。","このような出力は、特に画像分類やテキスト分類のように多くのカテゴリや特徴があるタスクで高次元になることが多い。","高次元の出力は、どのクラスが重要なのか、どの特徴に注目したのかを直感的に理解するのを難しくする。"],"originalSlideText":"- Multiklassenklassifikation\\n- Auch Ergebnisse von neuronalen Netzen können hochdimensional sein\\n- Bildklassifikation\\n- Textklassifikation\\n- Ausgabe ist dann ein Vektor mit Werten, wie stark ein einzelnes Feature in den Daten enthalten ist","questionImage":"","explanationImage":""},{"id":50,"questionDe":"(s56) Welche zwei Methoden können verwendet werden, um die Interpretation hochdimensionaler Ausgabevektoren neuronaler Netze zu erleichtern?","questionJa":"ニューラルネットワークの高次元な出力ベクトルの解釈を助けるために使える2つの手法を挙げよ。","answerDe":["1. Anwendung von Dimensionsreduktionsmethoden wie PCA oder t-SNE.","2. Visuelle Analyse von Clustern zur Bewertung der Trennung und Auffälligkeiten."],"answerJa":["1. PCAやt-SNEなどの次元削減手法を使って、出力ベクトルを2〜3次元に可視化する。","2. クラスタを視覚的に分析し、どのクラスがはっきりと分かれているか、異常なパターンがあるかを確認する。"],"explanationDe":["Hochdimensionale Ausgabevektoren lassen sich schwer analysieren.","Durch Methoden wie PCA (Principal Component Analysis) oder t-SNE (t-distributed stochastic neighbor embedding) kann man diese Vektoren auf 2D oder 3D reduzieren.","So kann man Cluster erkennen, deren Trennung bewerten und eventuell fehlerhafte oder ungewöhnliche Daten entdecken."],"explanationJa":["高次元の出力はそのままでは解釈が難しいため、PCA（主成分分析）やt-SNE（確率的近傍埋め込み）などの手法で次元を下げることが有効である。","次元を下げることで、似た出力同士がどのようにグループ化されるか（クラスタリング）を視覚的に把握できるようになる。","また、他と離れたクラスタがある場合、それは分類ミスや新しいパターンを示している可能性がある。"],"originalSlideText":"- Anwendung von Dimensionsreduktionsmethoden auf den Ausgabevektoren\\n- Visuelle Analyse auf Cluster\\n- Wie trennscharf sind die Cluster separiert?\\n- Gibt es ungewöhnliche Cluster, die nicht passen?","questionImage":"","explanationImage":""},{"id":51,"questionDe":"(s58) Was ist TCAV (Testing with Concept Activation Vectors) und welche Vorteile bietet es im Vergleich zu Salience Maps? Nennen Sie 3 Aspekte.","questionJa":"TCAV（Concept Activation Vectorsを用いたテスト）とは何か、またSalience Mapと比べたときの利点を3つ挙げよ。","answerDe":["1. TCAV analysiert Konzepte wie z. B. \'Streifen\' bei einem Zebra, anstatt einzelne Pixel zu bewerten.","2. TCAV liefert ein besseres Verständnis darüber, wie gut ein Netz generalisiert, indem es auf allgemeine Eigenschaften fokussiert.","3. Es wurde bisher hauptsächlich auf Bilddaten getestet, hat aber Potenzial für andere Bereiche."],"answerJa":["1. TCAVは、ゼブラにおける「ストライプ」のような概念的特徴を分析するものであり、Salience Mapのようにピクセル単位ではなく、概念に着目する。","2. データの一般的な性質（概念）に注目するため、ネットワークの一般化能力をより深く理解できる。","3. 現時点では画像データに限って試されているが、他分野への応用の可能性もある。"],"explanationDe":["TCAV (Testing with Concept Activation Vectors) ist eine Methode, um zu messen, wie wichtig bestimmte menschlich verständliche Konzepte für eine Klassifikation im neuronalen Netz sind.","Zum Beispiel kann überprüft werden, wie stark das Konzept \'Streifen\' bei der Klassifikation von Zebras eine Rolle spielt.","Im Gegensatz zu Salience Maps, die auf einzelne Pixel schauen, betrachtet TCAV höhere, abstraktere Eigenschaften der Daten. Dadurch kann man besser verstehen, warum ein Netz eine bestimmte Entscheidung getroffen hat und wie gut es generalisiert.","Allerdings wurde TCAV bisher fast nur mit Bilddaten verwendet."],"explanationJa":["TCAV（Testing with Concept Activation Vectors）は、ネットワークの判断が人間が理解できるような『概念』にどれだけ影響されているかを調べる手法である。","たとえば、ゼブラの分類において「ストライプ（縞模様）」という概念がどれだけ重要かを定量的に評価できる。","Salience Mapが画像中のピクセルに注目するのに対し、TCAVはもっと抽象的・概念的な特徴（例：模様、色、形）に着目する。そのため、モデルの一般化性能や概念理解に関する洞察が得やすい。","ただし、TCAVはまだ画像データでの実験に限られており、今後他のデータへの拡張が期待される。"],"originalSlideText":"- TCAV (Testing with Concept Activation Vectors)\\n- Analyse von Konzepten, die zu einer Klassifikation führen\\n- Wie wichtig sind einzelne Konzepte für eine Klassifikation im Netz?\\n  - Zebra → Streifen\\n- Fokus auf generelle Eigenschaften der Daten\\n- Salience Maps sind dagegen auf einzelne Pixel fokussiert\\n- Bietet dadurch auch besseres Verständnis der Generalisierbarkeit eines Netzes\\n- Bisher nur auf Bilddaten getestet","questionImage":"","explanationImage":""},{"id":52,"questionDe":"(s61) Welche Voraussetzungen und Grenzen hat die Methode TCAV? Nennen Sie 3 Aspekte.","questionJa":"TCAV（Testing with Concept Activation Vectors）の使用上の前提条件や限界を3点挙げよ。","answerDe":["1. TCAV wird erst nach dem Training des neuronalen Netzes angewendet.","2. Der Nutzer muss die zu testenden Konzepte manuell definieren.","3. Die Methode kann keine unbekannten oder nicht benannten Faktoren aufdecken."],"answerJa":["1. TCAVはニューラルネットの訓練後にのみ適用される。","2. 評価したい概念はユーザー自身が定義しなければならない。","3. 未知または定義されていない特徴を自動的に検出することはできない。"],"explanationDe":["TCAV ist ein Werkzeug zur Erklärung von neuronalen Netzen auf Basis menschlich verstehbarer Konzepte (z. B. \'Streifen\').","Die Methode setzt voraus, dass das Netz bereits trainiert ist und der Nutzer gezielt Konzepte bereitstellt, deren Einfluss auf das Klassifikationsergebnis getestet werden soll.","TCAV kann besonders hilfreich sein, um problematische oder ethisch fragwürdige Eigenschaften im Netzverhalten aufzudecken, etwa wenn Vorurteile erlernt wurden.","Jedoch ist sie nicht geeignet, um völlig unbekannte oder nicht spezifizierte Merkmale im Netz zu identifizieren."],"explanationJa":["TCAV（Testing with Concept Activation Vectors）は、ネットワークの出力に対する人間が理解可能な『概念』の影響を測定する手法である。","この手法は、すでに学習が完了したモデルに対して使用する必要があり、分析対象となる概念はユーザーが事前に定義する必要がある。","たとえば、『ストライプ』『色』などの明示的な特徴に関しては有効だが、モデルが密かに使っている未知の特徴（バイアスなど）は発見できない。","しかし、モデルが偏った特徴を使っていないかをチェックしたり、学習した特徴が事前の知識と一致しているかを検証するなど、倫理的・検証的な目的では非常に有用である。"],"originalSlideText":"- TCAV\\n- Anwendung nach Training des Netzes\\n- Nutzer muss die Konzepte erstellen, die getestet werden sollen\\n- Ergebnis gibt nur Auskunft, wie wichtig das getestete Konzept war\\n- Methode eignet sich damit gut, um auf problematische Eigenschaften zu testen\\n  - Decken sich die erlernten Eigenschaften mit Vorwissen?\\n  - Nutzt das Netz ethisch fragwürdige Features?\\n  - Methode kann aber keine unbekannte Faktoren offenlegen!","questionImage":"","explanationImage":""},{"id":53,"questionDe":"(s61) Wie kann TCAV zur Identifikation problematischer Eigenschaften eines neuronalen Netzes beitragen? Nennen Sie 3 konkrete Aspekte.","questionJa":"TCAV（Testing with Concept Activation Vectors）は、ニューラルネットの問題ある性質を検出するのにどのように役立つか。3つの具体例を挙げよ。","answerDe":["1. TCAV kann überprüfen, ob erlernte Konzepte mit dem Vorwissen übereinstimmen.","2. Es lässt sich feststellen, ob das Netz ethisch fragwürdige Merkmale wie z. B. Hautfarbe oder Kleidung verwendet.","3. Die Methode kann jedoch keine unbekannten oder nicht benannten Faktoren identifizieren."],"answerJa":["1. TCAVにより、学習された特徴が事前の専門的知識（例：正しい診断根拠など）と一致しているか検証できる。","2. モデルが倫理的に望ましくない特徴（例：肌の色、服装）を分類根拠として使用していないかを確認できる。","3. ただし、明示的に定義されていない未知の特徴に対してはTCAVは無力である。"],"explanationDe":["Mit TCAV können Forscher gezielt überprüfen, ob ein Netz auf problematische oder unerwünschte Konzepte zurückgreift.","Ein Beispiel wäre ein Bewerbungsfilter, der auf Kleidung oder Geschlecht statt Qualifikation basiert.","Dabei kann TCAV helfen festzustellen, ob solche Konzepte relevant für die Klassifikation sind.","Allerdings kann TCAV nur Konzepte testen, die vorher definiert wurden – unentdeckte oder unerwünschte Muster bleiben möglicherweise verborgen."],"explanationJa":["TCAVは、ネットワークが学習した特徴が適切か、もしくは倫理的に問題がないかを検証するための手段として活用できる。","例えば、医療画像の分類で本来見るべき部位とは異なる場所（例：背景、画像のノイズなど）が分類根拠になっていないかを確認することができる。","また、性別や人種など差別的要素が分類根拠として使われていないかをチェックする用途でも有効である。","ただし、評価する特徴はユーザーが定義しなければならず、モデルが使っているが人間が気づいていない特徴については把握できないという限界がある。"],"originalSlideText":"- Methode eignet sich damit gut, um auf problematische Eigenschaften zu testen\\n  - Decken sich die erlernten Eigenschaften mit Vorwissen?\\n  - Nutzt das Netz ethisch fragwürdige Features?\\n  - Methode kann aber keine unbekannte Faktoren offenlegen!","questionImage":"","explanationImage":""},{"id":54,"questionDe":"(s62) Wie läuft die TCAV-Methode zur Analyse neuronaler Netze ab? Nennen Sie die vier Hauptschritte und zwei wichtige Hinweise zur Anwendung.","questionJa":"★（試験類題）ニューラルネットの解釈におけるTCAV（Testing with Concept Activation Vectors）の基本的な流れを4つのステップに分けて説明し、使用上の注意点を2つ挙げよ。","answerDe":["1. Training eines Netzes.","2. Erstellen eines Konzeptes durch Beispiele mit zufälligen Bildern.","3. Berechnung von Concept Activation Vectors (CAV) auf den Konzepten.","4. Berechnung des Einflusses eines CAV durch die Richtungsableitung.","Hinweise:","5. Kann auf jeden Layer im Netz angewendet werden.","6. Konzepte müssen statistisch getestet werden, ob sie signifikant sind."],"answerJa":["★1. ニューラルネットを訓練する。","★2. 任意の画像を使って、ある概念（コンセプト）の例を準備する。","★3. その概念に基づいてConcept Activation Vector（CAV）を計算する。","★4. CAVの方向に対するモデル出力の勾配（方向微分）を用いて、CAVの影響度を測定する。","注意点:","★5. CAVはネットワーク内の任意のレイヤーに適用可能である。","★6. 概念が本当に意味あるものか、統計的に検定する必要がある。"],"explanationDe":["TCAV ist ein Verfahren zur Interpretation neuronaler Netze, indem untersucht wird, wie stark ein bestimmtes Konzept zur Klassifikation beiträgt.","Zunächst wird ein Netz trainiert, dann wird ein Konzept (z. B. Streifen für Zebra) über Beispielbilder definiert.","Ein Concept Activation Vector (CAV) wird auf Basis dieser Bilder berechnet.","Mithilfe der Richtungsableitung (Directional Derivative) kann der Einfluss dieses Konzepts auf die Vorhersage bestimmt werden.","Die Methode lässt sich auf beliebige Layer im Netz anwenden, wobei die Signifikanz der Konzepte statistisch überprüft werden muss, um zufällige Effekte auszuschließen."],"explanationJa":["TCAVは、特定の概念（例：縞模様）がモデルの判断にどの程度影響しているかを定量的に評価する手法である。","まずニューラルネットを訓練し、次にその概念の例を複数用意する（例：ストライプのある画像をZebraの概念として使う）。","次に、その概念に対応するCAV（Concept Activation Vector）を計算し、ネットワークのある層での活性化との方向関係を調べる。","最後に、CAVの方向に沿った出力の変化（方向微分）を見ることで、ネットがその概念にどれほど依存しているかがわかる。","この方法はネットワークのどの層にも適用でき、また、概念が偶然ではなく有意に影響しているかどうかを統計的に検証する必要がある。"],"originalSlideText":"- TCAV Ablauf\\n  - Training eines Netzes\\n  - Erstellen eines Konzeptes durch Beispiele mit zufälligen Bildern\\n  - Berechnung von Concept Activation Vectors (CAV) auf den erstellten Konzepten\\n  - Berechnung des Einflusses eines CAV durch die Richtungsableitung\\n- Kann auf jeden Layer im Netz angewendet werden\\n- Konzepte müssen statistisch getestet werden, ob sie signifikant sind","questionImage":"","explanationImage":""}]');const u={class:"container py-4"},h={class:"text-center mb-5"},g={class:"display-5 fw-bold text-primary"},c={class:"fs-5 text-muted"},m={class:"text-dark"};var w={__name:"Lecture10Page",setup(e){const n=(0,s.lq)(),i=(0,a.KR)(""),w=(0,a.KR)(""),b=(0,a.KR)(""),k=(0,a.KR)([]);return(0,r.sV)(()=>{const e="lecture01",r=parseInt(n.name.split("_")[1]),t=o[e];i.value=t.title,b.value=r.toString().padStart(2,"0");const a=t.lectures.find(e=>e.number===r);w.value=a?a.title:"",k.value=d}),(e,n)=>((0,r.uX)(),(0,r.CE)("div",u,[(0,r.Lk)("div",h,[(0,r.Lk)("h1",g,(0,t.v_)(i.value),1),(0,r.Lk)("p",c,[(0,r.eW)(" Lecture "+(0,t.v_)(b.value)+": ",1),(0,r.Lk)("span",m,(0,t.v_)(w.value),1)]),n[0]||(n[0]=(0,r.Lk)("hr",{class:"w-25 mx-auto border-primary"},null,-1))]),((0,r.uX)(!0),(0,r.CE)(r.FK,null,(0,r.pI)(k.value,e=>((0,r.uX)(),(0,r.Wv)(l.A,{key:e.id,question:e},null,8,["question"]))),128))]))}};const b=w;var k=b}}]);
//# sourceMappingURL=4505.ddc062a9.js.map