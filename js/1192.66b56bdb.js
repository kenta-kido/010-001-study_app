"use strict";(self["webpackChunkstudy_site"]=self["webpackChunkstudy_site"]||[]).push([[1192],{495:function(e,n,i){i.d(n,{A:function(){return q}});var t=i(6768),r=i(4232),a=i(144);const s={class:"card mb-4 shadow-sm"},l={class:"card-body"},u={class:"card-title"},d={class:"text-muted fst-italic"},o={key:0},h=["src"],g={key:1,class:"mt-3"},c={class:"alert alert-success"},w={key:0},m={key:1},b={class:"alert alert-info mt-2"},k={key:0},p={key:1},f={class:"mt-3"},v={key:0},z={key:1},x={key:2},D={key:3},N={key:4},S=["src"],A={class:"mt-4"},E={class:"border rounded p-3 bg-white text-secondary",style:{"white-space":"pre-wrap","font-family":"inherit"}};var I={__name:"QuestionItem",props:{question:Object},setup(e){const n=(0,a.KR)(!1);return(i,a)=>((0,t.uX)(),(0,t.CE)("div",s,[(0,t.Lk)("div",l,[(0,t.Lk)("h5",u,"Q"+(0,r.v_)(e.question.id)+": "+(0,r.v_)(e.question.questionJa),1),(0,t.Lk)("p",d,"("+(0,r.v_)(e.question.questionDe)+")",1),e.question.questionImage?((0,t.uX)(),(0,t.CE)("div",o,[(0,t.Lk)("img",{src:`./images/${e.question.questionImage}`,class:"img-fluid rounded my-2 border",alt:"question image"},null,8,h)])):(0,t.Q3)("",!0),(0,t.Lk)("button",{class:"btn btn-outline-primary mt-2",onClick:a[0]||(a[0]=e=>n.value=!n.value)},(0,r.v_)(n.value?"Hide Answer":"Check Answer"),1),n.value?((0,t.uX)(),(0,t.CE)("div",g,[(0,t.Lk)("div",c,[a[1]||(a[1]=(0,t.Lk)("strong",null,"Antwort (De):",-1)),a[2]||(a[2]=(0,t.Lk)("br",null,null,-1)),Array.isArray(e.question.answerDe)?((0,t.uX)(),(0,t.CE)("ul",w,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.answerDe,(e,n)=>((0,t.uX)(),(0,t.CE)("li",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",m,(0,r.v_)(e.question.answerDe),1))]),(0,t.Lk)("div",b,[a[3]||(a[3]=(0,t.Lk)("strong",null,"Ãœbersetzung (Ja):",-1)),a[4]||(a[4]=(0,t.Lk)("br",null,null,-1)),Array.isArray(e.question.answerJa)?((0,t.uX)(),(0,t.CE)("ul",k,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.answerJa,(e,n)=>((0,t.uX)(),(0,t.CE)("li",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",p,(0,r.v_)(e.question.answerJa),1))]),(0,t.Lk)("div",f,[a[6]||(a[6]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"ErklÃ¤rung (De):",-1)),Array.isArray(e.question.explanationDe)?((0,t.uX)(),(0,t.CE)("div",v,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.explanationDe,(e,n)=>((0,t.uX)(),(0,t.CE)("p",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",z,(0,r.v_)(e.question.explanationDe),1)),a[7]||(a[7]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"è§£èª¬ (Ja):",-1)),Array.isArray(e.question.explanationJa)?((0,t.uX)(),(0,t.CE)("div",x,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.explanationJa,(e,n)=>((0,t.uX)(),(0,t.CE)("p",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",D,(0,r.v_)(e.question.explanationJa),1)),e.question.explanationImage?((0,t.uX)(),(0,t.CE)("div",N,[(0,t.Lk)("img",{src:`./images/${e.question.explanationImage}`,class:"img-fluid rounded my-2 border",alt:"explanation image"},null,8,S)])):(0,t.Q3)("",!0),(0,t.Lk)("div",A,[a[5]||(a[5]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"åŸæ–‡ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰æŠœç²‹ï¼‰:",-1)),(0,t.Lk)("div",E,(0,r.v_)(e.question.originalSlideText),1)])])])):(0,t.Q3)("",!0)])]))}};const _=I;var q=_},1192:function(e,n,i){i.r(n),i.d(n,{default:function(){return k}});i(8111),i(116);var t=i(6768),r=i(4232),a=i(144),s=i(1387),l=i(495),u=i(3529),d=JSON.parse('[{"id":1,"questionDe":"(s3) Wie sind kÃ¼nstliche neuronale Netze aufgebaut?","questionJa":"äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã©ã®ã‚ˆã†ã«æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ","answerDe":["Massiv parallel verbundene Netzwerke","Einfache (adaptive) Elemente","Hierarchische Ordnung oder Organisation"],"answerJa":["å¤§è¦æ¨¡ã«ä¸¦åˆ—æ¥ç¶šã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯","å˜ç´”ï¼ˆé©å¿œçš„ï¼‰ãªè¦ç´ ","éšå±¤çš„ãªæ§‹é€ ã¾ãŸã¯çµ„ç¹”"],"explanationDe":["KÃ¼nstliche neuronale Netze bestehen aus vielen einfachen Einheiten (Neuronen), die Ã¤hnlich wie Nervenzellen arbeiten. Diese Einheiten sind in einer Netzwerkstruktur stark miteinander verbunden â€“ das bedeutet, viele Verbindungen arbeiten gleichzeitig (massiv parallel).","Ein Beispiel: Beim Erkennen von Handschriften verarbeitet ein neuronales Netz gleichzeitig viele Pixelinformationen, wobei jedes Neuron nur einen kleinen Teil betrachtet, aber gemeinsam ein Gesamtbild entsteht.","AuÃŸerdem sind die Neuronen in mehreren Schichten organisiert, was man als hierarchische Struktur bezeichnet â€“ wie bei der menschlichen Wahrnehmung von einfachen zu komplexeren Informationen."],"explanationJa":["äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€å¤šãã®å˜ç´”ãªãƒ¦ãƒ‹ãƒƒãƒˆï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼‰ã§æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã‚‰ã¯ç¥çµŒç´°èƒã®ã‚ˆã†ã«å‹•ä½œã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¦ãƒ‹ãƒƒãƒˆã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã§å¯†æ¥ã«æ¥ç¶šã•ã‚Œã¦ãŠã‚Šã€åŒæ™‚ã«å¤šãã®å‡¦ç†ãŒè¡Œã‚ã‚Œã‚‹ï¼ˆå¤§é‡ä¸¦åˆ—å‡¦ç†ï¼‰ã¨ã„ã†ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚","ä¾‹ãˆã°ã€æ‰‹æ›¸ãæ–‡å­—èªè­˜ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒåŒæ™‚ã«å¤šæ•°ã®ãƒ”ã‚¯ã‚»ãƒ«æƒ…å ±ã‚’å‡¦ç†ã—ã€ãã‚Œãã‚Œã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒä¸€éƒ¨åˆ†ã‚’æ‹…å½“ã—ãªãŒã‚‰ã€å…¨ä½“ã¨ã—ã¦æ„å‘³ã‚’æ‰ãˆã¾ã™ã€‚","ã•ã‚‰ã«ã€ã“ã‚Œã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯éšå±¤çš„ã«çµ„ç¹”ã•ã‚Œã¦ãŠã‚Šã€å˜ç´”ãªæƒ…å ±ã‹ã‚‰è¤‡é›‘ãªæƒ…å ±ã¸ã¨æ®µéšçš„ã«å‡¦ç†ãŒé€²ã‚€æ§‹é€ ã«ãªã£ã¦ã„ã¾ã™ã€‚"],"originalSlideText":"KÃ¼nstliche neuronale Netze sind:\\nâ€“ Massiv parallel verbundene Netzwerke aus\\nâ€“ Einfachen (adaptiven) Elementen in\\nâ€“ Hierarchischer Ordnung oder Organisation","explanationImage":"","questionImage":""},{"id":2,"questionDe":"(s3) Wie sollen kÃ¼nstliche neuronale Netze mit der Welt interagieren?","questionJa":"äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã©ã®ã‚ˆã†ã«ä¸–ç•Œã¨é–¢ã‚ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã‹ï¼Ÿ","answerDe":["In der selben Art wie biologische Nervensysteme"],"answerJa":["ç”Ÿç‰©ã®ç¥çµŒç³»ã¨åŒã˜ã‚ˆã†ãªæ–¹æ³•ã§ä¸–ç•Œã¨ç›¸äº’ä½œç”¨ã™ã‚‹ã“ã¨"],"explanationDe":["Neuronale Netze sind inspiriert von biologischen Nervensystemen, insbesondere dem menschlichen Gehirn. Ziel ist es, dass sie Ã¤hnlich wie biologische Systeme Informationen verarbeiten und auf Reize aus der Umgebung reagieren kÃ¶nnen.","Beispiel: Ein selbstfahrendes Auto nutzt neuronale Netze, um visuelle Daten zu analysieren und in Echtzeit Entscheidungen zu treffen â€“ Ã¤hnlich wie ein Mensch beim Autofahren reagiert."],"explanationJa":["ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€ç”Ÿç‰©ã®ç¥çµŒç³»ã€ç‰¹ã«äººé–“ã®è„³ã«ç€æƒ³ã‚’å¾—ã¦ã„ã¾ã™ã€‚ç›®çš„ã¯ã€ç”Ÿç‰©ã¨åŒã˜ã‚ˆã†ã«æƒ…å ±ã‚’å‡¦ç†ã—ã€ç’°å¢ƒã‹ã‚‰ã®åˆºæ¿€ã«å¯¾ã—ã¦é©åˆ‡ã«åå¿œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§ã™ã€‚","ä¾‹ãˆã°ã€è‡ªå‹•é‹è»¢è»Šã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ã¦ã‚«ãƒ¡ãƒ©æ˜ åƒãªã©ã®æƒ…å ±ã‚’è§£æã—ã€äººé–“ã®ã‚ˆã†ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é‹è»¢åˆ¤æ–­ã‚’è¡Œã„ã¾ã™ã€‚"],"originalSlideText":"Diese Netze sollen in der selben Art wie biologische Nervensysteme mit der Welt interagieren.\\n(Kohonen 84)","explanationImage":"","questionImage":""},{"id":3,"questionDe":"(s6) Wodurch sind kÃ¼nstliche neuronale Netze gekennzeichnet?","questionJa":"â˜…äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¯ã©ã‚“ãªç‰¹å¾´ãŒã‚ã‚‹ã‹ï¼Ÿ","answerDe":["Massiv parallele Informationsverarbeitung","Propagierung der Informationen durch Kanten","Verteilte Informationsspeicherung","Black Box Modell"],"answerJa":["â˜…å¤§è¦æ¨¡ä¸¦åˆ—ã«ã‚ˆã‚‹æƒ…å ±å‡¦ç†","â˜…ã‚¨ãƒƒã‚¸ï¼ˆæ¥ç¶šï¼‰ã‚’é€šã˜ãŸæƒ…å ±ã®ä¼æ’­","â˜…åˆ†æ•£å‹ã®æƒ…å ±è¨˜æ†¶æ–¹å¼","â˜…ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆä¸­èº«ãŒè¦‹ãˆã«ãã„ï¼‰"],"explanationDe":["Neuronale Netze verarbeiten viele Informationen gleichzeitig, was als massiv parallele Verarbeitung bezeichnet wird. Dadurch kÃ¶nnen sie komplexe Aufgaben effizient bewÃ¤ltigen.","Die Informationen flieÃŸen in diesen Netzen entlang von Kanten â€“ das sind die Verbindungen zwischen Neuronen â€“ was man als Propagation bezeichnet.","Statt alle Informationen zentral zu speichern, sind sie Ã¼ber viele Einheiten (Neuronen) verteilt. Das erhÃ¶ht die Robustheit des Systems.","Da man oft nicht genau nachvollziehen kann, wie das Netz zu seinen Entscheidungen kommt, spricht man vom \'Black Box Modell\'. Zum Beispiel erkennt ein Netz ein Bild korrekt, aber es ist schwer zu sagen, welche Merkmale genau dafÃ¼r ausschlaggebend waren."],"explanationJa":["ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€å¤šãã®æƒ…å ±ã‚’åŒæ™‚ã«å‡¦ç†ã™ã‚‹ã€å¤§è¦æ¨¡ä¸¦åˆ—å‡¦ç†ã€ã‚’è¡Œã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¤‡é›‘ãªå•é¡Œã«ã‚‚é«˜é€Ÿã«å¯¾å¿œã§ãã¾ã™ã€‚","æƒ…å ±ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³åŒå£«ã‚’çµã¶ã€ã‚¨ãƒƒã‚¸ï¼ˆæ¥ç¶šï¼‰ã€ã‚’é€šã—ã¦ä¼ã‚ã£ã¦ã„ãã€ã“ã®æƒ…å ±ã®æµã‚Œã‚’ã€ä¼æ’­ï¼ˆãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã€ã¨å‘¼ã³ã¾ã™ã€‚","æƒ…å ±ã¯1ã‹æ‰€ã«é›†ä¸­ã—ã¦ä¿å­˜ã•ã‚Œã‚‹ã®ã§ã¯ãªãã€å¤šãã®ãƒ¦ãƒ‹ãƒƒãƒˆï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼‰ã«åˆ†æ•£ã—ã¦è¨˜æ†¶ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ä¸€éƒ¨ãŒå£Šã‚Œã¦ã‚‚æ©Ÿèƒ½ã‚’ç¶­æŒã—ã‚„ã™ããªã‚Šã¾ã™ã€‚","ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã€ã¨ã‚‚å‘¼ã°ã‚Œã€ãã®å†…éƒ¨ã§ã©ã®ã‚ˆã†ã«åˆ¤æ–­ãŒè¡Œã‚ã‚Œã¦ã„ã‚‹ã®ã‹ãŒå¤–ã‹ã‚‰ã¯åˆ†ã‹ã‚Šã«ãã„ã¨ã„ã†ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°ç”»åƒã‚’æ­£ã—ãåˆ†é¡ã—ã¦ã‚‚ã€ä½•ã«æ³¨ç›®ã—ã¦åˆ¤æ–­ã—ãŸã®ã‹æ˜ç¢ºã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"],"originalSlideText":"KÃ¼nstliche neuronale Netze sind gekennzeichnet durch:\\nâ€“ Massiv parallele Informationsverarbeitung\\nâ€“ Propagierung der Informationen durch Kanten\\nâ€“ Verteilte Informationsspeicherung\\nâ€“ Black Box Modell","explanationImage":"","questionImage":""},{"id":4,"questionDe":"(s6) Welche Phasen durchlaufen kÃ¼nstliche neuronale Netze?","questionJa":"äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã©ã®ã‚ˆã†ãªãƒ•ã‚§ãƒ¼ã‚ºï¼ˆæ®µéšï¼‰ã‚’çµŒã‚‹ã‹ï¼Ÿ","answerDe":["Aufbauphase (Topologie des Netzes)","Trainingsphase (Lernen)","Arbeitsphase (Propagation)"],"answerJa":["æ§‹ç¯‰ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã®è¨­è¨ˆï¼‰","ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆå­¦ç¿’ï¼‰","å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚ºï¼ˆæƒ…å ±ã®ä¼æ’­ã¨åˆ©ç”¨ï¼‰"],"explanationDe":["In der Aufbauphase wird das GrundgerÃ¼st des neuronalen Netzes definiert â€“ z.â€¯B. wie viele Schichten und Neuronen vorhanden sind und wie sie miteinander verbunden sind.","In der Trainingsphase lernt das Netz aus Beispieldaten. Es passt seine internen Verbindungen (Gewichte) an, um bessere Ergebnisse zu erzielen.","In der Arbeitsphase (auch Inferenz genannt) wird das trainierte Netz verwendet, um neue Eingaben zu verarbeiten. Die gelernten Informationen werden hier angewendet, z.â€¯B. zur Klassifikation von Bildern oder zur Spracherkennung."],"explanationJa":["æ§‹ç¯‰ãƒ•ã‚§ãƒ¼ã‚ºã§ã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŸºæœ¬æ§‹é€ ã‚’è¨­è¨ˆã—ã¾ã™ã€‚ãŸã¨ãˆã°ã€ä½•å±¤ã«ã™ã‚‹ã‹ã€å„å±¤ã«ã©ã®ãã‚‰ã„ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’æŒãŸã›ã‚‹ã‹ã€ã©ã®ã‚ˆã†ã«æ¥ç¶šã™ã‚‹ã‹ãªã©ã‚’æ±ºã‚ã¾ã™ã€‚","ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚ºã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«å­¦ç¿’ã—ã¾ã™ã€‚é‡ã¿ï¼ˆæ¥ç¶šã®å¼·ã•ï¼‰ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€äºˆæ¸¬ã‚„åˆ†é¡ã®ç²¾åº¦ã‚’é«˜ã‚ã¦ã„ãã¾ã™ã€‚","å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚ºã§ã¯ã€å­¦ç¿’æ¸ˆã¿ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ã¦æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¾ã™ã€‚ãŸã¨ãˆã°ã€ç”»åƒèªè­˜ã‚„éŸ³å£°èªè­˜ãªã©ã«å¿œç”¨ã•ã‚Œã¾ã™ã€‚"],"originalSlideText":"Phasen:\\nâ€“ Aufbauphase (Topologie des Netzes)\\nâ€“ Trainingsphase (Lernen)\\nâ€“ Arbeitsphase (Propagation)","explanationImage":"","questionImage":""},{"id":5,"questionDe":"(s10) Wie funktioniert das McCulloch-Pitts-Neuronmodell mathematisch?","questionJa":"McCulloch-Pittsãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«ã¯æ•°å­¦çš„ã«ã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã‹ï¼Ÿ","answerDe":["Eingaben x_i sind binÃ¤r: 0 oder 1","Funktion f: B^n â†’ B","Ausgabe ist ebenfalls binÃ¤r (0 oder 1)"],"answerJa":["å…¥åŠ› x_i ã¯0ã‹1ã®2å€¤","é–¢æ•° f: B^n â†’ Bï¼ˆè¤‡æ•°ã®å…¥åŠ›ã‹ã‚‰å‡ºåŠ›ã‚’æ±ºå®šï¼‰","å‡ºåŠ›ã‚‚0ã¾ãŸã¯1ã®2å€¤"],"explanationDe":["Das McCulloch-Pitts-Neuron nimmt mehrere binÃ¤re Eingaben entgegen (zum Beispiel x1, x2, ..., xn), wobei jede Eingabe entweder 0 (inaktiv) oder 1 (aktiv) ist.","Diese Eingaben werden durch eine logische Funktion f verarbeitet. Die Funktion entscheidet basierend auf den Eingaben, ob das Neuron aktiviert wird (Ausgabe = 1) oder nicht (Ausgabe = 0).","Zum Beispiel kann f eine UND-Funktion sein: Nur wenn alle x_i gleich 1 sind, wird das Neuron aktiviert."],"explanationJa":["McCulloch-Pittsãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€è¤‡æ•°ã®2å€¤å…¥åŠ›ï¼ˆä¾‹ï¼šx1, x2, ..., xnï¼‰ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚ãã‚Œãã‚Œã®å…¥åŠ›ã¯0ï¼ˆéæ´»æ€§ï¼‰ã¾ãŸã¯1ï¼ˆæ´»æ€§ï¼‰ã§ã™ã€‚","ã“ã‚Œã‚‰ã®å…¥åŠ›ã¯é–¢æ•°fã«ã‚ˆã£ã¦å‡¦ç†ã•ã‚Œã€å‡ºåŠ›ãŒ0ï¼ˆéæ´»æ€§ï¼‰ã¾ãŸã¯1ï¼ˆæ´»æ€§ï¼‰ã«æ±ºå®šã•ã‚Œã¾ã™ã€‚","ä¾‹ãˆã°ã€ã™ã¹ã¦ã®å…¥åŠ›ãŒ1ã®ã¨ãã ã‘å‡ºåŠ›ãŒ1ã«ãªã‚‹ã€ŒANDé–¢æ•°ã€ã¨ã—ã¦å‹•ä½œã•ã›ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"],"originalSlideText":"Modell\\nMcCulloch-Pitts-Neuron 1943:\\nxi âˆˆ {0, 1} =: ğ”¹\\nf: ğ”¹â¿ â†’ ğ”¹","explanationImage":"","questionImage":""},{"id":6,"questionDe":"(s11) Was ist die Grundidee des McCulloch-Pitts-Neurons?","questionJa":"â˜…McCulloch-Pittsãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®åŸºæœ¬çš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã¨ã¯ï¼Ÿ","answerDe":["Neuron ist entweder aktiv oder inaktiv","FÃ¤higkeiten entstehen durch Vernetzung der Neuronen"],"answerJa":["ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€Œæ´»æ€§ã€ã¾ãŸã¯ã€Œéæ´»æ€§ã€ã®ã„ãšã‚Œã‹ã«ãªã‚‹","ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã¤ãªãŒã‚Šã«ã‚ˆã£ã¦è¤‡é›‘ãªæ©Ÿèƒ½ãŒç”Ÿã¾ã‚Œã‚‹"],"explanationDe":["Die Grundidee ist, dass jedes Neuron ein binÃ¤rer Schalter ist: Es feuert entweder (aktiv = 1) oder es feuert nicht (inaktiv = 0).","Die Intelligenz eines Systems entsteht nicht durch die KomplexitÃ¤t einzelner Neuronen, sondern durch ihre Verschaltung. Viele einfache Neuronen kÃ¶nnen durch ihre Verbindung komplexe Funktionen ausfÃ¼hren."],"explanationJa":["åŸºæœ¬çš„ãªè€ƒãˆæ–¹ã¯ã€å„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒ2ã¤ã®çŠ¶æ…‹ï¼ˆç™ºç«ã™ã‚‹ï¼1ã€ç™ºç«ã—ãªã„ï¼0ï¼‰ã‚’æŒã¤ã‚¹ã‚¤ãƒƒãƒã®ã‚ˆã†ãªå­˜åœ¨ã§ã‚ã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚","å€‹ã€…ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³è‡ªä½“ã¯å˜ç´”ã§ã‚‚ã€ãã‚Œã‚‰ãŒã©ã†æ¥ç¶šã•ã‚Œã‚‹ã‹ã«ã‚ˆã£ã¦ã€å…¨ä½“ã¨ã—ã¦è¤‡é›‘ãªå‡¦ç†ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"Grundidee:\\nâ€“ Neuron ist entweder aktiv oder inaktiv\\nâ€“ FÃ¤higkeiten entstehen durch Vernetzung der Neuronen","explanationImage":"","questionImage":""},{"id":7,"questionDe":"(s11) Welche EinschrÃ¤nkungen hat das McCulloch-Pitts-Modell?","questionJa":"â˜…McCulloch-Pittsãƒ¢ãƒ‡ãƒ«ã«ã¯ã©ã‚“ãªåˆ¶é™ãŒã‚ã‚‹ã‹ï¼Ÿ","answerDe":["Nur statische Netze werden betrachtet","Topologie wird vorher festgelegt","Keine neuen Verbindungen beim Lernen","Widerspruch zur Biologie"],"answerJa":["â˜…é™çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã—ã‹æ‰±ã‚ãªã„","â˜…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ï¼ˆãƒˆãƒãƒ­ã‚¸ãƒ¼ï¼‰ã¯äº‹å‰ã«æ±ºã¾ã£ã¦ã„ã‚‹","â˜…å­¦ç¿’æ™‚ã«æ–°ãŸãªæ¥ç¶šã‚’ä½œã‚‰ãªã„","ç”Ÿç‰©ã®ä»•çµ„ã¿ã¨çŸ›ç›¾ã™ã‚‹"],"explanationDe":["Das Modell erlaubt keine Ã„nderung der Netzwerkstruktur wÃ¤hrend des Lernens â€“ das widerspricht der biologischen RealitÃ¤t, wo sich Verbindungen im Gehirn anpassen oder neu entstehen kÃ¶nnen.","Das Netz ist statisch, d.â€¯h. seine Struktur bleibt immer gleich. In der Praxis ist jedoch bekannt, dass sich die neuronalen Verbindungen im Gehirn durch Erfahrung verÃ¤ndern kÃ¶nnen.","AuÃŸerdem ist die Topologie (VerknÃ¼pfungsmuster) fest vorgegeben â€“ im Gegensatz zu realen Nervensystemen, die sich plastisch entwickeln."],"explanationJa":["ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å­¦ç¿’ä¸­ã«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹é€ ãŒå¤‰åŒ–ã™ã‚‹ã“ã¨ã¯ãªãã€ç”Ÿç‰©ã®è„³ã®ã‚ˆã†ã«æ¥ç¶šãŒæ–°ãŸã«ä½œã‚‰ã‚ŒãŸã‚Šæ¶ˆãˆãŸã‚Šã™ã‚‹æŸ”è»Ÿæ€§ãŒã‚ã‚Šã¾ã›ã‚“ã€‚","ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ï¼ˆãƒˆãƒãƒ­ã‚¸ãƒ¼ï¼‰ã¯æœ€åˆã«æ±ºã‚ã‚‰ã‚Œã¦ã„ã¦ã€ãã®å¾Œã¯å¤‰ã‚ã‚Šã¾ã›ã‚“ã€‚ã—ã‹ã—ã€ç”Ÿç‰©ã®è„³ã§ã¯å­¦ç¿’ã‚„çµŒé¨“ã«ã‚ˆã£ã¦æ¥ç¶šãŒå¤‰åŒ–ã—ã¾ã™ã€‚","ã“ã®ã‚ˆã†ãªåˆ¶ç´„ã®ãŸã‚ã€McCulloch-Pittsãƒ¢ãƒ‡ãƒ«ã¯ç¾å®Ÿã®ç¥çµŒç³»ã¨ã¯ç•°ãªã‚‹ã€é™çš„ãªã€ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã¨ã„ãˆã¾ã™ã€‚"],"originalSlideText":"â€“ Es werden nur statische Netze betrachtet\\nâ€“ Topologie wird vorher festgelegt\\nâ€“ Es werden keine neuen Verbindungen beim Lernen erstellt\\nâ€“ Widerspruch zur Biologie!","explanationImage":"","questionImage":""},{"id":8,"questionDe":"(s11â€“14) Wie funktioniert das McCulloch-Pitts-Neuron und wie wurde es erweitert?","questionJa":"â˜…McCulloch-Pittsãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã—ã€ãã‚Œã¯ã©ã®ã‚ˆã†ã«æ‹¡å¼µã•ã‚Œã¦ããŸã‹ï¼Ÿ","answerDe":["BinÃ¤re Eingaben (0 oder 1), Neuron ist aktiv oder inaktiv","Schwellwert entscheidet Ã¼ber Aktivierung","Logische Funktionen wie AND und OR realisierbar","Hemmende Eingaben kÃ¶nnen Aktivierung verhindern","Verallgemeinerung durch gewichtete Eingaben"],"answerJa":["â˜…å…¥åŠ›ã¯0ã¾ãŸã¯1ã®2å€¤ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯æ´»æ€§ã¾ãŸã¯éæ´»æ€§","â˜…ã—ãã„å€¤ã«ã‚ˆã£ã¦å‡ºåŠ›ãŒæ±ºã¾ã‚‹","â˜…ANDã‚„ORãªã©ã®è«–ç†é–¢æ•°ã‚’è¡¨ç¾å¯èƒ½","æŠ‘åˆ¶å…¥åŠ›ãŒã‚ã‚‹ã¨å‡ºåŠ›ã¯ç„¡åŠ¹ã«ãªã‚‹","é‡ã¿ä»˜ãå…¥åŠ›ã«ã‚ˆã£ã¦ä¸€èˆ¬åŒ–ã•ã‚Œã‚‹"],"explanationDe":["Das McCulloch-Pitts-Neuron wurde 1943 eingefÃ¼hrt. Es arbeitet mit binÃ¤ren Eingangssignalen (0 oder 1). Das Neuron gibt 1 aus, wenn die Summe der EingÃ¤nge grÃ¶ÃŸer oder gleich einem festen Schwellwert ist.","Damit lassen sich logische Funktionen wie AND (Schwellwert = n) oder OR (Schwellwert = 1) einfach umsetzen.","SpÃ¤ter wurde das Modell erweitert: Es gibt hemmende EingÃ¤nge (y1 bis ym), die sofort die Ausgabe auf 0 setzen, wenn einer davon aktiv (gleich 1) ist. Das erlaubt auch die Darstellung einer NOT-Funktion.","Um flexibler zu sein, fÃ¼hrte man Gewichtungen ein. Jeder Eingang hat ein Gewicht, das seinen Einfluss bestimmt. Die gewichteten Eingaben werden summiert und mit dem Schwellwert verglichen.","Beispiel: Wenn 0,2*x1 + 0,4*x2 + 0,3*x3 >= 0,7, dann ist die Ausgabe 1. Das kann durch ganzzahlige Skalierung und Duplikation ebenfalls als Schwellenwert-Modell dargestellt werden."],"explanationJa":["McCulloch-Pittsãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯1943å¹´ã«ææ¡ˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã€å…¥åŠ›ã¯0ã¾ãŸã¯1ã®2å€¤ã§ã™ã€‚å…¥åŠ›ã®åˆè¨ˆãŒã‚ã‚‹ã—ãã„å€¤ä»¥ä¸Šã§ã‚ã‚Œã°ã€å‡ºåŠ›ã¯1ï¼ˆæ´»æ€§ï¼‰ã«ãªã‚Šã¾ã™ã€‚","ã“ã®ä»•çµ„ã¿ã«ã‚ˆã‚Šã€ANDï¼ˆã—ãã„å€¤=nï¼‰ã‚„ORï¼ˆã—ãã„å€¤=1ï¼‰ãªã©ã®è«–ç†é–¢æ•°ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚","å¾Œã«ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ‹¡å¼µã•ã‚Œã€æŠ‘åˆ¶å…¥åŠ›ï¼ˆy1ã€œymï¼‰ã‚‚å°å…¥ã•ã‚Œã¾ã—ãŸã€‚æŠ‘åˆ¶å…¥åŠ›ãŒ1ã§ã‚ã‚Œã°ã€ãã‚Œã ã‘ã§å‡ºåŠ›ã¯å¼·åˆ¶çš„ã«0ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚ŠNOTæ¼”ç®—ã‚‚è¡¨ç¾å¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚","ã•ã‚‰ã«ã€å„å…¥åŠ›ã«é‡ã¿ã‚’æŒãŸã›ã‚‹æ‹¡å¼µã‚‚è¡Œã‚ã‚Œã¾ã—ãŸã€‚å…¥åŠ›ã”ã¨ã®é‡ã¿ã‚’ã‹ã‘ç®—ã—ã€ãã‚Œã‚‰ã®åˆè¨ˆãŒã—ãã„å€¤ä»¥ä¸Šã§ã‚ã‚Œã°å‡ºåŠ›ã¯1ã«ãªã‚Šã¾ã™ã€‚","ä¾‹ã¨ã—ã¦ã€0.2Ã—x1 + 0.4Ã—x2 + 0.3Ã—x3 â‰¥ 0.7 ã®ã¨ãå‡ºåŠ›ãŒ1ã«ãªã‚Šã¾ã™ã€‚ã“ã®å¼ã‚’10å€ã—ã¦æ•´æ•°ã«ç›´ã—ã€å…¥åŠ›ã‚’è¤‡è£½ã™ã‚‹ã“ã¨ã§ç­‰ä¾¡ãªãƒ¢ãƒ‡ãƒ«ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚"],"originalSlideText":"McCulloch Pitts Neuron\\nâ€“ n binÃ¤re Eingangssignale x_1 bis x_n\\nâ€“ Schwellwert Î¸ > 0\\nâ€“ Realisierung logischer Funktionen (AND, OR)\\nâ€“ Erweiterung: hemmende Signale y_1 bis y_m\\nâ€“ Ausgabe = 0 wenn y_j = 1\\nâ€“ Verallgemeinerung mit Gewichten\\nâ€“ Beispiel: 0,2 x1 + 0,4 x2 + 0,3 x3 â‰¥ 0,7","explanationImage":"","questionImage":""},{"id":9,"questionDe":"(s16) Was ist ein Perceptron und welche Eigenschaften hat es?","questionJa":"â˜…Perceptronã¨ã¯ä½•ã§ã€ã©ã®ã‚ˆã†ãªç‰¹å¾´ãŒã‚ã‚‹ã‹ï¼Ÿ","answerDe":["Ein Outputneuron pro unabhÃ¤ngigem Netzbereich","Zur Vereinfachung nur ein Output pro Netz","UrsprÃ¼nglich fÃ¼r Hardware gedacht","SpÃ¤ter zu Multilayer-Perceptrons erweitert"],"answerJa":["â˜…å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ç‹¬ç«‹ã—ãŸãƒãƒƒãƒˆé ˜åŸŸã‚’æŒã¤","â˜…ç°¡ç•¥åŒ–ã®ãŸã‚ã«å„ãƒãƒƒãƒˆã¯1ã¤ã®å‡ºåŠ›ã‚’æŒã¤","â˜…å½“åˆã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã¨ã—ã¦è¨­è¨ˆã•ã‚ŒãŸ","â˜…å¾Œã«å¤šå±¤Perceptronã¸ã¨æ‹¡å¼µã•ã‚ŒãŸ"],"explanationDe":["Ein Perceptron ist ein einfaches kÃ¼nstliches neuronales Netz, das 1958 von Rosenblatt entwickelt wurde. Es besteht aus einer Eingabeschicht und einer Ausgabeschicht.","Jedes Ausgabeneuron verarbeitet die Eingaben durch gewichtete Verbindungen und entscheidet, ob es aktiviert wird.","In frÃ¼hen Versionen hatte jedes Netz nur ein einziges Ausgabeneuron, um das Modell zu vereinfachen.","Das ursprÃ¼ngliche Ziel war eine Umsetzung auf Hardware â€“ also nicht nur Software-Modelle, sondern echte elektrische Schaltungen.","SpÃ¤ter wurde das Konzept erweitert, indem man mehrere Schichten hinzufÃ¼gte. Diese Multilayer-Perceptrons sind die Grundlage moderner neuronaler Netzwerke."],"explanationJa":["Perceptronã¯ã€1958å¹´ã«ãƒ­ãƒ¼ã‚¼ãƒ³ãƒ–ãƒ©ãƒƒãƒˆã«ã‚ˆã£ã¦ææ¡ˆã•ã‚ŒãŸå˜ç´”ãªäººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚å…¥åŠ›å±¤ã¨å‡ºåŠ›å±¤ã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚","å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯å…¥åŠ›ã‹ã‚‰ã®é‡ã¿ä»˜ãä¿¡å·ã‚’å—ã‘å–ã‚Šã€ã‚ã‚‹åŸºæº–ï¼ˆã—ãã„å€¤ï¼‰ã‚’è¶…ãˆã‚‹ã¨æ´»æ€§åŒ–ã•ã‚Œã¾ã™ã€‚","åˆæœŸã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒ1ã¤ã®å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã¿ã‚’æŒã£ã¦ã„ã¾ã—ãŸã€‚ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ä¿ã¤ãŸã‚ã§ã™ã€‚","å½“åˆã¯ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã§ã¯ãªãã€å®Ÿéš›ã®é›»å­å›è·¯ã¨ã—ã¦ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§å®Ÿè£…ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã—ãŸã€‚","ãã®å¾Œã€éš ã‚Œå±¤ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§å¤šå±¤Perceptronï¼ˆMultilayer Perceptronï¼‰ãŒç™»å ´ã—ã€ç¾åœ¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŸºç›¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚"],"originalSlideText":"Perceptron (Rosenblatt 1958)\\nâ€“ Jedes Outputneuron hat einen eigenen unabhÃ¤ngigen Netzbereich\\nâ€“ Zur Vereinfachung hat jedes Netz im folgenden nur einen Output\\nâ€“ War historisch als Hardwareimplementierung gedacht\\nâ€“ SpÃ¤ter auf Multilayer Perceptrons erweitert","explanationImage":"lecture01/lecture09_ex01.png","questionImage":""},{"id":10,"questionDe":"(s16) Welche Bestandteile hat ein einfaches Perzeptron?","questionJa":"â˜…å˜ç´”ãªPerceptronã¯ã©ã®ã‚ˆã†ãªæ§‹æˆè¦ç´ ã‚’æŒã¤ã‹ï¼Ÿ","answerDe":["Inputschicht: EnthÃ¤lt mehrere Eingangssignale (x1 bis x4), die Merkmale der Eingabedaten darstellen.","Gewichtete Verbindungen: Jede Verbindung zwischen Eingabe- und Ausgabeneuron hat ein Gewicht (z.â€¯B. w11, w12).","Ausgabeschicht: Neuronen (o1 bis o3) berechnen eine gewichtete Summe der EingÃ¤nge.","Aktivierung: Ein Neuron wird aktiv (gibt 1 aus), wenn die gewichtete Summe einen Schwellwert Ã¼bersteigt."],"answerJa":["â˜…å…¥åŠ›å±¤ï¼šè¤‡æ•°ã®å…¥åŠ›ä¿¡å·ï¼ˆx1ã€œx4ï¼‰ã‚’æŒã¡ã€ãã‚Œãã‚ŒãŒãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ã‚’è¡¨ã™ã€‚","â˜…é‡ã¿ä»˜ãæ¥ç¶šï¼šå„å…¥åŠ›ã¨å‡ºåŠ›ã®é–“ã«ã¯é‡ã¿ï¼ˆä¾‹ï¼šw11, w12ï¼‰ãŒè¨­å®šã•ã‚Œã€å½±éŸ¿ã®å¤§ãã•ã‚’ç¤ºã™ã€‚","â˜…å‡ºåŠ›å±¤ï¼šãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼ˆo1ã€œo3ï¼‰ãŒã€å—ã‘å–ã£ãŸä¿¡å·ã«é‡ã¿ã‚’ã‹ã‘ã¦åˆè¨ˆã™ã‚‹ã€‚","â˜…æ´»æ€§åŒ–ï¼šåˆè¨ˆãŒã—ãã„å€¤ã‚’è¶…ãˆãŸã¨ãã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒæ´»æ€§åŒ–ï¼ˆå‡ºåŠ›ãŒ1ï¼‰ã•ã‚Œã‚‹ã€‚"],"explanationDe":["Ein Perzeptron besteht aus einer Inputschicht mit mehreren Eingangssignalen (z.â€¯B. x1 bis x4), die bestimmte Merkmale eines Eingabedatensatzes beschreiben â€“ etwa Pixel eines Bildes oder Messwerte von Sensoren.","Zwischen Input- und Outputschicht gibt es gewichtete Verbindungen. Die Gewichte (z.â€¯B. w11, w43) bestimmen, wie stark jedes Eingangssignal das Ergebnis beeinflusst.","Die Ausgabeneuronen summieren die gewichteten Eingangssignale. Diese Summe wird mit einem Schwellwert verglichen.","Die Aktivierung entscheidet, ob ein Neuron \'feuert\' â€“ also eine 1 ausgibt â€“ oder nicht (0). Ein Neuron wird aktiv, wenn die gewichtete Summe seiner EingÃ¤nge grÃ¶ÃŸer oder gleich dem Schwellwert ist.","Beispiel: Wenn x1 = 1, x2 = 1, und die Gewichte w11 = 0.5, w21 = 0.5 betragen, dann ist die gewichtete Summe 1.0. Wenn der Schwellwert 0.8 ist, wird das Neuron aktiviert und gibt 1 aus â€“ z.â€¯B. fÃ¼r die Entscheidung \'Ja, das ist eine Katze auf dem Bild\'."],"explanationJa":["Perceptronã¯ã€å…¥åŠ›å±¤ã«è¤‡æ•°ã®ä¿¡å·ï¼ˆx1ã€œx4ï¼‰ã‚’æŒã¡ã€ãã‚Œãã‚ŒãŒç”»åƒã®ç‰¹å¾´ã‚„ã‚»ãƒ³ã‚µãƒ¼ã®æ¸¬å®šå€¤ãªã©ã‚’è¡¨ã—ã¾ã™ã€‚","â˜…å„å…¥åŠ›ã¯é‡ã¿ï¼ˆä¾‹ï¼šw11, w43ï¼‰ã‚’æŒã£ã¦ãŠã‚Šã€ã“ã‚Œã¯ãã®å…¥åŠ›ãŒå‡ºåŠ›ã«ã©ã‚Œã ã‘å½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚","â˜…å‡ºåŠ›å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€ãã‚Œãã‚Œã®å…¥åŠ›ã«å¯¾å¿œã™ã‚‹é‡ã¿ã‚’ã‹ã‘ãŸå€¤ã‚’åˆè¨ˆã—ã¾ã™ã€‚ãã—ã¦ãã®åˆè¨ˆãŒã€Œã—ãã„å€¤ã€ä»¥ä¸Šã§ã‚ã‚Œã°æ´»æ€§åŒ–ã•ã‚Œã€å‡ºåŠ›ãŒ1ã«ãªã‚Šã¾ã™ã€‚","â˜…ã€æ´»æ€§åŒ–ã€ã¨ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒä¿¡å·ã‚’å‡ºã™çŠ¶æ…‹ã«ãªã‚‹ã“ã¨ã§ã™ã€‚å‡ºåŠ›ãŒ1ã§ã‚ã‚Œã°ã€Œã¯ã„ã€ã€0ã§ã‚ã‚Œã°ã€Œã„ã„ãˆã€ã®ã‚ˆã†ãªæ„å‘³ã‚’æŒã¡ã¾ã™ã€‚","ä¾‹ï¼šx1 = 1ã€x2 = 1ã€é‡ã¿w11 = 0.5ã€w21 = 0.5ã¨ã™ã‚‹ã¨ã€åˆè¨ˆã¯1.0ã«ãªã‚Šã¾ã™ã€‚ã—ãã„å€¤ãŒ0.8ã§ã‚ã‚Œã°ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯æ´»æ€§åŒ–ã•ã‚Œã€1ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ãŸã¨ãˆã°ã€Œã“ã‚Œã¯çŒ«ã®ç”»åƒã§ã‚ã‚‹ã€ã¨åˆ¤æ–­ã™ã‚‹ã‚±ãƒ¼ã‚¹ã§ã™ã€‚"],"originalSlideText":"Abbildung: Inputschicht (x1â€“x4), Gewichtungen (w_ij), Outputschicht (o1â€“o3), Aktivierung bei SchwellenÃ¼berschreitung","explanationImage":"","questionImage":"lecture01/lecture09_q01.png"},{"id":11,"questionDe":"(s17) Wie ist ein kÃ¼nstliches Neuron im Allgemeinen aufgebaut?","questionJa":"â˜…äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ä¸€èˆ¬çš„ã«ã©ã®ã‚ˆã†ãªæ§‹é€ ã‚’ã—ã¦ã„ã‚‹ã‹ï¼Ÿ","answerDe":["Eingaben (x1 bis xn) mit zugehÃ¶rigen Gewichtungen (w1j bis wnj)","Ãœbertragungsfunktion: Bildung der gewichteten Summe (netj)","Aktivierungsfunktion Ï† vergleicht mit dem Schwellwert Î¸j","Ausgabe oj je nach Aktivierung"],"answerJa":["å…¥åŠ›ï¼ˆx1ã€œxnï¼‰ã«ã¯ãã‚Œãã‚Œé‡ã¿ï¼ˆw1jã€œwnjï¼‰ãŒã‚ã‚‹","ä¼é”é–¢æ•°ã«ã‚ˆã£ã¦é‡ã¿ä»˜ãå…¥åŠ›ã®åˆè¨ˆï¼ˆnetjï¼‰ã‚’è¨ˆç®—ã™ã‚‹","æ´»æ€§åŒ–é–¢æ•°Ï†ãŒåˆè¨ˆã‚’ã—ãã„å€¤Î¸jã¨æ¯”è¼ƒã™ã‚‹","æ´»æ€§åŒ–ã®æœ‰ç„¡ã«ã‚ˆã£ã¦å‡ºåŠ›ojãŒæ±ºã¾ã‚‹"],"explanationDe":["Ein kÃ¼nstliches Neuron erhÃ¤lt mehrere Eingangssignale (x1 bis xn), die jeweils mit einem Gewicht (w1j bis wnj) multipliziert werden. Diese Gewichte bestimmen den Einfluss jedes Eingabewertes.","Die Ãœbertragungsfunktion berechnet die gewichtete Summe aller EingÃ¤nge: netj = x1*w1j + x2*w2j + ... + xn*wnj.","Diese Summe (netj) wird an die Aktivierungsfunktion Ï† weitergegeben. Sie vergleicht netj mit einem Schwellwert Î¸j.","Wenn netj â‰¥ Î¸j ist, wird das Neuron aktiviert und gibt den Wert 1 aus (oj = 1), sonst bleibt es inaktiv (oj = 0).","Beispiel: Wenn ein Bild mehrere Merkmale enthÃ¤lt (z.â€¯B. x1 = \'rund\', x2 = \'klein\') und die gewichtete Summe dieser Merkmale groÃŸ genug ist, kÃ¶nnte das Neuron \'aktiviert\' werden, um etwa \'Ball erkannt\' auszugeben."],"explanationJa":["äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€è¤‡æ•°ã®å…¥åŠ›ä¿¡å·ï¼ˆx1ã€œxnï¼‰ã‚’å—ã‘å–ã‚Šã€ãã‚Œãã‚Œã«é‡ã¿ï¼ˆw1jã€œwnjï¼‰ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®é‡ã¿ã¯ã€å„å…¥åŠ›ãŒå‡ºåŠ›ã«ã©ã‚Œã»ã©å½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚","ä¼é”é–¢æ•°ï¼ˆã¾ãŸã¯åŠ ç®—é–¢æ•°ï¼‰ã¯ã€ã™ã¹ã¦ã®å…¥åŠ›ã«é‡ã¿ã‚’ã‹ã‘ã¦åˆè¨ˆï¼ˆnetjï¼‰ã‚’æ±‚ã‚ã¾ã™ã€‚ãŸã¨ãˆã° netj = x1*w1j + x2*w2j + ... + xn*wnj ã®ã‚ˆã†ã«è¨ˆç®—ã—ã¾ã™ã€‚","ãã®åˆè¨ˆå€¤ netj ã¯æ´»æ€§åŒ–é–¢æ•°Ï†ã«æ¸¡ã•ã‚Œã€è¨­å®šã•ã‚ŒãŸã—ãã„å€¤Î¸jã¨æ¯”è¼ƒã•ã‚Œã¾ã™ã€‚","ã‚‚ã—netjãŒÎ¸jä»¥ä¸Šã§ã‚ã‚Œã°ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯æ´»æ€§åŒ–ã•ã‚Œã¦å‡ºåŠ›1ï¼ˆoj = 1ï¼‰ã‚’è¿”ã—ã¾ã™ã€‚ãã†ã§ãªã‘ã‚Œã°å‡ºåŠ›ã¯0ï¼ˆoj = 0ï¼‰ã«ãªã‚Šã¾ã™ã€‚","ä¾‹ï¼šç”»åƒã®ç‰¹å¾´ã¨ã—ã¦ã€Œä¸¸ã„ï¼ˆx1ï¼‰ã€ã€Œå°ã•ã„ï¼ˆx2ï¼‰ã€ãªã©ãŒå…¥åŠ›ã•ã‚Œã€ãã‚Œã‚‰ã®åˆè¨ˆãŒååˆ†å¤§ãã‘ã‚Œã°ã€Œã“ã‚Œã¯ãƒœãƒ¼ãƒ«ã ã€ã¨åˆ¤æ–­ã™ã‚‹ãŸã‚ã«ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒæ´»æ€§åŒ–ã•ã‚Œã€å‡ºåŠ›ãŒ1ã«ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"Genereller Aufbau\\nGewichtungen, Eingaben x1â€¦xn, Ãœbertragungsfunktion (Summe), Aktivierungsfunktion Ï†, Schwellwert Î¸j, Ausgabe oj","explanationImage":"lecture01/lecture09_ex02.png","questionImage":""},{"id":12,"questionDe":"(s18) Was ist eine Schwellenwertfunktion und wie funktioniert sie?","questionJa":"â˜…ã—ãã„å€¤é–¢æ•°ã¨ã¯ä½•ã§ã€ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ï¼Ÿ","answerDe":["Nimmt nur die Werte 0 und 1 an","Schwellwert entscheidet Ã¼ber Aktivierung","Funktioniert nach dem Alles-oder-Nichts-Prinzip"],"answerJa":["â˜…å‡ºåŠ›ã¯0ã‹1ã®ã©ã¡ã‚‰ã‹ã®ã¿ã‚’ã¨ã‚‹","â˜…ã—ãã„å€¤ã«ã‚ˆã£ã¦æ´»æ€§åŒ–ãŒæ±ºã¾ã‚‹","ã™ã¹ã¦ã‹ã‚¼ãƒ­ã‹ã®å‹•ä½œï¼ˆAll-or-Nothingï¼‰ã‚’ã™ã‚‹"],"explanationDe":["Die Schwellenwertfunktion (auch Heaviside-Funktion genannt) ist eine einfache Aktivierungsfunktion, die nur zwei Ausgaben erlaubt: 0 oder 1.","Wenn der Eingabewert v kleiner als 0 ist, gibt die Funktion 0 zurÃ¼ck. Wenn v grÃ¶ÃŸer oder gleich 0 ist, gibt sie 1 zurÃ¼ck.","Das bedeutet: Nur wenn die Eingabe stark genug ist (Ã¼ber dem Schwellwert liegt), wird das Neuron aktiviert.","Diese Art der Funktion wird als Alles-oder-Nichts-Funktion beschrieben, da es keine Zwischenwerte gibt.","Beispiel: Wenn v = -0,2 â†’ Ausgabe 0 (nicht aktiv), aber bei v = 0,3 â†’ Ausgabe 1 (aktiv)."],"explanationJa":["ã—ãã„å€¤é–¢æ•°ï¼ˆã¾ãŸã¯Heavisideé–¢æ•°ï¼‰ã¯ã€æœ€ã‚‚åŸºæœ¬çš„ãªæ´»æ€§åŒ–é–¢æ•°ã§ã€å‡ºåŠ›ã¯0ã¾ãŸã¯1ã®ã¿ã«ãªã‚Šã¾ã™ã€‚","å…¥åŠ›å€¤vãŒ0æœªæº€ã®ã¨ãã¯å‡ºåŠ›ãŒ0ã€0ä»¥ä¸Šã®ã¨ãã¯å‡ºåŠ›ãŒ1ã«ãªã‚Šã¾ã™ã€‚","ã¤ã¾ã‚Šã€å…¥åŠ›ãŒã—ãã„å€¤ã‚’è¶…ãˆã‚‹ã¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒæ´»æ€§åŒ–ã•ã‚Œã€ãã†ã§ãªã‘ã‚Œã°éæ´»æ€§ã®ã¾ã¾ã§ã™ã€‚","ã“ã®ã‚ˆã†ã«ã€ã€Œå‡ºåŠ›ã™ã‚‹ã‹ã—ãªã„ã‹ã€ã¨ã„ã†2æŠã®å‹•ä½œã§ã‚ã‚‹ã“ã¨ã‹ã‚‰ã€ã€å…¨ã‹ç„¡ã‹ï¼ˆAll-or-Nothingï¼‰ã€å‹ã®é–¢æ•°ã¨å‘¼ã°ã‚Œã¾ã™ã€‚","ä¾‹ï¼šv = -0.2 ã®å ´åˆã¯å‡ºåŠ›0ï¼ˆéæ´»æ€§ï¼‰ã€v = 0.3 ã®å ´åˆã¯å‡ºåŠ›1ï¼ˆæ´»æ€§ï¼‰ã«ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"Schwellenwertfunktion:\\nâ€“ Nimmt nur die Werte 0 und 1 an\\nâ€“ Schwellwert bestimmt die Aktivierung\\nâ€“ Alles oder Nichts Funktionsweise\\nÏ†^hlm(v) = {1 wenn v â‰¥ 0, 0 wenn v < 0}","explanationImage":"lecture01/lecture09_ex03.png","questionImage":""},{"id":13,"questionDe":"(s19) Was ist eine stÃ¼ckweise lineare Aktivierungsfunktion und wie funktioniert sie?","questionJa":"â˜…åŒºåˆ†çš„ç·šå½¢æ´»æ€§åŒ–é–¢æ•°ã¨ã¯ä½•ã§ã€ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ï¼Ÿ","answerDe":["Abbildung eines begrenzten Intervalls mit linearer Funktion","AuÃŸerhalb des Intervalls konstante Werte (0 oder 1)","Funktion nimmt Werte zwischen 0 und 1 an"],"answerJa":["ä¸€å®šç¯„å›²å†…ã‚’ç·šå½¢é–¢æ•°ã§è¡¨ç¾ã™ã‚‹æ´»æ€§åŒ–é–¢æ•°","ç¯„å›²å¤–ã¯ä¸€å®šã®å€¤ï¼ˆ0ã¾ãŸã¯1ï¼‰ã‚’å–ã‚‹","å‡ºåŠ›å€¤ã¯0ã‹ã‚‰1ã®é–“ã®å€¤ã‚’ã¨ã‚‹"],"explanationDe":["Die stÃ¼ckweise lineare Funktion ist eine Aktivierungsfunktion, die fÃ¼r Werte innerhalb eines Intervalls linear ansteigt.","Genauer: FÃ¼r Werte v zwischen -0,5 und 0,5 wÃ¤chst die Ausgabe linear von 0 auf 1 an.","FÃ¼r v â‰¤ -0,5 ist die Ausgabe konstant 0, fÃ¼r v â‰¥ 0,5 konstant 1.","Das bedeutet, die Funktion begrenzt die Ausgabe auf den Bereich zwischen 0 und 1 und sorgt fÃ¼r eine sanfte Ãœbergangsphase.","Beispiel: Bei v = 0 ist der Ausgabewert 0,5 (Mitte des Intervalls), bei v = 0,3 ist der Ausgabewert etwa 0,8."],"explanationJa":["åŒºåˆ†çš„ç·šå½¢é–¢æ•°ã¯ã€ã‚ã‚‹ç¯„å›²å†…ã§å…¥åŠ›ã«å¯¾ã—ã¦å‡ºåŠ›ãŒç·šå½¢ã«å¤‰åŒ–ã™ã‚‹æ´»æ€§åŒ–é–¢æ•°ã§ã™ã€‚","å…·ä½“çš„ã«ã¯ã€å…¥åŠ›vãŒ-0.5ã‹ã‚‰0.5ã®ç¯„å›²ã§ã¯ã€å‡ºåŠ›ãŒ0ã‹ã‚‰1ã¾ã§ç›´ç·šçš„ã«å¢—åŠ ã—ã¾ã™ã€‚","å…¥åŠ›ãŒ-0.5ä»¥ä¸‹ã®ã¨ãã¯å‡ºåŠ›ã¯0ã€0.5ä»¥ä¸Šã®ã¨ãã¯å‡ºåŠ›ã¯1ã§ä¸€å®šã¨ãªã‚Šã¾ã™ã€‚","ã“ã®é–¢æ•°ã¯å‡ºåŠ›å€¤ã‚’0ã€œ1ã«åˆ¶é™ã—ã€æ»‘ã‚‰ã‹ãªåˆ‡ã‚Šæ›¿ãˆã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚","ä¾‹ï¼šv=0ã®ã¨ãå‡ºåŠ›ã¯0.5ï¼ˆä¸­é–“ç‚¹ï¼‰ã€v=0.3ã®ã¨ãã¯ç´„0.8ã¨ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"StÃ¼ckweise lineare Funktion:\\nâ€“ Abbildung eines begrenzten Intervalls mit einer linearen Funktion\\nâ€“ AuÃŸerhalb konstante Werte\\nÏ†^pwl(v) = {1 wenn v â‰¥ 1/2, v+1/2 wenn -1/2 < v < 1/2, 0 wenn v â‰¤ -1/2}","explanationImage":"lecture01/lecture09_ex04.png","questionImage":""},{"id":14,"questionDe":"(s20) Was ist die Sigmoidfunktion und welche Eigenschaften hat sie?","questionJa":"â˜…ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã¨ã¯ä½•ã§ã€ã©ã®ã‚ˆã†ãªç‰¹å¾´ã‚’æŒã¤ã‹ï¼Ÿ","answerDe":["Wurde lange Zeit als Standard-Aktivierungsfunktion verwendet","Hat eine glatte, S-fÃ¶rmige Kurve","Steigungsparameter alpha kann angepasst werden","Gibt Werte zwischen 0 und 1 aus"],"answerJa":["é•·ã„é–“ã€æ¨™æº–çš„ãªæ´»æ€§åŒ–é–¢æ•°ã¨ã—ã¦ä½¿ã‚ã‚Œã¦ããŸ","â˜…æ»‘ã‚‰ã‹ã§Så­—å‹ã®æ›²ç·šã‚’æŒã¤","â˜…å‚¾ãï¼ˆalphaï¼‰ã‚’èª¿æ•´ã§ãã‚‹","å‡ºåŠ›å€¤ã¯0ã‹ã‚‰1ã®é–“ã‚’ã¨ã‚‹"],"explanationDe":["Die Sigmoidfunktion ist eine nichtlineare Aktivierungsfunktion, die eine S-fÃ¶rmige Kurve beschreibt.","Sie wird definiert als: Ï†_alpha(v) = 1 / (1 + exp(-alpha * v)), wobei alpha die Steilheit der Kurve bestimmt.","Durch Variation von alpha kann die Funktion steiler oder flacher gemacht werden, was das Lernverhalten eines neuronalen Netzes beeinflusst.","Historisch wurde die Sigmoidfunktion hÃ¤ufig verwendet, da sie Werte zwischen 0 und 1 ausgibt und dadurch gut fÃ¼r Wahrscheinlichkeitsinterpretationen geeignet ist.","Allerdings wurde sie in neueren Netzwerken teilweise durch andere Funktionen wie ReLU ersetzt, da sie Probleme mit verschwindenden Gradienten hat."],"explanationJa":["ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã¯éç·šå½¢ã®æ´»æ€§åŒ–é–¢æ•°ã§ã€æ»‘ã‚‰ã‹ãªSå­—å‹ã®æ›²ç·šã‚’æãã¾ã™ã€‚","æ•°å¼ã¯ Ï†_alpha(v) = 1 / (1 + exp(-alpha * v)) ã§è¡¨ã•ã‚Œã€alphaã¯æ›²ç·šã®å‚¾ãã‚’èª¿æ•´ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚","alphaã‚’å¤‰ãˆã‚‹ã“ã¨ã§æ›²ç·šã®æ€¥å‹¾é…å…·åˆã‚’å¤‰ãˆã‚‰ã‚Œã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’æŒ™å‹•ã«å½±éŸ¿ã—ã¾ã™ã€‚","æ­´å²çš„ã«ã¯ã€å‡ºåŠ›ãŒ0ã‹ã‚‰1ã®é–“ã§å€¤ã‚’å–ã‚‹ãŸã‚ã€ç¢ºç‡çš„ãªè§£é‡ˆãŒã—ã‚„ã™ãæ¨™æº–çš„ãªæ´»æ€§åŒ–é–¢æ•°ã¨ã—ã¦å¤šç”¨ã•ã‚Œã¦ãã¾ã—ãŸã€‚","â˜…ã—ã‹ã—ã€å‹¾é…æ¶ˆå¤±å•é¡Œãªã©ã®èª²é¡Œã‹ã‚‰ã€æœ€è¿‘ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã¯ReLUãªã©ä»–ã®é–¢æ•°ãŒä½¿ã‚ã‚Œã‚‹ã“ã¨ã‚‚å¢—ãˆã¦ã„ã¾ã™ã€‚"],"originalSlideText":"Sigmoid Funktion:\\nâ€“ Wurden lange als Standardfunktion genutzt\\nâ€“ SteigungsmaÃŸ alpha kann modifiziert werden\\nÏ†^sig_Î±(v) = 1 / (1 + exp(-Î±v))","explanationImage":"lecture01/lecture09_ex05.png","questionImage":""},{"id":15,"questionDe":"(s21) Was ist die Rectifier (ReLU) Funktion und wie funktioniert sie?","questionJa":"â˜…Rectifierï¼ˆReLUï¼‰é–¢æ•°ã¨ã¯ä½•ã§ã€ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ï¼Ÿ","answerDe":["ReLU steht fÃ¼r \'rectified linear activation unit\'","Abwandlung der stÃ¼ckweisen linearen Funktion","Nur der positive Teil des Inputs wird linear abgebildet","Ausgabe ist max(0, v)"],"answerJa":["ReLUã¯ã€æ•´æµç·šå½¢æ´»æ€§åŒ–å˜ä½ã€ã®ç•¥","åŒºåˆ†çš„ç·šå½¢é–¢æ•°ã®ä¸€ç¨®ã®å¤‰å½¢","â˜…å…¥åŠ›ã®æ­£ã®éƒ¨åˆ†ã®ã¿ã‚’ç·šå½¢ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹","å‡ºåŠ›ã¯ max(0, v) ã§è¡¨ã•ã‚Œã‚‹"],"explanationDe":["Die ReLU-Funktion gibt fÃ¼r negative Eingaben 0 aus und fÃ¼r positive Eingaben den Wert der Eingabe selbst zurÃ¼ck.","Das heiÃŸt, fÃ¼r v < 0 ist die Ausgabe 0, und fÃ¼r v â‰¥ 0 ist die Ausgabe gleich v.","Diese Funktion ist einfach und effektiv, da sie NichtlinearitÃ¤t einfÃ¼hrt, aber gleichzeitig Rechenaufwand gering hÃ¤lt.","ReLU ist eine der beliebtesten Aktivierungsfunktionen in modernen neuronalen Netzwerken, da sie das Problem des verschwindenden Gradienten verringert.","Beispiel: FÃ¼r v = -2 ist die Ausgabe 0, fÃ¼r v = 3 ist die Ausgabe 3."],"explanationJa":["ReLUé–¢æ•°ã¯ã€å…¥åŠ›ãŒè² ã®ã¨ãã¯å‡ºåŠ›ã‚’0ã«ã—ã€æ­£ã®ã¨ãã¯å…¥åŠ›å€¤ãã®ã‚‚ã®ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚","ã¤ã¾ã‚Šã€v < 0 ã®å ´åˆã¯å‡ºåŠ›0ã€v â‰¥ 0 ã®å ´åˆã¯å‡ºåŠ›vã¨ãªã‚Šã¾ã™ã€‚","ã“ã®é–¢æ•°ã¯å˜ç´”ã§è¨ˆç®—ã‚³ã‚¹ãƒˆãŒä½ãã€éç·šå½¢æ€§ã‚’æŒãŸã›ã‚‹ã®ã«åŠ¹æœçš„ã§ã™ã€‚","ç¾ä»£ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹æ´»æ€§åŒ–é–¢æ•°ã§ã€å‹¾é…æ¶ˆå¤±å•é¡Œã®ç·©å’Œã«å½¹ç«‹ã£ã¦ã„ã¾ã™ã€‚","ä¾‹ï¼šv = -2 ã®ã¨ãã¯å‡ºåŠ›0ã€v = 3 ã®ã¨ãã¯å‡ºåŠ›3ã¨ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"Rectifier (ReLU) Funktion\\nâ€“ rectified linear activation unit\\nâ€“ Abwandlung der stÃ¼ckweisen linearen Funktion\\nâ€“ Nur positiver Teil wird linear abgebildet\\nÏ†(v) = max(0, v)","explanationImage":"lecture01/lecture09_ex06.png","questionImage":""},{"id":16,"questionDe":"(s22) Welche Eigenschaften hat ein Outputneuron in einem neuronalen Netz und wie bildet es eine Trennlinie zwischen zwei Klassen?","questionJa":"ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«ã¯ã©ã‚“ãªç‰¹å¾´ãŒã‚ã‚Šã€2ã¤ã®ã‚¯ãƒ©ã‚¹ã®å¢ƒç•Œç·šã‚’ã©ã®ã‚ˆã†ã«ä½œã‚‹ã‹ï¼Ÿ","answerDe":["Hat einen Schwellwert s","Berechnet AktivitÃ¤t aus Inputs und Gewichten","Nutzt Aktivierungsfunktion zur Berechnung des Outputs","Historisch wurde oft die Sprungfunktion verwendet","Erzeugt eine lineare Trennlinie zwischen zwei Klassen","Trennlinie kann als Ungleichung x2 â‰¥ (Î¸/w2) - (w1/w2) x1 dargestellt werden"],"answerJa":["ã—ãã„å€¤sã‚’æŒã¤","å…¥åŠ›ã¨é‡ã¿ã‹ã‚‰æ´»æ€§åº¦ã‚’è¨ˆç®—ã™ã‚‹","æ´»æ€§åŒ–é–¢æ•°ã‚’ä½¿ã£ã¦å‡ºåŠ›ã‚’æ±ºå®šã™ã‚‹","æ­´å²çš„ã«ã‚¹ãƒ†ãƒƒãƒ—é–¢æ•°ï¼ˆã‚¹ã‚¤ãƒƒãƒçš„ãªæ´»æ€§åŒ–ï¼‰ãŒä½¿ã‚ã‚Œã¦ããŸ","2ã¤ã®ã‚¯ãƒ©ã‚¹ã‚’åˆ†ã‘ã‚‹ç›´ç·šï¼ˆç·šå½¢åˆ†é›¢å¢ƒç•Œï¼‰ã‚’ä½œã‚‹","å¢ƒç•Œç·šã¯ä¸ç­‰å¼ x2 â‰¥ (Î¸/w2) - (w1/w2) x1 ã¨ã—ã¦è¡¨ã›ã‚‹"],"explanationDe":["Ein Outputneuron in einem neuronalen Netz hat einen Schwellwert s, der bestimmt, ab wann das Neuron aktiviert wird.","Die AktivitÃ¤t des Neurons wird aus den EingÃ¤ngen und den entsprechenden Gewichten berechnet, indem die gewichtete Summe gebildet wird.","AnschlieÃŸend wird eine Aktivierungsfunktion angewandt, um zu entscheiden, ob das Neuron \'feuert\' (z.â€¯B. Ausgabe 1) oder nicht (Ausgabe 0).","Historisch wurde hierfÃ¼r oft die Sprungfunktion (Schwellenwertfunktion) verwendet, die eine klare Trennung zwischen aktiv und inaktiv ermÃ¶glicht.","Dadurch entsteht im Eingaberaum eine lineare Trennlinie, die zwei Klassen J (Ja) und N (Nein) separiert.","Mathematisch kann man diese Trennlinie umstellen, sodass x2 abhÃ¤ngig von x1 durch eine lineare Gleichung oder Ungleichung beschrieben wird: x2 â‰¥ (Î¸/w2) - (w1/w2) x1."],"explanationJa":["ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€ã—ãã„å€¤sã‚’æŒã¡ã€ã“ã‚Œã‚’è¶…ãˆã‚‹ã¨æ´»æ€§åŒ–ã•ã‚Œã¾ã™ã€‚","å…¥åŠ›å€¤ã¨é‡ã¿ã®ç©ã®åˆè¨ˆã‚’è¨ˆç®—ã—ã€ãã®å€¤ã‚’åŸºã«æ´»æ€§åŒ–é–¢æ•°ã§å‡ºåŠ›ãŒæ±ºã¾ã‚Šã¾ã™ã€‚","æ­´å²çš„ã«ã¯ã€ã“ã®æ´»æ€§åŒ–ã«ã‚¹ãƒ†ãƒƒãƒ—é–¢æ•°ï¼ˆã‚¹ã‚¤ãƒƒãƒã®ã‚ˆã†ã«0ã‹1ã‹ã®åˆ¤æ–­ã‚’ã™ã‚‹é–¢æ•°ï¼‰ãŒã‚ˆãä½¿ã‚ã‚Œã¦ãã¾ã—ãŸã€‚","ã“ã®ä»•çµ„ã¿ã«ã‚ˆã‚Šã€å…¥åŠ›ç©ºé–“ã«ãŠã„ã¦2ã¤ã®ã‚¯ãƒ©ã‚¹ã‚’åˆ†ã‘ã‚‹ç·šå½¢ã®å¢ƒç•Œç·šï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¹³é¢ï¼‰ãŒå½¢æˆã•ã‚Œã¾ã™ã€‚","ã“ã®å¢ƒç•Œç·šã¯æ•°å¼ã§è¡¨ç¾å¯èƒ½ã§ã€x2ã¯x1ã®é–¢æ•°ã¨ã—ã¦ x2 â‰¥ (Î¸/w2) - (w1/w2) x1 ã¨ã„ã†å½¢ã«ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"Outputneuron hat:\\nâ€“ Schwellwert s\\nâ€“ AktivitÃ¤t a aus den Inputs und Gewichten\\nâ€“ Nutzt Aktivierungsfunktion, um den Output zu berechnen\\nâ€“ Historisch wurde die Sprungfunktion verwendet\\nâ€“ Bildet eine Trennlinie zwischen 2 Klassen\\nâ€“ Umgestellt nach x2: x2 â‰¥ (Î¸/w2) - (w1/w2) x1","explanationImage":"","questionImage":""},{"id":17,"questionDe":"(s23) Wie bildet ein Outputneuron eine lineare Trennlinie zwischen zwei Klassen und wie sieht ein Beispiel dafÃ¼r aus?","questionJa":"â˜…å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã©ã®ã‚ˆã†ã«ã—ã¦2ã‚¯ãƒ©ã‚¹é–“ã®ç·šå½¢åˆ†é›¢å¢ƒç•Œã‚’ä½œã‚Šã€å…·ä½“ä¾‹ã¯ã©ã®ã‚ˆã†ãªã‚‚ã®ã‹ï¼Ÿ","answerDe":["Erzeugt eine lineare Ungleichung: w1 x1 + w2 x2 â‰¥ Î¸","Umgestellt nach x2: x2 â‰¥ (Î¸/w2) - (w1/w2) x1","Beispiel: 0.9 x1 + 0.8 x2 â‰¥ 0.6","Umgestellt: x2 â‰¥ 3/4 - (9/8) x1","Diese Ungleichung beschreibt die Trennlinie, die die Klassen J und N separiert"],"answerJa":["ç·šå½¢ä¸ç­‰å¼ w1 x1 + w2 x2 â‰¥ Î¸ ã‚’ç”¨ã„ã¦å¢ƒç•Œç·šã‚’ä½œã‚‹","x2ã«ã¤ã„ã¦æ•´ç†ã™ã‚‹ã¨ x2 â‰¥ (Î¸/w2) - (w1/w2) x1 ã¨ãªã‚‹","å…·ä½“ä¾‹ï¼š0.9 x1 + 0.8 x2 â‰¥ 0.6","æ•´ç†ã™ã‚‹ã¨ x2 â‰¥ 3/4 - (9/8) x1 ã¨ãªã‚‹","ã“ã®ä¸ç­‰å¼ãŒã‚¯ãƒ©ã‚¹Jã¨Nã‚’åˆ†ã‘ã‚‹ç·šå½¢å¢ƒç•Œç·šã‚’è¡¨ã™"],"explanationDe":["Das Outputneuron erzeugt eine lineare Entscheidung anhand der gewichteten Summe seiner EingÃ¤nge verglichen mit dem Schwellwert Î¸.","Mathematisch lÃ¤sst sich die Entscheidung als Ungleichung darstellen, die eine Gerade im zweidimensionalen Raum definiert.","Durch Umstellen der Gleichung nach x2 erhÃ¤lt man eine Funktion, die auf der x1-Achse basiert und angibt, ab welchem Wert x2 zur Klasse J oder N gehÃ¶rt.","Im Beispiel mit den Gewichten 0.9 und 0.8 und dem Schwellwert 0.6 beschreibt die Gleichung die Trennlinie zwischen den Klassen.","Im Diagramm trennt die Linie die Klasse J (oberhalb) von Klasse N (unterhalb)."],"explanationJa":["å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€å…¥åŠ›ã«é‡ã¿ã‚’ã‹ã‘ãŸåˆè¨ˆã¨ã—ãã„å€¤Î¸ã‚’æ¯”è¼ƒã—ã¦ç·šå½¢ãªåˆ¤æ–­ã‚’ã—ã¾ã™ã€‚","ã“ã®åˆ¤æ–­ã¯2æ¬¡å…ƒç©ºé–“ä¸Šã®ç›´ç·šï¼ˆç·šå½¢ä¸ç­‰å¼ï¼‰ã§è¡¨ã•ã‚Œã¾ã™ã€‚","ä¸ç­‰å¼ã‚’x2ã«ã¤ã„ã¦æ•´ç†ã™ã‚‹ã¨ã€x1ã®é–¢æ•°ã¨ã—ã¦è¡¨ç¾ã•ã‚Œã€ã©ã®å€¤ã‹ã‚‰x2ãŒã‚¯ãƒ©ã‚¹Jã¾ãŸã¯Nã«åˆ†é¡ã•ã‚Œã‚‹ã‹ãŒã‚ã‹ã‚Šã¾ã™ã€‚","ä¾‹ã¨ã—ã¦ã€é‡ã¿0.9ã¨0.8ã€ã—ãã„å€¤0.6ã®å ´åˆã€ã“ã®ä¸ç­‰å¼ãŒã‚¯ãƒ©ã‚¹ã®åˆ†å‰²ç·šã¨ãªã‚Šã¾ã™ã€‚","å›³ã§ã¯ã€ã“ã®ç·šãŒã‚¯ãƒ©ã‚¹Jï¼ˆä¸Šå´ï¼‰ã¨ã‚¯ãƒ©ã‚¹Nï¼ˆä¸‹å´ï¼‰ã‚’åˆ†ã‘ã¦ã„ã¾ã™ã€‚"],"originalSlideText":"Lineare Trennlinie:\\nw1 x1 + w2 x2 â‰¥ Î¸\\nUmgestellt nach x2:\\nx2 â‰¥ (Î¸/w2) - (w1/w2) x1\\nBeispiel:\\n0.9 x1 + 0.8 x2 â‰¥ 0.6\\nx2 â‰¥ 3/4 - (9/8) x1","explanationImage":"lecture01/lecture09_ex07.png","questionImage":""},{"id":18,"questionDe":"(s24) Wie kÃ¶nnen neuronale Netze die logischen Funktionen AND, OR, NAND und NOR durch lineare Trennung darstellen?","questionJa":"â˜…ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ANDã€ORã€NANDã€NORã®è«–ç†é–¢æ•°ã‚’ã©ã®ã‚ˆã†ã«ç·šå½¢åˆ†é›¢ã§è¡¨ç¾ã§ãã‚‹ã‹ï¼Ÿ","answerDe":["AND: Punkte, bei denen beide EingÃ¤nge 1 sind, werden durch eine Gerade getrennt","OR: Punkte, bei denen mindestens ein Eingang 1 ist, liegen auf einer Seite der Trennlinie","NAND: Umkehrung von AND, auch linear trennbar","NOR: Umkehrung von OR, ebenfalls linear trennbar"],"answerJa":["AND: ä¸¡æ–¹ã®å…¥åŠ›ãŒ1ã®ç‚¹ãŒç›´ç·šã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹","OR: ã©ã¡ã‚‰ã‹ä¸€æ–¹ã®å…¥åŠ›ãŒ1ã®ç‚¹ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç·šã®ä¸€å´ã«ã‚ã‚‹","NAND: ANDã®å¦å®šã§ã€ã“ã¡ã‚‰ã‚‚ç·šå½¢åˆ†é›¢å¯èƒ½","NOR: ORã®å¦å®šã§ã€ã“ã¡ã‚‰ã‚‚ç·šå½¢åˆ†é›¢å¯èƒ½"],"explanationDe":["Neuronale Netze mit einem einzelnen Perzeptron kÃ¶nnen einfache logische Funktionen wie AND, OR, NAND und NOR modellieren.","Diese Funktionen sind linear trennbar, das heiÃŸt, es gibt eine Gerade, die die verschiedenen Ausgabewerte (0 oder 1) sauber trennt.","Im Diagramm reprÃ¤sentieren schwarze Punkte die Klasse 1 und weiÃŸe Punkte die Klasse 0.","Die Trennlinien in den Abbildungen zeigen, wie die Eingabepaare (x1, x2) entsprechend der logischen Funktion klassifiziert werden."],"explanationJa":["å˜ç´”ãªPerceptronã¯ã€ANDã€ORã€NANDã€NORã¨ã„ã£ãŸåŸºæœ¬çš„ãªè«–ç†é–¢æ•°ã‚’è¡¨ç¾ã§ãã¾ã™ã€‚","ã“ã‚Œã‚‰ã®é–¢æ•°ã¯ç·šå½¢åˆ†é›¢å¯èƒ½ã§ã‚ã‚Šã€ç•°ãªã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆ0ã¾ãŸã¯1ï¼‰ã‚’åˆ†ã‘ã‚‹ç›´ç·šãŒå­˜åœ¨ã—ã¾ã™ã€‚","å›³ã§ã¯ã€é»’ä¸¸ãŒã‚¯ãƒ©ã‚¹1ï¼ˆTrueï¼‰ã€ç™½ä¸¸ãŒã‚¯ãƒ©ã‚¹0ï¼ˆFalseï¼‰ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚","ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç·šã¯å…¥åŠ›ãƒšã‚¢ï¼ˆx1ã€x2ï¼‰ã‚’è«–ç†é–¢æ•°ã«åŸºã¥ã„ã¦åˆ†é¡ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"],"originalSlideText":"AND, OR, NAND, NOR als lineare Trennungsfunktionen","explanationImage":"lecture01/lecture09_ex08.png","questionImage":""},{"id":19,"questionDe":"(s25) Wie funktioniert das Lernverfahren in neuronalen Netzen und welche Voraussetzungen gibt es?","questionJa":"ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’æ‰‹æ³•ã¯ã©ã®ã‚ˆã†ã«å‹•ä½œã—ã€ã©ã‚“ãªå‰ææ¡ä»¶ãŒã‚ã‚‹ã‹ï¼Ÿ","answerDe":["Gegeben ist ein Trainingsdatensatz mit zwei disjunkten Mengen x und y","Gesucht wird eine trennende Hyperebene basierend auf den Gewichten","Die Hyperebene teilt die Daten in die Klassen x und y","Problem: x und y mÃ¼ssen linear trennbar sein"],"answerJa":["2ã¤ã®äº’ã„ã«é‡ãªã‚‰ãªã„ãƒ‡ãƒ¼ã‚¿é›†åˆxã¨yã‚’å«ã‚€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒä¸ãˆã‚‰ã‚Œã‚‹","é‡ã¿ã«åŸºã¥ã„ã¦ã€ã“ã‚Œã‚‰ã‚’åˆ†ã‘ã‚‹å¢ƒç•Œã¨ãªã‚‹è¶…å¹³é¢ã‚’æ¢ã™","è¶…å¹³é¢ã¯xã¨yã®2ã¤ã®ã‚¯ãƒ©ã‚¹ã‚’åˆ†å‰²ã™ã‚‹","å‰ææ¡ä»¶ã¨ã—ã¦xã¨yã¯ç·šå½¢ã«åˆ†é›¢å¯èƒ½ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹"],"explanationDe":["Beim Lernverfahren eines neuronalen Netzes wird ein Trainingsdatensatz verwendet, der in zwei klare Gruppen (Klassen) aufgeteilt ist, beispielsweise Klasse x und Klasse y.","Das Ziel ist es, eine Hyperebene zu finden â€“ eine Art Grenze â€“ die diese beiden Klassen voneinander trennt, so dass alle Punkte von Klasse x auf einer Seite und alle von Klasse y auf der anderen Seite liegen.","Diese Hyperebene wird durch die Gewichtungen des neuronalen Netzes definiert und ist entscheidend dafÃ¼r, wie das Netz neue, unbekannte Daten klassifiziert.","Wichtig ist dabei, dass die beiden Klassen tatsÃ¤chlich linear trennbar sind, das heiÃŸt, dass es mÃ¶glich ist, sie durch eine gerade Linie oder FlÃ¤che ohne Ãœberschneidungen zu trennen.","Wenn die Daten nicht linear trennbar sind, reicht dieses einfache Lernverfahren nicht aus und es werden komplexere Modelle benÃ¶tigt."],"explanationJa":["ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’ã§ã¯ã€ã¾ãšè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒ2ã¤ã®ã¯ã£ãã‚Šåˆ†ã‹ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆã‚¯ãƒ©ã‚¹ï¼‰ã€ä¾‹ãˆã°ã‚¯ãƒ©ã‚¹xã¨ã‚¯ãƒ©ã‚¹yã«åˆ†ã‘ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚","å­¦ç¿’ã®ç›®çš„ã¯ã€ã“ã®2ã¤ã®ã‚¯ãƒ©ã‚¹ã‚’åˆ†ã‘ã‚‹å¢ƒç•Œç·šã‚„å¢ƒç•Œé¢ï¼ˆè¶…å¹³é¢ï¼‰ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã§ã™ã€‚ã“ã®å¢ƒç•Œã«ã‚ˆã‚Šã€ã‚¯ãƒ©ã‚¹xã®ãƒ‡ãƒ¼ã‚¿ã¯ä¸€æ–¹ã®å´ã«ã€ã‚¯ãƒ©ã‚¹yã®ãƒ‡ãƒ¼ã‚¿ã¯ã‚‚ã†ä¸€æ–¹ã®å´ã«é…ç½®ã•ã‚Œã¾ã™ã€‚","ã“ã®å¢ƒç•Œç·šã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®é‡ã¿ã«ã‚ˆã£ã¦æ±ºã¾ã‚Šã€ã“ã®é‡ã¿ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§æ–°ã—ã„æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãåˆ†é¡ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚","é‡è¦ãªã®ã¯ã€2ã¤ã®ã‚¯ãƒ©ã‚¹ãŒç·šå½¢ã«åˆ†é›¢å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã§ã™ã€‚ã¤ã¾ã‚Šã€ç›´ç·šã‚„å¹³é¢ã§ãã‚Œã„ã«åˆ†ã‘ã‚‰ã‚Œã‚‹çŠ¶æ…‹ã§ãªã‘ã‚Œã°ã€ã“ã®å­¦ç¿’æ–¹æ³•ã¯ã†ã¾ãæ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚","ã‚‚ã—ç·šå½¢åˆ†é›¢ã§ããªã„å ´åˆã¯ã€ã“ã®å˜ç´”ãªæ–¹æ³•ã§ã¯å¯¾å¿œã§ããšã€ã‚ˆã‚Šè¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã‚„å¤šå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"Lernverfahren\\nâ€“ Gegeben ist ein Trainingsdatensatz\\n  â€“ Daten sind disjunkt in 2 Menge x und y aufgeteilt\\n  â€“ Gesucht wird eine trennende Hyperebene\\n  â€“ Basiert auf den Gewichten\\n  â€“ Teilt x und y auf\\nâ€“ Problem: x und y mÃ¼ssen linear trennbar sein!","explanationImage":"","questionImage":""},{"id":20,"questionDe":"(s26) Wie funktioniert das Lernverfahren der Delta Regel in neuronalen Netzen?","questionJa":"â˜…ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ãƒ‡ãƒ«ã‚¿å­¦ç¿’è¦å‰‡ã®å­¦ç¿’æ‰‹æ³•ã¯ã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã‹ï¼Ÿ","answerDe":["Beim Training werden Eingabedaten dem Netz gezeigt","Der korrekte Output fÃ¼r Beispiele ist bekannt (supervised learning)","Vergleich zwischen Soll- und Ist-Zustand im Output","Bei Abweichungen werden Schwellwert und Gewichte angepasst","Anpassung nur bei differenzierbaren Aktivierungsfunktionen mÃ¶glich","Gewichts- und Schwellwertanpassung kann nach jedem Input oder nach jeder Epoche erfolgen","Epoche = kompletter Durchlauf aller Trainingsdaten"],"answerJa":["è¨“ç·´æ™‚ã«ãƒãƒƒãƒˆã«å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ä¸ãˆã‚‹","å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ­£ã—ã„å‡ºåŠ›ãŒæ—¢çŸ¥ï¼ˆæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼‰","å®Ÿéš›ã®å‡ºåŠ›ã¨æ­£è§£ã‚’æ¯”è¼ƒã™ã‚‹","èª¤å·®ãŒã‚ã‚Œã°é‡ã¿ã¨ã—ãã„å€¤ã‚’èª¿æ•´ã™ã‚‹","â˜…å¾®åˆ†å¯èƒ½ãªæ´»æ€§åŒ–é–¢æ•°ã«ã®ã¿é©ç”¨å¯èƒ½","â˜…é‡ã¿ã®èª¿æ•´ã¯å…¥åŠ›ã”ã¨ã€ã¾ãŸã¯ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«è¡Œãˆã‚‹","â˜…ã‚¨ãƒãƒƒã‚¯ã¨ã¯å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®1å›åˆ†ã®å‡¦ç†"],"explanationDe":["Die Delta Regel ist ein grundlegendes Lernverfahren fÃ¼r neuronale Netze, das im Ã¼berwachten Lernen eingesetzt wird.","WÃ¤hrend des Trainings werden dem Netz Eingabebeispiele prÃ¤sentiert, deren korrekte Ausgaben (Labels) bekannt sind.","Das Netz berechnet eine Ausgabe, die mit der Soll-Ausgabe verglichen wird. Bei Abweichungen wird ein Fehler ermittelt.","Dieser Fehler wird genutzt, um die Gewichte und den Schwellwert des Neurons so anzupassen, dass der Fehler minimiert wird.","Die Methode funktioniert nur, wenn die Aktivierungsfunktion differenzierbar ist, da der Gradient zur Anpassung berechnet werden muss.","Die Anpassung kann nach jedem einzelnen Input-Datum erfolgen (Online-Lernen) oder nach der Verarbeitung aller Trainingsdaten (Batch-Lernen) â€“ ein kompletter Durchlauf wird Epoche genannt.","Durch diesen Prozess lernt das Netz, seine Parameter so zu verÃ¤ndern, dass die Ausgaben immer besser zu den gewÃ¼nschten Ergebnissen passen."],"explanationJa":["ãƒ‡ãƒ«ã‚¿å­¦ç¿’è¦å‰‡ã¯ã€æ•™å¸«ã‚ã‚Šå­¦ç¿’ã§ä½¿ã‚ã‚Œã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®åŸºæœ¬çš„ãªå­¦ç¿’æ–¹æ³•ã§ã™ã€‚","è¨“ç·´ã§ã¯ã€å…¥åŠ›ã¨ãã‚Œã«å¯¾å¿œã™ã‚‹æ­£è§£å‡ºåŠ›ã‚’ãƒãƒƒãƒˆã«ä¸ãˆã¾ã™ã€‚","ãƒãƒƒãƒˆã®å‡ºåŠ›ã¨æ­£è§£ã‚’æ¯”è¼ƒã—ã€èª¤å·®ã‚’è¨ˆç®—ã—ã¾ã™ã€‚","ã“ã®èª¤å·®ã«åŸºã¥ã„ã¦ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®é‡ã¿ã‚„ã—ãã„å€¤ã‚’èª¿æ•´ã—ã€èª¤å·®ãŒå°ã•ããªã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚","â˜…ã“ã®æ–¹æ³•ã¯æ´»æ€§åŒ–é–¢æ•°ãŒå¾®åˆ†å¯èƒ½ã§ã‚ã‚‹ã“ã¨ãŒæ¡ä»¶ã§ã€èª¤å·®ã®å¾®åˆ†ã‚’ä½¿ã£ã¦é‡ã¿ã‚’æ›´æ–°ã—ã¾ã™ã€‚","é‡ã¿ã®æ›´æ–°ã¯1ã¤1ã¤ã®å…¥åŠ›ã”ã¨ã«è¡Œã†ã“ã¨ã‚‚ã€å…¨ãƒ‡ãƒ¼ã‚¿ã®1å›åˆ†ã®å‡¦ç†ï¼ˆã‚¨ãƒãƒƒã‚¯ï¼‰ã”ã¨ã«ã¾ã¨ã‚ã¦è¡Œã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚","ã“ã†ã—ãŸèª¿æ•´ã‚’ç¹°ã‚Šè¿”ã™ã“ã¨ã§ã€ãƒãƒƒãƒˆã¯å¾ã€…ã«æœ›ã¾ã—ã„å‡ºåŠ›ã«è¿‘ã¥ã‘ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã¾ã™ã€‚"],"originalSlideText":"Lernverfahren Delta Regel\\nâ€“ Beim Training werden die Daten dem Netz als Input gezeigt\\nâ€“ Output fÃ¼r die Beispiele bekannt (supervised)\\nâ€“ Vergleich von Soll und Ist Zustand im Output\\nâ€“ Bei Abweichung werden Schwellwert und Gewichte adaptiert\\nâ€“ Nur auf differenzierbare Aktivierungsfunktionen anwendbar\\nâ€“ Anpassung nach jedem Input oder nach jeder Epoche mÃ¶glich\\nâ€“ Epoche = Ein Durchlauf aller Trainingsdaten","explanationImage":"","questionImage":""},{"id":21,"questionDe":"(s27) Wie funktioniert die Delta-Regel fÃ¼r das Lernen in neuronalen Netzen und was bedeuten die einzelnen Terme in der Formel?","questionJa":"ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’ã«ãŠã‘ã‚‹ãƒ‡ãƒ«ã‚¿å‰‡ã¯ã©ã®ã‚ˆã†ã«å‹•ä½œã—ã€å¼ã®å„é …ã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã‹ï¼Ÿ","answerDe":["Î”w_j_i = Î± (t_j - y_j) g\'(h_j) x_i beschreibt die Gewichtsanpassung","Î± ist die konstante Lernrate, die bestimmt, wie stark die Gewichte angepasst werden","g(x) ist die Aktivierungsfunktion, g\' deren Ableitung","t_j ist die Sollausgabe (gewÃ¼nschtes Ziel)","y_j ist die Ist-Ausgabe des Neurons","h_j ist die gewichtete Summe aller Eingaben: h_j = Î£ x_i w_j_i","x_i ist der Input i","Die gleiche Formel wird auch fÃ¼r die Anpassung des Schwellwertes verwendet"],"answerJa":["Î”w_j_i = Î± (t_j - y_j) g\'(h_j) x_i ã¯é‡ã¿ã®æ›´æ–°ã‚’è¡¨ã™å¼","Î±ã¯å­¦ç¿’ç‡ã§ã€é‡ã¿ã®èª¿æ•´ã®å¤§ãã•ã‚’æ±ºã‚ã‚‹å®šæ•°","g(x)ã¯æ´»æ€§åŒ–é–¢æ•°ã€g\'ã¯ãã®å¾®åˆ†ï¼ˆå‚¾ãï¼‰","t_jã¯ç›®æ¨™ã¨ãªã‚‹æ­£ã—ã„å‡ºåŠ›","y_jã¯å®Ÿéš›ã«ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå‡ºã—ãŸå‡ºåŠ›","h_jã¯å…¥åŠ›ã«é‡ã¿ã‚’ã‹ã‘ã¦åˆè¨ˆã—ãŸå€¤ï¼ˆh_j = Î£ x_i w_j_iï¼‰","x_iã¯iç•ªç›®ã®å…¥åŠ›ä¿¡å·","åŒã˜å¼ãŒã—ãã„å€¤ã®èª¿æ•´ã«ã‚‚ä½¿ã‚ã‚Œã‚‹"],"explanationDe":["Die Delta-Regel gibt an, wie die Gewichte in einem neuronalen Netz wÃ¤hrend des Trainings angepasst werden, um den Fehler zwischen Soll- und Ist-Ausgabe zu minimieren.","Der Term (t_j - y_j) ist der Fehler zwischen gewÃ¼nschter und tatsÃ¤chlicher Ausgabe des Neurons.","Die Ableitung der Aktivierungsfunktion g\'(h_j) gibt an, wie empfindlich das Neuron auf Ã„nderungen der gewichteten Summe h_j reagiert.","Die Lernrate Î± bestimmt, wie schnell die Gewichte geÃ¤ndert werden â€“ zu groÃŸe Werte kÃ¶nnen instabil machen, zu kleine Werte verzÃ¶gern das Lernen.","Die Eingabe x_i beeinflusst, wie stark das Gewicht w_j_i angepasst wird; wenn der Eingangswert null ist, Ã¤ndert sich das Gewicht nicht.","Dieser Prozess wird fÃ¼r alle Gewichte wiederholt und sorgt so dafÃ¼r, dass das Netz seine Ausgabe Schritt fÃ¼r Schritt verbessert.","ZusÃ¤tzlich wird die gleiche Methode verwendet, um den Schwellwert des Neurons anzupassen."],"explanationJa":["ãƒ‡ãƒ«ã‚¿å‰‡ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®è¨“ç·´ä¸­ã«é‡ã¿ã‚’ã©ã®ã‚ˆã†ã«èª¿æ•´ã—ã¦èª¤å·®ã‚’æ¸›ã‚‰ã™ã‹ã‚’ç¤ºã™å¼ã§ã™ã€‚","(t_j - y_j) ã¯ã€æ­£è§£ã®å‡ºåŠ›ã¨å®Ÿéš›ã®å‡ºåŠ›ã®å·®ï¼ˆèª¤å·®ï¼‰ã‚’è¡¨ã—ã¾ã™ã€‚","æ´»æ€§åŒ–é–¢æ•°ã®å¾®åˆ† g\'(h_j) ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å‡ºåŠ›ãŒå…¥åŠ›ã®é‡ã¿ä»˜ãå’Œ h_j ã®å¤‰åŒ–ã«ã©ã‚Œã ã‘æ•æ„Ÿã‹ã‚’ç¤ºã—ã¾ã™ã€‚","å­¦ç¿’ç‡ Î± ã¯é‡ã¿ã®å¤‰åŒ–ã®å¤§ãã•ã‚’æ±ºã‚ã‚‹å®šæ•°ã§ã€å¤§ãã™ãã‚‹ã¨å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚Šã€å°ã•ã™ãã‚‹ã¨å­¦ç¿’ãŒé…ããªã‚Šã¾ã™ã€‚","å…¥åŠ› x_i ã¯ã€ãã®å…¥åŠ›ãŒã©ã‚Œã ã‘é‡ã¿ã®èª¿æ•´ã«å½±éŸ¿ã™ã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚å…¥åŠ›ãŒ0ãªã‚‰ã°ã€ãã®é‡ã¿ã¯å¤‰ã‚ã‚Šã¾ã›ã‚“ã€‚","ã“ã®å‡¦ç†ã¯å…¨ã¦ã®é‡ã¿ã§ç¹°ã‚Šè¿”ã•ã‚Œã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå°‘ã—ãšã¤æ­£ã—ã„å‡ºåŠ›ã«è¿‘ã¥ãã‚ˆã†å­¦ç¿’ã—ã¾ã™ã€‚","ã•ã‚‰ã«åŒã˜å¼ã‚’ä½¿ã£ã¦ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã—ãã„å€¤ï¼ˆãƒã‚¤ã‚¢ã‚¹ï¼‰ã‚‚èª¿æ•´ã—ã¾ã™ã€‚"],"originalSlideText":"Lernverfahren Delta Regel\\nÎ”w_j_i = Î± (t_j - y_j) g\'(h_j) x_i\\nAlpha: konstante Lernrate\\ng(x) ist die Aktivierungsfunktion\\ng\' ist die Ableitung von g\\nt_j ist die Sollausgabe\\ny_j ist die Istausgabe\\nh_j = Î£ x_i w_j_i\\ny_j = g(h_j)\\nDie gleiche Formel wird auf den Schwellwert angewendet","explanationImage":"lecture01/lecture09_ex09.png","questionImage":""},{"id":22,"questionDe":"(s28) Wie wird der Schwellwert im Lernverfahren der Delta Regel behandelt?","questionJa":"ãƒ‡ãƒ«ã‚¿å­¦ç¿’å‰‡ã§ã¯ã—ãã„å€¤ã‚’ã©ã®ã‚ˆã†ã«æ‰±ã†ã‹ï¼Ÿ","answerDe":["Der Schwellwert Î¸ wird als ein zusÃ¤tzliches Gewicht betrachtet","Dieses zusÃ¤tzliche Gewicht wird mit einem konstanten Eingang x_0 = 1 multipliziert","Dadurch kann die Ungleichung umgeschrieben werden als Summe aller gewichteteten EingÃ¤nge minus Î¸ â‰¥ 0","Das ermÃ¶glicht eine einheitliche Behandlung von Gewicht und Schwellwert bei der Gewichtsanpassung"],"answerJa":["ã—ãã„å€¤Î¸ã¯è¿½åŠ ã®é‡ã¿ã¨ã—ã¦æ‰±ã‚ã‚Œã‚‹","ã“ã®è¿½åŠ ã®é‡ã¿ã¯å®šæ•°å…¥åŠ›x_0 = 1ã«æ›ã‘ã‚‰ã‚Œã‚‹","ä¸ç­‰å¼ã¯ã€Œã™ã¹ã¦ã®é‡ã¿ä»˜ãå…¥åŠ›ã®åˆè¨ˆï¼Î¸ â‰¥ 0ã€ã¨ã—ã¦æ›¸ãæ›ãˆã‚‰ã‚Œã‚‹","ã“ã‚Œã«ã‚ˆã‚Šã€é‡ã¿ã¨ã—ãã„å€¤ã‚’åŒã˜ã‚ˆã†ã«èª¿æ•´ã§ãã‚‹"],"explanationDe":["Im Lernverfahren der Delta Regel wird der Schwellwert Î¸ nicht separat behandelt, sondern als ein spezielles Gewicht w_0 interpretiert.","Dieses Gewicht w_0 wird mit einem fixen Eingang x_0 = 1 multipliziert, so dass es Teil der gewichteten Summe der Eingaben wird.","Dadurch kann man die ursprÃ¼ngliche Ungleichung âˆ‘ w_i x_i â‰¥ Î¸ in die Form âˆ‘ w_i x_i - Î¸ â‰¥ 0 umschreiben.","Mit dieser Umformung ist der Schwellwert in die Gewichtsanpassung integriert, was die Berechnung und Anpassung vereinfacht."],"explanationJa":["ãƒ‡ãƒ«ã‚¿å­¦ç¿’å‰‡ã§ã¯ã€ã—ãã„å€¤Î¸ã¯åˆ¥æ‰±ã„ã›ãšã€ç‰¹åˆ¥ãªé‡ã¿w_0ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚","ã“ã®é‡ã¿w_0ã¯å®šæ•°å…¥åŠ›x_0 = 1ã«ã‹ã‘ã‚‰ã‚Œã€å…¥åŠ›ã®é‡ã¿ä»˜ãå’Œã®ä¸€éƒ¨ã¨ãªã‚Šã¾ã™ã€‚","ã“ã‚Œã«ã‚ˆã‚Šã€å…ƒã®ä¸ç­‰å¼ âˆ‘ w_i x_i â‰¥ Î¸ ã‚’ âˆ‘ w_i x_i - Î¸ â‰¥ 0 ã¨ã„ã†å½¢ã«å¤‰å½¢ã§ãã¾ã™ã€‚","ã“ã®å¤‰å½¢ã«ã‚ˆã‚Šã€ã—ãã„å€¤ã‚‚ä»–ã®é‡ã¿ã¨åŒã˜ã‚ˆã†ã«èª¿æ•´å¯èƒ½ã«ãªã‚Šã€è¨ˆç®—ãŒã‚·ãƒ³ãƒ—ãƒ«ã«ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"Lernverfahren Delta Regel\\nDer Schwellwert ist damit auch nur ein Gewicht!\\nÎ£ w_i x_i â‰¥ Î¸\\nÎ£ w_i x_i - Î¸ â‰¥ 0\\nmit x_0 = 1, w_0 = -Î¸","explanationImage":"lecture01/lecture09_ex10.png","questionImage":""},{"id":23,"questionDe":"(s29) Wie funktioniert die Delta-Regel Lernverfahren anhand eines Beispiels mit initialem Schwellwert und Lernfaktor?","questionJa":"ãƒ‡ãƒ«ã‚¿å­¦ç¿’å‰‡ã®å­¦ç¿’æ‰‹é †ã‚’ã€åˆæœŸã—ãã„å€¤ã¨å­¦ç¿’ç‡ã‚’ç”¨ã„ãŸå…·ä½“ä¾‹ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚","answerDe":["Initialer Schwellwert ist -1","Lernfaktor (Alpha) ist 0.2","Trainingsbeispiele enthalten alle Kombinationen von Eingang1 und Eingang2 mit zugehÃ¶rigem Ausgang","Gewichte (Ï‰0, Ï‰1, Ï‰2) werden Schritt fÃ¼r Schritt angepasst","Die Tabellen zeigen die Gewichtswerte fÃ¼r jede Kombination von EingÃ¤ngen im Verlauf des Trainings"],"answerJa":["åˆæœŸã—ãã„å€¤ã¯ -1 ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹","å­¦ç¿’ç‡ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ï¼‰ã¯ 0.2","è¨“ç·´ä¾‹ã¯å…¥åŠ›1ã¨å…¥åŠ›2ã®ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›ã¨å¯¾å¿œã™ã‚‹å‡ºåŠ›ã‚’å«ã‚€","é‡ã¿ï¼ˆÏ‰0, Ï‰1, Ï‰2ï¼‰ã¯è¨“ç·´ã®é€²è¡Œã«å¾“ã£ã¦é€æ¬¡èª¿æ•´ã•ã‚Œã‚‹","è¡¨ã¯å„å…¥åŠ›çµ„ã¿åˆã‚ã›ã«å¯¾ã™ã‚‹é‡ã¿ã®å¤‰åŒ–ã‚’ç¤ºã—ã¦ã„ã‚‹"],"explanationDe":["In diesem Beispiel wird das Lernverfahren der Delta-Regel mit einem initialen Schwellwert von -1 und einem Lernfaktor von 0.2 demonstriert.","Die Trainingsdaten umfassen alle Kombinationen von zwei EingÃ¤ngen (0 und 1) mit den jeweiligen Soll-Ausgaben.","Die Gewichte Ï‰0, Ï‰1 und Ï‰2 werden jeweils fÃ¼r jeden Trainingsfall angepasst, um die Differenz zwischen Soll- und Ist-Ausgabe zu minimieren.","Die beiden Tabellen zeigen die Entwicklung der Gewichte fÃ¼r alle vier mÃ¶glichen Eingangskombinationen wÃ¤hrend des Trainings.","Dieses Beispiel verdeutlicht, wie das neuronale Netz durch schrittweise Anpassung der Gewichte lernt, die korrekten Ausgaben zu erzeugen."],"explanationJa":["ã“ã®ä¾‹ã§ã¯ã€ãƒ‡ãƒ«ã‚¿å­¦ç¿’å‰‡ã‚’ä½¿ã„ã€åˆæœŸã—ãã„å€¤ã‚’ -1ã€å­¦ç¿’ç‡ã‚’0.2ã«è¨­å®šã—ã¦ã„ã¾ã™ã€‚","è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯ã€2ã¤ã®å…¥åŠ›ï¼ˆ0ã¾ãŸã¯1ï¼‰ã®å…¨çµ„ã¿åˆã‚ã›ã¨ã€ãã‚Œã«å¯¾å¿œã™ã‚‹æ­£ã—ã„å‡ºåŠ›ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚","é‡ã¿Ï‰0ã€Ï‰1ã€Ï‰2ã¯å„è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã”ã¨ã«æ›´æ–°ã•ã‚Œã€æ­£è§£å‡ºåŠ›ã¨ã®èª¤å·®ã‚’å°ã•ãã™ã‚‹ã‚ˆã†èª¿æ•´ã•ã‚Œã¾ã™ã€‚","2ã¤ã®è¡¨ã¯ã€è¨“ç·´ä¸­ã«å„å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾ã—ã¦é‡ã¿ãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã—ã¦ã„ãã‹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚","ã“ã®å…·ä½“ä¾‹ã«ã‚ˆã£ã¦ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒã©ã®ã‚ˆã†ã«é‡ã¿ã®èª¿æ•´ã‚’é€šã—ã¦å­¦ç¿’ã—ã¦ã„ãã‹ãŒã‚ã‹ã‚Šã‚„ã™ãç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚"],"originalSlideText":"Lernverfahren Delta Regel\\nInitialer Schwellwert -1\\nLernfaktor 0.2\\nEingangsausgabe-Tabelle\\nGewichtstabelle vor und nach Training","explanationImage":"lecture01/lecture09_ex11.png","questionImage":""},{"id":24,"questionDe":"(s30) Was besagt der Satz Ã¼ber Konvergenz und Korrektheit der Delta Regel und welche Probleme kÃ¶nnen dabei auftreten?","questionJa":"ãƒ‡ãƒ«ã‚¿å‰‡ã®åæŸæ€§ã¨æ­£ç¢ºæ€§ã«é–¢ã™ã‚‹å®šç†ã¯ä½•ã‚’ç¤ºã—ã€ã©ã®ã‚ˆã†ãªå•é¡ŒãŒèµ·ã“ã‚Šã†ã‚‹ã‹ï¼Ÿ","answerDe":["Wenn ein Perzeptron eine Klasseneinteilung lernen kann, dann lernt es diese mit der Delta Regel in endlich vielen Schritten","Problem: Man kann nicht sicher sagen, ob genug Epochen gelernt wurden oder das Problem nicht erlernbar ist","Es gibt keine obere Schranke fÃ¼r die Lerndauer","Es besteht die Gefahr von Overfitting"],"answerJa":["ã‚‚ã—ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ãŒã‚ã‚‹ã‚¯ãƒ©ã‚¹åˆ†ã‘ã‚’å­¦ç¿’å¯èƒ½ãªã‚‰ã°ã€ãƒ‡ãƒ«ã‚¿å‰‡ã‚’ä½¿ã£ã¦æœ‰é™ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§ãã®å­¦ç¿’ã‚’çµ‚ãˆã‚‹ã“ã¨ãŒã§ãã‚‹","ãŸã ã—ã€ååˆ†ãªã‚¨ãƒãƒƒã‚¯æ•°ã‚’å­¦ç¿’ã—ãŸã®ã‹ã€ãã‚Œã¨ã‚‚ãã®å•é¡Œè‡ªä½“ãŒå­¦ç¿’ä¸å¯èƒ½ãªã®ã‹ã‚’é€”ä¸­ã§åˆ¤æ–­ã™ã‚‹ã“ã¨ã¯ã§ããªã„","å­¦ç¿’ã«ã‹ã‹ã‚‹æ™‚é–“ã®ä¸Šé™ã¯å­˜åœ¨ã—ãªã„","ã¾ãŸã€éå­¦ç¿’ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼‰ã®å•é¡Œã‚‚ã‚ã‚‹"],"explanationDe":["Der Satz zur Konvergenz und Korrektheit der Delta Regel garantiert, dass ein Perzeptron jede linear trennbare Klassifikation in einer endlichen Anzahl von Schritten lernen kann.","Das bedeutet, wenn das Problem lÃ¶sbar ist, wird das Netz mit genÃ¼gend Trainingsschritten die richtige Klassifikation erreichen.","Das Problem ist jedoch, dass man wÃ¤hrend des Trainings nicht sicher feststellen kann, ob bereits genÃ¼gend Epochen absolviert wurden oder ob das Problem Ã¼berhaupt nicht linear trennbar und damit nicht lÃ¶sbar ist.","Folglich gibt es keine garantierte obere Grenze fÃ¼r die Trainingsdauer, was das Training potenziell sehr lang machen kann.","AuÃŸerdem kann es passieren, dass das Netz zu lange trainiert und dabei die Trainingsdaten zu genau lernt, was als Overfitting bezeichnet wird und die Generalisierung auf neue Daten verschlechtert."],"explanationJa":["ãƒ‡ãƒ«ã‚¿å‰‡ã®åæŸæ€§ã¨æ­£ç¢ºæ€§ã«é–¢ã™ã‚‹å®šç†ã¯ã€ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ãŒç·šå½¢ã«åˆ†é›¢å¯èƒ½ãªå•é¡Œã«å¯¾ã—ã¦ã€æœ‰é™ã®ã‚¹ãƒ†ãƒƒãƒ—ã§æ­£ã—ã„ã‚¯ãƒ©ã‚¹åˆ†ã‘ã‚’å­¦ç¿’ã§ãã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚","ã¤ã¾ã‚Šã€å•é¡ŒãŒè§£ã‘ã‚‹ã‚‚ã®ã§ã‚ã‚Œã°ã€ååˆ†ãªå­¦ç¿’å›æ•°ã‚’çµŒã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯æ­£ã—ã„åˆ†é¡çµæœã‚’å‡ºã™ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚","ã—ã‹ã—ã€å­¦ç¿’ä¸­ã«ã©ã“ã¾ã§å­¦ç¿’ã‚’é€²ã‚ã‚Œã°ååˆ†ãªã®ã‹ã€ã‚ã‚‹ã„ã¯å•é¡Œè‡ªä½“ãŒç·šå½¢åˆ†é›¢ã§ããšå­¦ç¿’ä¸å¯èƒ½ãªã®ã‹ã‚’åˆ¤åˆ¥ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚","ãã®ãŸã‚ã€å­¦ç¿’æ™‚é–“ã«ä¸Šé™ãŒãªãã€å­¦ç¿’ãŒé•·æœŸåŒ–ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚","ã•ã‚‰ã«ã€å­¦ç¿’ãŒé€²ã¿ã™ãã‚‹ã¨ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éå‰°é©åˆï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼‰ã—ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ãƒªã‚¹ã‚¯ã‚‚ã‚ã‚Šã¾ã™ã€‚"],"originalSlideText":"Lernverfahren Delta Regel\\nKovergenz und Korrektheit\\nSatz: Wenn ein Perzeptron eine Klasseneinteilung lernen kann, dann lernt es diese mit der Delta Regel in endlich vielen Schritten\\nProblem: Falls das Perzeptron das Modell nicht erlernt, kann man nicht entscheiden:\\n- Ob genÃ¼gend Epochen gelernt wurde oder\\n- Das Problem nicht erlernbar ist\\n-> Es gibt keine obere Schranke fÃ¼r die Lerndauer\\n-> Overfitting Problematik","explanationImage":"","questionImage":""},{"id":25,"questionDe":"(s31) Wie lÃ¤sst sich das Lernverfahren der Delta Regel geometrisch interpretieren?","questionJa":"ãƒ‡ãƒ«ã‚¿å­¦ç¿’å‰‡ã®å­¦ç¿’æ‰‹é †ã¯å¹¾ä½•å­¦çš„ã«ã©ã®ã‚ˆã†ã«è§£é‡ˆã§ãã‚‹ã‹ï¼Ÿ","answerDe":["Die Gewichte werden als Vektor im Merkmalsraum betrachtet","Anfangssituation: Gewichtsvektor trennt die Klassen nicht korrekt","Bei jeder Korrektur wird der Gewichtsvektor so angepasst, dass er nÃ¤her an die korrekte Trennung heranrÃ¼ckt","Dies wird durch Addition eines korrigierenden Vektors (basierend auf dem Input) visualisiert","Nach mehreren Korrekturen nÃ¤hert sich der Gewichtsvektor der optimalen Trennung an"],"answerJa":["é‡ã¿ã¯ç‰¹å¾´ç©ºé–“å†…ã®ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦æ‰ãˆã‚‰ã‚Œã‚‹","åˆæœŸçŠ¶æ…‹ã§ã¯é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¯ã‚¯ãƒ©ã‚¹ã‚’æ­£ã—ãåˆ†é›¢ã—ã¦ã„ãªã„","ä¿®æ­£ã®ãŸã³ã«ã€é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¯æ­£ã—ã„åˆ†é›¢ã«è¿‘ã¥ãã‚ˆã†ã«èª¿æ•´ã•ã‚Œã‚‹","èª¿æ•´ã¯å…¥åŠ›ã«åŸºã¥ãè£œæ­£ãƒ™ã‚¯ãƒˆãƒ«ã®åŠ ç®—ã¨ã—ã¦è¡¨ç¾ã•ã‚Œã‚‹","è¤‡æ•°å›ã®ä¿®æ­£ã‚’çµŒã¦ã€é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¯æœ€é©ãªåˆ†é›¢ç·šã«è¿‘ã¥ã"],"explanationDe":["Das Lernverfahren der Delta Regel kann geometrisch interpretiert werden, indem man die Gewichte als Vektor im Merkmalsraum betrachtet.","Anfangs trennt der Gewichtsvektor die Datenpunkte mÃ¶glicherweise nicht korrekt, das heiÃŸt, die Trennlinie ist nicht optimal.","Jede Korrektur durch das Lernverfahren entspricht einer Verschiebung des Gewichtsvektors in Richtung eines Vektors, der auf dem aktuellen Eingabevektor basiert.","Diese Verschiebung bewegt die Trennlinie nÃ¤her an die korrekte Trennung der Klassen.","Nach mehreren solcher Anpassungen konvergiert der Gewichtsvektor zu einer Position, die eine gute Trennung zwischen den Klassen ermÃ¶glicht."],"explanationJa":["ãƒ‡ãƒ«ã‚¿å­¦ç¿’å‰‡ã¯ã€é‡ã¿ã‚’ç‰¹å¾´ç©ºé–“å†…ã®ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è€ƒãˆã‚‹ã“ã¨ã§å¹¾ä½•å­¦çš„ã«ç†è§£ã§ãã¾ã™ã€‚","æœ€åˆã¯é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãåˆ†é›¢ã—ã¦ã„ãªã„çŠ¶æ…‹ã§ã™ã€‚","å­¦ç¿’ã®ãŸã³ã«ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãè£œæ­£ãƒ™ã‚¯ãƒˆãƒ«ã‚’é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«åŠ ãˆã‚‹ã“ã¨ã§ã€åˆ†é›¢ç·šãŒæ­£ã—ã„æ–¹å‘ã¸å‹•ãã¾ã™ã€‚","ã“ã®æ“ä½œã‚’ç¹°ã‚Šè¿”ã™ã“ã¨ã§ã€é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¯ã‚¯ãƒ©ã‚¹ã‚’é©åˆ‡ã«åˆ†ã‘ã‚‹ä½ç½®ã«è¿‘ã¥ã„ã¦ã„ãã¾ã™ã€‚","ã¤ã¾ã‚Šã€ãƒ‡ãƒ«ã‚¿å‰‡ã®å­¦ç¿’ã¯é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ç©ºé–“å†…ã§ã®èª¿æ•´ã¨ç§»å‹•ã¨ã—ã¦æ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"],"originalSlideText":"Lernverfahren Delta Regel\\nGeometrische Interpretation nach Rojas","explanationImage":"lecture01/lecture09_ex12.png","questionImage":""},{"id":26,"questionDe":"(s25-30) Wie funktioniert das Lernverfahren der Delta Regel im Detail, welche Rolle spielt der Schwellwert und welche Probleme kÃ¶nnen dabei auftreten?","questionJa":"â˜…ï¼ˆè©¦é¨“é¡é¡Œï¼‰ãƒ‡ãƒ«ã‚¿å­¦ç¿’å‰‡ã®å¤§ã¾ã‹ãªæµã‚Œã¯ã©ã†ãªã£ã¦ã„ã‚‹ã‹ã€ã—ãã„å€¤ã®å½¹å‰²ã¯ä½•ã‹ã€ã¾ãŸã©ã‚“ãªå•é¡Œç‚¹ãŒã‚ã‚‹ã‹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚","answerDe":["Das Lernverfahren zeigt dem Netz Trainingsdaten mit bekannten korrekten Ausgaben (supervised learning).","Das Netz berechnet eine Ausgabe basierend auf aktuellen Gewichten und dem Schwellwert.","Die Differenz zwischen Soll- und Ist-Ausgabe wird berechnet (Fehler).","Gewichte und Schwellwert werden angepasst, um den Fehler zu minimieren, mithilfe der Ableitung der Aktivierungsfunktion und einem Lernfaktor.","Der Schwellwert wird als zusÃ¤tzliches Gewicht behandelt, multipliziert mit einem konstanten Eingang (z.B. 1).","Dieser Prozess wird wiederholt (Epochen) bis das Netz gut generalisiert oder ein Abbruchkriterium erreicht wird.","Probleme: Es ist oft unklar, ob genug trainiert wurde oder das Problem nicht lÃ¶sbar ist.","Lernzeit hat keine obere Grenze, und es besteht Risiko von Overfitting."],"answerJa":["ãƒ‡ãƒ«ã‚¿å­¦ç¿’å‰‡ã§ã¯ã€æ­£è§£ãƒ©ãƒ™ãƒ«ä»˜ãã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒˆã«ä¸ãˆã¾ã™ï¼ˆæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼‰ã€‚","ãƒãƒƒãƒˆã¯ç¾åœ¨ã®é‡ã¿ã¨ã—ãã„å€¤ã‚’ä½¿ã„å‡ºåŠ›ã‚’è¨ˆç®—ã—ã¾ã™ã€‚","å‡ºåŠ›ã®ç›®æ¨™å€¤ï¼ˆæ­£è§£ï¼‰ã¨ã®å·®ï¼ˆèª¤å·®ï¼‰ã‚’æ±‚ã‚ã¾ã™ã€‚","èª¤å·®ã‚’æ¸›ã‚‰ã™ãŸã‚ã«ã€æ´»æ€§åŒ–é–¢æ•°ã®å¾®åˆ†ã‚„å­¦ç¿’ç‡ã‚’ç”¨ã„ã¦é‡ã¿ã¨ã—ãã„å€¤ã‚’èª¿æ•´ã—ã¾ã™ã€‚","ã—ãã„å€¤ã¯å®šæ•°å…¥åŠ›ã¨çµã³ã¤ã„ãŸä¸€ã¤ã®é‡ã¿ã¨ã—ã¦æ‰±ã‚ã‚Œã¾ã™ã€‚","ã“ã®èª¿æ•´ã‚’ç¹°ã‚Šè¿”ã—ï¼ˆã‚¨ãƒãƒƒã‚¯ï¼‰ã€ååˆ†ã«æ€§èƒ½ãŒå‡ºã‚‹ã‹åœæ­¢æ¡ä»¶ã«é”ã™ã‚‹ã¾ã§ç¶šã‘ã¾ã™ã€‚","å•é¡Œç‚¹ã¨ã—ã¦ã€ååˆ†ã«å­¦ç¿’ã—ãŸã®ã‹ã€ã‚ã‚‹ã„ã¯å•é¡Œè‡ªä½“ãŒè§£æ±ºä¸èƒ½ãªã®ã‹é€”ä¸­ã§åˆ¤æ–­ãŒé›£ã—ã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚","å­¦ç¿’æ™‚é–“ã«ä¸Šé™ã¯ãªãã€éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ã‚‚ã‚ã‚Šã¾ã™ã€‚"],"explanationDe":["Das Lernverfahren der Delta Regel ist ein iterativer Prozess im Ã¼berwachten Lernen, bei dem das Netz wiederholt Eingabedaten mit bekannten Ausgaben erhÃ¤lt.","Zu Beginn berechnet das Netz eine Ausgabe basierend auf aktuellen Gewichten und dem Schwellwert, welcher als ein spezielles Gewicht mit einem konstanten Eingang betrachtet wird.","Die Differenz zwischen der berechneten Ausgabe und der korrekten Zielausgabe ergibt den Fehler.","Dieser Fehler wird genutzt, um Gewichte und Schwellwert schrittweise zu korrigieren. Die Anpassung erfolgt mithilfe der Ableitung der Aktivierungsfunktion, was ermÃ¶glicht, den Fehler effizient zu minimieren.","Der Lernprozess wird Ã¼ber mehrere DurchlÃ¤ufe (Epochen) wiederholt, wobei das Netz seine Parameter so verÃ¤ndert, dass die Vorhersagen immer genauer werden.","Ein praktisches Problem ist jedoch, dass es oft schwierig ist zu bestimmen, wann das Netz ausreichend trainiert ist oder ob das Problem Ã¼berhaupt lÃ¶sbar ist.","Es gibt keine festgelegte obere Grenze fÃ¼r die Trainingsdauer, was zu sehr langen Trainingszeiten fÃ¼hren kann.","Zudem besteht das Risiko des Overfittings, bei dem das Netz die Trainingsdaten zu genau lernt und dadurch an GeneralisierungsfÃ¤higkeit verliert."],"explanationJa":["ãƒ‡ãƒ«ã‚¿å­¦ç¿’å‰‡ã¯ã€æ•™å¸«ã‚ã‚Šå­¦ç¿’ã«ãŠã‘ã‚‹åå¾©çš„ãªãƒ—ãƒ­ã‚»ã‚¹ã§ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«æ­£è§£ãƒ©ãƒ™ãƒ«ä»˜ãã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ä½•åº¦ã‚‚æç¤ºã—ã¾ã™ã€‚","ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ç¾åœ¨ã®é‡ã¿ã¨ã—ãã„å€¤ï¼ˆã—ãã„å€¤ã¯å®šæ•°å…¥åŠ›ã«çµã³ä»˜ã‘ã‚‰ã‚ŒãŸç‰¹åˆ¥ãªé‡ã¿ã¨ã¿ãªã™ï¼‰ã‚’ä½¿ã£ã¦å‡ºåŠ›ã‚’è¨ˆç®—ã—ã¾ã™ã€‚","è¨ˆç®—ã•ã‚ŒãŸå‡ºåŠ›ã¨æ­£ã—ã„ç›®æ¨™å‡ºåŠ›ã¨ã®å·®ãŒèª¤å·®ã¨ãªã‚Šã¾ã™ã€‚","ã“ã®èª¤å·®ã‚’æ´»æ€§åŒ–é–¢æ•°ã®å¾®åˆ†ã‚’ç”¨ã„ã¦åŠ¹æœçš„ã«æ¸›ã‚‰ã™ãŸã‚ã«ã€é‡ã¿ã¨ã—ãã„å€¤ã‚’å°‘ã—ãšã¤ä¿®æ­£ã—ã¾ã™ã€‚","ã“ã®èª¿æ•´ä½œæ¥­ã‚’ä½•åº¦ã‚‚ç¹°ã‚Šè¿”ã—ï¼ˆã‚¨ãƒãƒƒã‚¯ï¼‰ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã‚ˆã‚Šæ­£ç¢ºãªäºˆæ¸¬ãŒã§ãã‚‹ã‚ˆã†ã«å­¦ç¿’ã—ã¦ã„ãã¾ã™ã€‚","ã—ã‹ã—ã€ã©ã®æ™‚ç‚¹ã§ååˆ†ã«å­¦ç¿’ã§ããŸã‹ã€ã‚ã‚‹ã„ã¯ãã‚‚ãã‚‚å•é¡ŒãŒè§£ã‘ã‚‹ã‚‚ã®ã‹é€”ä¸­ã§åˆ¤æ–­ã™ã‚‹ã®ã¯é›£ã—ã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚","å­¦ç¿’æ™‚é–“ã«ã¯ä¸Šé™ãŒãªãã€é•·æ™‚é–“ã®å­¦ç¿’ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚","ã•ã‚‰ã«ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éå‰°ã«é©åˆã—ã¦ã—ã¾ã†ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã®ãƒªã‚¹ã‚¯ã‚‚å­˜åœ¨ã—ã¾ã™ã€‚"],"originalSlideText":"Lernverfahren Delta Regel\\nâ€“ Beim Training werden die Daten dem Netz als Input gezeigt\\nâ€“ Output fÃ¼r die Beispiele bekannt (supervised)\\nâ€“ Vergleich von Soll und Ist Zustand im Output\\nâ€“ Bei Abweichung werden Schwellwert und Gewichte adaptiert\\nâ€“ Nur auf differenzierbare Aktivierungsfunktionen anwendbar\\nâ€“ Anpassung nach jedem Input oder nach jeder Epoche mÃ¶glich\\nâ€“ Epoche = Ein Durchlauf aller Trainingsdaten\\n\\nLernverfahren Delta Regel\\nDer Schwellwert ist damit auch nur ein Gewicht!\\n\\nSatz: Wenn ein Perzeptron eine Klasseneinteilung lernen kann, dann lernt es diese mit der Delta Regel in endlich vielen Schritten\\nProblem: Falls das Perzeptron das Modell nicht erlernt, kann man nicht entscheiden:\\n- Ob genÃ¼gend Epochen gelernt wurde oder\\n- Das Problem nicht erlernbar ist\\n-> Es gibt keine obere Schranke fÃ¼r die Lerndauer\\n-> Overfitting Problematik","explanationImage":"","questionImage":""},{"id":27,"questionDe":"(s32) Warum ist das XOR-Problem mit einfachen kÃ¼nstlichen Neuronen nicht lÃ¶sbar?","questionJa":"â˜…ãªãœXORå•é¡Œã¯å˜ç´”ãªäººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã§ã¯è§£ã‘ãªã„ã®ã‹ï¼Ÿ","answerDe":["XOR ist nicht linear trennbar","Die Ungleichungen fÃ¼r die Gewichte widersprechen sich","Kein Satz von Gewichten und Schwellwert kann alle Bedingungen gleichzeitig erfÃ¼llen","Deshalb kann ein einzelnes kÃ¼nstliches Neuron das XOR-Problem nicht lÃ¶sen"],"answerJa":["XORå•é¡Œã¯ç·šå½¢åˆ†é›¢ä¸å¯èƒ½ã§ã‚ã‚‹","é‡ã¿ã¨ã—ãã„å€¤ã«é–¢ã™ã‚‹ä¸ç­‰å¼ãŒçŸ›ç›¾ã—ã¦ã„ã‚‹","ã©ã‚“ãªé‡ã¿ã¨ã—ãã„å€¤ã®çµ„ã¿åˆã‚ã›ã‚‚å…¨ã¦ã®æ¡ä»¶ã‚’æº€ãŸã›ãªã„","ãã®ãŸã‚å˜ä¸€ã®äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã§ã¯XORå•é¡Œã‚’è§£ã‘ãªã„"],"explanationDe":["Das XOR-Problem zeigt eine Funktion, bei der die Ausgabe 1 ist, wenn genau eines der beiden EingÃ¤nge 1 ist, ansonsten 0.","Die Datenpunkte des XOR sind im zweidimensionalen Raum nicht durch eine einzige gerade Linie trennbar.","Die Bedingungen fÃ¼r die Gewichtung und den Schwellwert fÃ¼hren zu widersprÃ¼chlichen Ungleichungen, die keine LÃ¶sung zulassen.","Ein einfaches Perzeptron kann daher diese Funktion nicht modellieren, was zeigt, dass komplexere Netzwerke notwendig sind."],"explanationJa":["XORå•é¡Œã¯ã€2ã¤ã®å…¥åŠ›ã®ã†ã¡ã©ã¡ã‚‰ã‹ä¸€æ–¹ãŒ1ã®å ´åˆã«å‡ºåŠ›ãŒ1ã¨ãªã‚‹é–¢æ•°ã§ã™ã€‚","ã“ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã¯2æ¬¡å…ƒç©ºé–“ä¸Šã§ç›´ç·šä¸€ã¤ã§ã¯åˆ†é›¢ã§ãã¾ã›ã‚“ï¼ˆç·šå½¢åˆ†é›¢ä¸å¯èƒ½ï¼‰ã€‚","é‡ã¿ã‚„ã—ãã„å€¤ã®æ¡ä»¶ã¯çŸ›ç›¾ã—ãŸä¸ç­‰å¼ã¨ãªã‚Šã€åŒæ™‚ã«æº€ãŸã™ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚","ãã®ãŸã‚ã€å˜ç´”ãªãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã§ã¯XORå•é¡Œã‚’è§£æ±ºã§ããšã€ã‚ˆã‚Šè¤‡é›‘ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå¿…è¦ã¨ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"XOR Problem\\nTriviales Problem ist mit kÃ¼nstlichen Neuronen nicht lÃ¶sbar!\\nw_1 x_1 + w_2 x_2 â‰¥ Î¸\\nBedingungen fÃ¼hren zu Widerspruch","explanationImage":"","questionImage":""},{"id":28,"questionDe":"(s33) Was sind die Probleme und LÃ¶sungen fÃ¼r das XOR-Problem in kÃ¼nstlichen neuronalen Netzen?","questionJa":"â˜…äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã«ãŠã‘ã‚‹XORå•é¡Œã®èª²é¡Œã¨è§£æ±ºç­–ã¯ä½•ã‹ï¼Ÿ","answerDe":["1969 von Minsky und Papert verÃ¶ffentlicht, dass kÃ¼nstliche Neuronen das XOR-Problem nicht lÃ¶sen kÃ¶nnen","Folgerung war, dass kÃ¼nstliche Neuronen eine Sackgasse darstellen","Forschung zu neuronalen Netzen wurde daraufhin etwa 15 Jahre eingestellt","LÃ¶sungen sind mehrschichtige Perzeptrons (Feedforward-Netze)","Verwendung nichtlinearer Trennfunktionen"],"answerJa":["1969å¹´ã«ãƒŸãƒ³ã‚¹ã‚­ãƒ¼ã¨ãƒ‘ãƒ‘ãƒ¼ãƒˆã«ã‚ˆã£ã¦ã€å˜ç´”ãªäººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã§ã¯XORå•é¡Œã‚’è§£ã‘ãªã„ã“ã¨ãŒç¤ºã•ã‚ŒãŸ","ãã®çµæœã€äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯è¡Œãè©°ã¾ã‚Šã¨ã¿ãªã•ã‚ŒãŸ","ã“ã®ãŸã‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ç ”ç©¶ã¯ç´„15å¹´é–“åœæ»ã—ãŸ","â˜…è§£æ±ºç­–ã¨ã—ã¦ã€å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ãŒææ¡ˆã•ã‚ŒãŸ","â˜…éç·šå½¢ã®åˆ†é›¢é–¢æ•°ã‚’ç”¨ã„ã‚‹ã“ã¨ã§å•é¡Œã‚’è§£æ±º"],"explanationDe":["Das XOR-Problem zeigte die Grenzen einfacher kÃ¼nstlicher Neuronen auf, da es nicht linear trennbar ist.","Minsky und Papert verÃ¶ffentlichten 1969 diese Erkenntnis und fÃ¼hrten daraus die Schlussfolgerung, dass kÃ¼nstliche Neuronen fÃ¼r viele Probleme nicht geeignet seien.","Diese Sichtweise fÃ¼hrte dazu, dass die Forschung an neuronalen Netzen fÃ¼r etwa 15 Jahre zurÃ¼ckging.","Die LÃ¶sung wurde spÃ¤ter mit der EinfÃ¼hrung von mehrschichtigen Perzeptrons und nichtlinearen Aktivierungsfunktionen gefunden, die komplexere Muster erkennen kÃ¶nnen.","Dadurch konnten auch nicht linear trennbare Probleme wie XOR gelÃ¶st werden."],"explanationJa":["XORå•é¡Œã¯å˜ç´”ãªäººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®é™ç•Œã‚’ç¤ºã—ã¾ã—ãŸã€‚XORã¯ç·šå½¢ã«åˆ†é›¢ã§ããªã„ãŸã‚ã§ã™ã€‚","1969å¹´ã€ãƒŸãƒ³ã‚¹ã‚­ãƒ¼ã¨ãƒ‘ãƒ‘ãƒ¼ãƒˆã¯ã“ã®äº‹å®Ÿã‚’å…¬è¡¨ã—ã€å¤šãã®å•é¡Œã«å¯¾ã—ã¦å˜ç´”ãªäººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯é©ã•ãªã„ã¨çµè«–ã¥ã‘ã¾ã—ãŸã€‚","ã“ã®è¦‹è§£ã«ã‚ˆã‚Šã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ç ”ç©¶ã¯ç´„15å¹´é–“åœæ»ã—ã¾ã—ãŸã€‚","ãã®å¾Œã€å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã‚„éç·šå½¢æ´»æ€§åŒ–é–¢æ•°ã®å°å…¥ã«ã‚ˆã‚Šã€ã‚ˆã‚Šè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®èªè­˜ãŒå¯èƒ½ã¨ãªã‚Šã¾ã—ãŸã€‚","ã“ã‚Œã«ã‚ˆã£ã¦ã€XORã®ã‚ˆã†ãªç·šå½¢åˆ†é›¢ä¸å¯èƒ½ãªå•é¡Œã‚‚è§£æ±ºã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚"],"originalSlideText":"XOR Problem\\n1969 publiziert von Minsky und Papert\\nFolgerung: kÃ¼nstliche neuronen sind eine Sackgasse\\nForschung wurde daraufhin ca 15 Jahre eingestellt\\nLÃ¶sungen sind:\\nMehrschichtige Perzeptrons (Feedforward Netze)\\nNichtlineare Trennfunktionen","explanationImage":"lecture01/lecture09_ex13.png","questionImage":""},{"id":29,"questionDe":"(s34) Was sind die Vorteile kÃ¼nstlicher neuronaler Netze?","questionJa":"â˜…äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®åˆ©ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ","answerDe":["Sehr gute Mustererkennung","Verarbeitung von verrauschten, unvollstÃ¤ndigen und widersprÃ¼chlichen Eigenschaften","MÃ¶glicher multimodaler Input (Zahlen, Farben, TÃ¶ne, Sprache etc.)","Erstellt ein Modell ohne Hypothesen des Nutzers","Fehlertolerant","Einfach im Produktivbetrieb zu nutzen"],"answerJa":["â˜…éå¸¸ã«é«˜ã„ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜èƒ½åŠ›","â˜…ãƒã‚¤ã‚ºãŒã‚ã‚‹ã€ä¸å®Œå…¨ãªã€çŸ›ç›¾ã™ã‚‹ç‰¹å¾´ã®å‡¦ç†ãŒå¯èƒ½","â˜…å¤šæ§˜ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®å…¥åŠ›ãŒå¯èƒ½ï¼ˆæ•°å€¤ã€è‰²ã€éŸ³å£°ã€è¨€èªãªã©ï¼‰","â˜…ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»®èª¬ãªã—ã«ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã‚‹","â˜…èª¤å·®ã«å¼·ã„ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ãƒˆï¼‰","å®Ÿé‹ç”¨ç’°å¢ƒã§ä½¿ã„ã‚„ã™ã„"],"explanationDe":["KÃ¼nstliche neuronale Netze zeichnen sich durch ihre FÃ¤higkeit aus, komplexe Muster in Daten zu erkennen, selbst wenn die Daten verrauscht oder unvollstÃ¤ndig sind.","Sie kÃ¶nnen widersprÃ¼chliche Informationen verarbeiten und sind nicht auf eine spezielle Art von Daten beschrÃ¤nkt, sondern kÃ¶nnen verschiedene Datentypen wie Zahlen, Farben, TÃ¶ne und Sprache verarbeiten.","Ein weiterer Vorteil ist, dass sie Modelle ohne vorgegebene Hypothesen des Nutzers erstellen, was die FlexibilitÃ¤t erhÃ¶ht.","Durch ihre Fehlertoleranz kÃ¶nnen sie auch bei Fehlern in den Daten robuste Ergebnisse liefern.","Zudem sind sie oft einfach in produktiven Systemen einzusetzen und kÃ¶nnen dort effektiv genutzt werden."],"explanationJa":["äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ã€ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã™ã‚‹èƒ½åŠ›ã«å„ªã‚Œã¦ãŠã‚Šã€ãƒã‚¤ã‚ºã‚„æ¬ æãŒã‚ã£ã¦ã‚‚å¯¾å¿œå¯èƒ½ã§ã™ã€‚","çŸ›ç›¾ã—ãŸæƒ…å ±ã‚‚å‡¦ç†ã§ãã€ç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«é™å®šã•ã‚Œãšã€æ•°å€¤ã€è‰²ã€éŸ³å£°ã€è¨€èªãªã©å¤šæ§˜ãªå…¥åŠ›ã«å¯¾å¿œã§ãã¾ã™ã€‚","ã¾ãŸã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒäº‹å‰ã«ä»®èª¬ã‚’ç«‹ã¦ãªãã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•çš„ã«æ§‹ç¯‰ã§ãã‚‹ãŸã‚ã€æŸ”è»Ÿæ€§ãŒã‚ã‚Šã¾ã™ã€‚","èª¤å·®ã«å¼·ãã€ãƒ‡ãƒ¼ã‚¿ã«èª¤ã‚ŠãŒã‚ã£ã¦ã‚‚å®‰å®šã—ãŸçµæœã‚’å‡ºã›ã¾ã™ã€‚","ã•ã‚‰ã«ã€å®Ÿéš›ã®é‹ç”¨ç’°å¢ƒã§åˆ©ç”¨ã—ã‚„ã™ãã€å¤šãã®åˆ†é‡ã§åŠ¹æœçš„ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚"],"originalSlideText":"Vorteile kÃ¼nstlicher neuronaler Netze\\nâ€“ Sehr gute Mustererkennung\\nâ€“ Verarbeitung von Inputs mit\\n  â€“ Verrauschten Eigenschaften\\n  â€“ UnvollstÃ¤ndigen Eigenschaften\\n  â€“ WidersprÃ¼chlichen Eigenschaften\\nâ€“ MÃ¶glicher multimodaler Input (Zahlen, Farben, TÃ¶ne, Sprache, etc)\\nâ€“ Erstellt ein Modell ohne Hypothesen des Nutzers\\nâ€“ Fehlertolerant\\nâ€“ Im Produktivbetrieb einfach zu nutzen","explanationImage":"","questionImage":""},{"id":30,"questionDe":"(s34) Nenne mindestens sechs Vorteile kÃ¼nstlicher neuronaler Netze und erlÃ¤utere diese konkret.","questionJa":"â˜…äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®åˆ©ç‚¹ã‚’å°‘ãªãã¨ã‚‚6ã¤æŒ™ã’ã€ãã‚Œãã‚Œå…·ä½“çš„ã«èª¬æ˜ã›ã‚ˆã€‚","answerDe":["Sehr gute Mustererkennung","Verarbeitung von verrauschten Eigenschaften","Verarbeitung von unvollstÃ¤ndigen Eigenschaften","Verarbeitung von widersprÃ¼chlichen Eigenschaften","MÃ¶glicher multimodaler Input (Zahlen, Farben, TÃ¶ne, Sprache etc.)","Erstellt ein Modell ohne Hypothesen des Nutzers","Fehlertolerant","Einfach im Produktivbetrieb zu nutzen"],"answerJa":["â˜…éå¸¸ã«é«˜ã„ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜èƒ½åŠ›","â˜…ãƒã‚¤ã‚ºã®ã‚ã‚‹ç‰¹å¾´ã®å‡¦ç†ãŒå¯èƒ½","â˜…æ¬ æã—ã¦ã„ã‚‹ç‰¹å¾´ã®å‡¦ç†ãŒå¯èƒ½","â˜…çŸ›ç›¾ã™ã‚‹ç‰¹å¾´ã®å‡¦ç†ãŒå¯èƒ½","â˜…å¤šæ§˜ãªå…¥åŠ›å½¢å¼ã«å¯¾å¿œå¯èƒ½ï¼ˆæ•°å€¤ã€è‰²ã€éŸ³ã€è¨€èªãªã©ï¼‰","â˜…ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒäº‹å‰ã«ä»®èª¬ã‚’ç«‹ã¦ãªãã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•æ§‹ç¯‰å¯èƒ½","â˜…èª¤å·®ã‚„ä¸€éƒ¨ã®æ•…éšœã«å¼·ã„ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ãƒˆï¼‰","å®Ÿéš›ã®é‹ç”¨ç’°å¢ƒã§ã®å°å…¥ãƒ»åˆ©ç”¨ãŒå®¹æ˜“"],"explanationDe":["KÃ¼nstliche neuronale Netze sind in der Lage, sehr komplexe Muster in Daten zu erkennen, was sie besonders nÃ¼tzlich fÃ¼r Anwendungen wie Bilderkennung oder Sprachverarbeitung macht.","Sie kÃ¶nnen auch mit verrauschten Daten umgehen, also wenn Messungen oder Eingaben fehlerhaft oder unscharf sind, ohne dass die Leistung stark leidet.","UnvollstÃ¤ndige Daten, bei denen manche Merkmale fehlen, kÃ¶nnen ebenfalls verarbeitet werden, was in realen Situationen hÃ¤ufig vorkommt.","Auch wenn Eingabedaten widersprÃ¼chliche oder uneinheitliche Merkmale enthalten, kÃ¶nnen neuronale Netze diese Informationen oft noch sinnvoll verarbeiten.","Sie sind flexibel gegenÃ¼ber verschiedenen Arten von Eingabedaten, beispielsweise Zahlenwerte, Farbwerte, akustische Signale oder natÃ¼rliche Sprache.","Ein weiterer Vorteil ist, dass sie ohne explizite Vorannahmen oder Hypothesen des Anwenders lernen, das heiÃŸt sie mÃ¼ssen nicht vorher wissen, welche Merkmale wichtig sind.","Neuronale Netze sind fehlertolerant, das heiÃŸt sie kÃ¶nnen auch bei Fehlern oder AusfÃ¤llen einzelner Neuronen weiterhin gute Ergebnisse liefern.","SchlieÃŸlich sind sie in der Praxis oft leicht einzusetzen, da viele Frameworks und Werkzeuge verfÃ¼gbar sind, die eine Integration in produktive Systeme ermÃ¶glichen."],"explanationJa":["äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯éå¸¸ã«è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’èªè­˜ã§ãã‚‹ãŸã‚ã€ç”»åƒèªè­˜ã‚„éŸ³å£°èªè­˜ãªã©ã®åˆ†é‡ã§å¼·ã¿ã‚’ç™ºæ®ã—ã¾ã™ã€‚","ãƒã‚¤ã‚ºãŒæ··å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã§ã‚‚æ€§èƒ½ã‚’å¤§ããè½ã¨ã•ãšå‡¦ç†ã§ãã‚‹ãŸã‚ã€ç¾å®Ÿã®æ¸¬å®šèª¤å·®ã‚„ä¸å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã«å¼·ã„ã§ã™ã€‚","ç‰¹å¾´ã®ä¸€éƒ¨ãŒæ¬ ã‘ã¦ã„ã‚‹ä¸å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚‚æ‰±ãˆã‚‹ãŸã‚ã€å¤šæ§˜ãªç¾å®Ÿç’°å¢ƒã«å¯¾å¿œå¯èƒ½ã§ã™ã€‚","å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«çŸ›ç›¾ãŒã‚ã£ã¦ã‚‚ã€ãã‚Œã‚’å«ã‚ã¦æ„å‘³ã®ã‚ã‚‹å‡¦ç†ã‚’è¡Œãˆã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚","æ•°å€¤ã€è‰²ã€éŸ³å£°ã€è¨€èªãªã©ã€æ§˜ã€…ãªå½¢å¼ã®å…¥åŠ›ã‚’æ‰±ãˆã‚‹æŸ”è»Ÿæ€§ãŒã‚ã‚Šã¾ã™ã€‚","ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒäº‹å‰ã«ã©ã®ç‰¹å¾´ãŒé‡è¦ã‹ã‚’ä»®å®šã—ãªãã¦ã‚‚ã€è‡ªå‹•ã§é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã§ãã‚‹ç‚¹ã‚‚å¤§ããªåˆ©ç‚¹ã§ã™ã€‚","æ•…éšœã‚„èª¤å·®ã«å¼·ã„è¨­è¨ˆã®ãŸã‚ã€ä¸€éƒ¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«å•é¡ŒãŒã‚ã£ã¦ã‚‚å®‰å®šã—ãŸçµæœãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚","å¤šãã®é–‹ç™ºãƒ„ãƒ¼ãƒ«ã‚„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæƒã£ã¦ãŠã‚Šã€å®Ÿé‹ç”¨ç’°å¢ƒã¸ã®å°å…¥ã‚„é‹ç”¨ãŒå®¹æ˜“ã§ã™ã€‚"],"originalSlideText":"Vorteile kÃ¼nstlicher neuronaler Netze\\nâ€“ Sehr gute Mustererkennung\\nâ€“ Verarbeitung von Inputs mit\\n  â€“ Verrauschten Eigenschaften\\n  â€“ UnvollstÃ¤ndigen Eigenschaften\\n  â€“ WidersprÃ¼chlichen Eigenschaften\\nâ€“ MÃ¶glicher multimodaler Input (Zahlen, Farben, TÃ¶ne, Sprache, etc)\\nâ€“ Erstellt ein Modell ohne Hypothesen des Nutzers\\nâ€“ Fehlertolerant\\nâ€“ Im Produktivbetrieb einfach zu nutzen","explanationImage":"","questionImage":""},{"id":31,"questionDe":"(s35) Nenne mindestens sechs Nachteile kÃ¼nstlicher neuronaler Netze und erlÃ¤utere diese genauer.","questionJa":"â˜…äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®æ¬ ç‚¹ã‚’å°‘ãªãã¨ã‚‚6ã¤æŒ™ã’ã€ãã‚Œãã‚Œè©³ã—ãèª¬æ˜ã›ã‚ˆã€‚","answerDe":["Lange Trainingszeiten, die sich Ã¼ber Monate erstrecken kÃ¶nnen","Lernerfolg ist nicht garantiert","Generalisierbarkeit auf neue Daten ist nicht garantiert","GroÃŸer Bedarf an vielen Trainingsdaten","Komplexes Blackbox-Verfahren, das schwer zu interpretieren ist","Evaluierung des Netzes ist schwierig","Anzahl der Knoten und Kanten kann schnell sehr groÃŸ werden","LÃ¶st nur das Problem mit einem Modell, gibt aber keine Hinweise auf wichtige Merkmale"],"answerJa":["â˜…å­¦ç¿’ã«éå¸¸ã«é•·ã„æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã€æ•°ã‹æœˆã«åŠã¶ã“ã¨ã‚‚ã‚ã‚‹","å¿…ãšã—ã‚‚å­¦ç¿’ãŒæˆåŠŸã™ã‚‹ã¨ã¯é™ã‚‰ãªã„","â˜…æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã†ã¾ãä¸€èˆ¬åŒ–ã§ãã‚‹ä¿è¨¼ãŒãªã„","â˜…å¤§é‡ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã¨ãªã‚‹","â˜…è¤‡é›‘ã§ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹çš„ãªæ‰‹æ³•ã®ãŸã‚ã€å†…éƒ¨ã®å‹•ä½œã‚’ç†è§£ã—ã¥ã‚‰ã„","â˜…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è©•ä¾¡ãŒé›£ã—ã„","ãƒãƒ¼ãƒ‰ã‚„ã‚¨ãƒƒã‚¸ã®æ•°ãŒæ€¥é€Ÿã«å¢—å¤§ã™ã‚‹ã“ã¨ãŒã‚ã‚‹","â˜…å•é¡Œã‚’ä¸€ã¤ã®ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦è§£ããŒã€é‡è¦ãªç‰¹å¾´ãŒä½•ã‹ã®æ‰‹ãŒã‹ã‚Šã‚’ä¸ãˆãªã„"],"explanationDe":["Neuronale Netze kÃ¶nnen sehr lange Trainingszeiten benÃ¶tigen, besonders bei groÃŸen Datenmengen und komplexen Modellen.","Ein Lernerfolg ist nicht garantiert, da das Training in lokalen Minima stecken bleiben kann.","Die FÃ¤higkeit, auf neuen, unbekannten Daten gut zu generalisieren, ist oft nicht sichergestellt.","Zur effektiven Modellierung sind oft groÃŸe Mengen an Trainingsdaten erforderlich.","Die internen Prozesse sind oft undurchsichtig, was die Interpretation der Ergebnisse erschwert.","Die Bewertung und Validierung der Modelle ist kompliziert und erfordert viel Aufwand.","Mit zunehmender KomplexitÃ¤t wachsen die Anzahl der Knoten (Neuronen) und Kanten (Verbindungen) schnell an, was die Berechnung teuer macht.","Das Modell lÃ¶st zwar das Problem, liefert aber keine Einsichten darÃ¼ber, welche Merkmale besonders wichtig sind."],"explanationJa":["ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ç‰¹ã«å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚„è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€å­¦ç¿’ã«éå¸¸ã«é•·ã„æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚","å­¦ç¿’ãŒå¿…ãšæˆåŠŸã™ã‚‹ã¨ã¯é™ã‚‰ãšã€å±€æ‰€æœ€é©è§£ã«é™¥ã‚‹ãƒªã‚¹ã‚¯ã‚‚ã‚ã‚Šã¾ã™ã€‚","æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã†ã¾ãä¸€èˆ¬åŒ–ã§ãã‚‹ä¿è¨¼ã¯ãªãã€éå­¦ç¿’ã®æ‡¸å¿µã‚‚ã‚ã‚Šã¾ã™ã€‚","åŠ¹æœçš„ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹ãŸã‚ã«ã¯å¤§é‡ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã«ãªã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚","å†…éƒ¨ã®å‹•ä½œãŒãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§åˆ†ã‹ã‚Šã¥ã‚‰ãã€çµæœã®è§£é‡ˆãŒé›£ã—ã„ã§ã™ã€‚","ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚„æ¤œè¨¼ã«ã¯å¤§ããªåŠ´åŠ›ãŒå¿…è¦ã§ã™ã€‚","ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ãŒå¢—ã™ã¨ã€ãƒãƒ¼ãƒ‰ã‚„æ¥ç¶šã®æ•°ãŒæ€¥æ¿€ã«å¢—ãˆã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ããªã‚Šã¾ã™ã€‚","å•é¡Œã¯è§£æ±ºã§ãã¦ã‚‚ã€ã©ã®ç‰¹å¾´é‡ãŒé‡è¦ã‹ã®æƒ…å ±ã¯æä¾›ã•ã‚Œã¾ã›ã‚“ã€‚"],"originalSlideText":"Nachteile kÃ¼nstlicher neuronaler Netze\\nâ€“ Lange Trainingszeiten (bis zu Monaten)\\nâ€“ Lernerfolg ist nicht garantiert\\nâ€“ Generalisierbarkeit ist nicht garantiert\\nâ€“ Viele Daten sind notwendig\\nâ€“ Komplexes Blackbox Verfahren\\nâ€“ Evaluierung des Netzes ist schwierig\\nâ€“ Anzahl der Knoten und Kanten kann schnell sehr groÃŸ werden\\nâ€“ LÃ¶st â€nurâ€œ das Problem mit einem Modell, liefert aber keine Hinweise auf die wichtiges Features","explanationImage":"","questionImage":""}]');const o={class:"container py-4"},h={class:"text-center mb-5"},g={class:"display-5 fw-bold text-primary"},c={class:"fs-5 text-muted"},w={class:"text-dark"};var m={__name:"Lecture08Page",setup(e){const n=(0,s.lq)(),i=(0,a.KR)(""),m=(0,a.KR)(""),b=(0,a.KR)(""),k=(0,a.KR)([]);return(0,t.sV)(()=>{const e="lecture01",t=parseInt(n.name.split("_")[1]),r=u[e];i.value=r.title,b.value=t.toString().padStart(2,"0");const a=r.lectures.find(e=>e.number===t);m.value=a?a.title:"",k.value=d}),(e,n)=>((0,t.uX)(),(0,t.CE)("div",o,[(0,t.Lk)("div",h,[(0,t.Lk)("h1",g,(0,r.v_)(i.value),1),(0,t.Lk)("p",c,[(0,t.eW)(" Lecture "+(0,r.v_)(b.value)+": ",1),(0,t.Lk)("span",w,(0,r.v_)(m.value),1)]),n[0]||(n[0]=(0,t.Lk)("hr",{class:"w-25 mx-auto border-primary"},null,-1))]),((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(k.value,e=>((0,t.uX)(),(0,t.Wv)(l.A,{key:e.id,question:e},null,8,["question"]))),128))]))}};const b=m;var k=b}}]);
//# sourceMappingURL=1192.66b56bdb.js.map