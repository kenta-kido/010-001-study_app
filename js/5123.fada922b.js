"use strict";(self["webpackChunkstudy_site"]=self["webpackChunkstudy_site"]||[]).push([[5123],{495:function(e,n,i){i.d(n,{A:function(){return I}});var t=i(6768),r=i(4232),s=i(144);const a={class:"card mb-4 shadow-sm"},u={class:"card-body"},l={class:"card-title"},d={class:"text-muted fst-italic"},m={key:0},g=["src"],h={key:1,class:"mt-3"},o={class:"alert alert-success"},c={key:0},k={key:1},C={class:"alert alert-info mt-2"},b={key:0},z={key:1},D={class:"mt-3"},w={key:0},p={key:1},v={key:2},f={key:3},x={key:4},_=["src"],j={class:"mt-4"},A={class:"border rounded p-3 bg-white text-secondary",style:{"white-space":"pre-wrap","font-family":"inherit"}};var B={__name:"QuestionItem",props:{question:Object},setup(e){const n=(0,s.KR)(!1);return(i,s)=>((0,t.uX)(),(0,t.CE)("div",a,[(0,t.Lk)("div",u,[(0,t.Lk)("h5",l,"Q"+(0,r.v_)(e.question.id)+": "+(0,r.v_)(e.question.questionJa),1),(0,t.Lk)("p",d,"("+(0,r.v_)(e.question.questionDe)+")",1),e.question.questionImage?((0,t.uX)(),(0,t.CE)("div",m,[(0,t.Lk)("img",{src:`./images/${e.question.questionImage}`,class:"img-fluid rounded my-2 border",alt:"question image"},null,8,g)])):(0,t.Q3)("",!0),(0,t.Lk)("button",{class:"btn btn-outline-primary mt-2",onClick:s[0]||(s[0]=e=>n.value=!n.value)},(0,r.v_)(n.value?"Hide Answer":"Check Answer"),1),n.value?((0,t.uX)(),(0,t.CE)("div",h,[(0,t.Lk)("div",o,[s[1]||(s[1]=(0,t.Lk)("strong",null,"Antwort (De):",-1)),s[2]||(s[2]=(0,t.Lk)("br",null,null,-1)),Array.isArray(e.question.answerDe)?((0,t.uX)(),(0,t.CE)("ul",c,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.answerDe,(e,n)=>((0,t.uX)(),(0,t.CE)("li",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",k,(0,r.v_)(e.question.answerDe),1))]),(0,t.Lk)("div",C,[s[3]||(s[3]=(0,t.Lk)("strong",null,"Übersetzung (Ja):",-1)),s[4]||(s[4]=(0,t.Lk)("br",null,null,-1)),Array.isArray(e.question.answerJa)?((0,t.uX)(),(0,t.CE)("ul",b,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.answerJa,(e,n)=>((0,t.uX)(),(0,t.CE)("li",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",z,(0,r.v_)(e.question.answerJa),1))]),(0,t.Lk)("div",D,[s[6]||(s[6]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"Erklärung (De):",-1)),Array.isArray(e.question.explanationDe)?((0,t.uX)(),(0,t.CE)("div",w,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.explanationDe,(e,n)=>((0,t.uX)(),(0,t.CE)("p",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",p,(0,r.v_)(e.question.explanationDe),1)),s[7]||(s[7]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"解説 (Ja):",-1)),Array.isArray(e.question.explanationJa)?((0,t.uX)(),(0,t.CE)("div",v,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.explanationJa,(e,n)=>((0,t.uX)(),(0,t.CE)("p",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",f,(0,r.v_)(e.question.explanationJa),1)),e.question.explanationImage?((0,t.uX)(),(0,t.CE)("div",x,[(0,t.Lk)("img",{src:`./images/${e.question.explanationImage}`,class:"img-fluid rounded my-2 border",alt:"explanation image"},null,8,_)])):(0,t.Q3)("",!0),(0,t.Lk)("div",j,[s[5]||(s[5]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"原文（スライド抜粋）:",-1)),(0,t.Lk)("div",A,(0,r.v_)(e.question.originalSlideText),1)])])])):(0,t.Q3)("",!0)])]))}};const S=B;var I=S},5123:function(e,n,i){i.r(n),i.d(n,{default:function(){return b}});i(8111),i(116);var t=i(6768),r=i(4232),s=i(144),a=i(1387),u=i(495),l=i(3529),d=JSON.parse('[{"id":1,"questionDe":"(s.25) Was versteht man unter Clustering und welche Bedingungen gelten dafür?","questionJa":"クラスタリングとは何か？また、どのような条件が課されるか？","answerDe":["Gegeben: eine Menge von Objekten D = {d₁, ..., dₙ}","Gesucht: Klassen K = {C₁, ..., Cₖ}, wobei k ≪ n","Bedingungen:","⋃ᵢ Cᵢ = D (Vereinigung ergibt die ganze Menge)","Cᵢ ∩ Cⱼ = ∅ für i ≠ j (Disjunktheit – keine Überlappung)"],"answerJa":["与えられるもの：オブジェクトの集合 D = {d₁, ..., dₙ}","求めるもの：クラス集合 K = {C₁, ..., Cₖ}（k は n よりずっと小さい）","条件：","すべてのクラスの和集合が元の集合 D を構成すること（∪Cᵢ = D）","異なるクラス同士は重なりがないこと（Cᵢ ∩ Cⱼ = ∅, i ≠ j）"],"explanationDe":["Clustering bedeutet, dass man eine große Menge von Objekten (z. B. Datenpunkten) in kleinere Gruppen (Cluster) unterteilt.","Diese Gruppen sollen so beschaffen sein, dass jedes Objekt genau einem Cluster zugeordnet ist (keine Überlappung).","Das Ziel ist es, die gesamte Menge D durch disjunkte Teilmengen C₁ bis Cₖ zu partitionieren.","Beispiel: Bei einer Kundendatenbank mit 1000 Kunden möchte man 5 Kundengruppen finden – jede Gruppe enthält ähnliche Kunden, und jede Person ist nur in einer Gruppe."],"explanationJa":["クラスタリングとは、多数のデータ（オブジェクト）を意味のあるグループ（クラスタ）に分けることです。","このとき、各データ点は1つのクラスタにだけ属するようにし、クラスタ同士は重なりません（排他的）。","全体のデータ集合 D は、クラスタ C₁〜Cₖ の和集合になります。","例：1000人の顧客データがあるとき、それを購買傾向などによって5つのグループに分けたい。この場合、各顧客はどれか1つのグループに所属し、全員がどこかに分類されます。"],"originalSlideText":"- Clustering\\n  – Gegeben: Sei D = {d₁, ..., dₙ} eine Menge von Objekten.\\n  – Gesucht: Klassen K = {C₁, ..., Cₖ}, k ≪ n\\n  – Bedingungen:\\n    – D = ⋃ Cᵢ\\n    – ∀i ≠ j: Cᵢ ∩ Cⱼ = ∅ (Partitionierung)","explanationImage":"lecture01/lecture05_ex01.png"},{"id":2,"questionDe":"(s.25) Nennen Sie vier Motivationen für Clustering.","questionJa":"★クラスタリングを行う動機・目的を4つ挙げよ。","answerDe":["Suche nach Gemeinsamkeiten und Unterschieden","Suche nach zusätzlicher Struktur in den Daten","Ersetzen von Datenmengen durch Repräsentanten","Analyse der Cluster anstelle einzelner Datenpunkte"],"answerJa":["★共通点や相違点の発見","★データ内の追加的な構造の探索","★複数のデータ点を代表点で置き換える","★個別のデータ点ではなくクラスタ単位で分析する"],"explanationDe":["Clustering hilft, Gruppen von ähnlichen Objekten zu identifizieren – das erleichtert die Analyse großer Datenmengen.","Durch Clustering entdeckt man oft verborgene Strukturen oder Muster, die in Rohdaten nicht sichtbar sind.","Statt alle Datenpunkte zu verarbeiten, kann man jeden Cluster durch einen typischen Vertreter (z. B. Schwerpunkt) beschreiben.","Man fokussiert sich auf die Analyse von Gruppenverhalten statt auf individuelle Datenpunkte – z. B. Marketing-Cluster anstelle einzelner Kunden."],"explanationJa":["クラスタリングは、似た性質のデータをグループ化することで、大量の情報を整理しやすくします。","データに隠れているパターンや構造を見つけ出す手がかりにもなります。","各クラスタを代表する値（例えば重心）で表すことで、計算量や表現を簡素化できます。","個々のデータを扱う代わりに、クラスタごとに傾向や特徴を分析することが可能になります（例：顧客全体ではなく『若年層』『高齢層』などのグループで解析）。"],"originalSlideText":"- Motivation:\\n  – Suche nach Gemeinsamkeiten und Unterschieden\\n  – Suche nach zusätzlicher Struktur in den Daten\\n  – Ersetzen von Mengen von Datenpunkten (die Cluster) durch ihre Repräsentanten\\n  – Analyse der Cluster anstelle der Datenpunkte"},{"id":2,"questionDe":"(s.29–30) Nennen Sie vier verschiedene Arten von Clustering-Methoden.","questionJa":"クラスタリングの代表的な手法を4つ挙げよ。","answerDe":["1. Divisive Clustering","2. Hierarchisches Clustering","3. Überlappendes Clustering","4. Probabilistisches Clustering"],"answerJa":["1. 分割型クラスタリング（Divisive Clustering）","2. 階層的クラスタリング（Hierarchical Clustering）","3. 重なり合うクラスタリング（Overlapping Clustering）","4. 確率的クラスタリング（Probabilistic Clustering）"],"explanationDe":["Es gibt verschiedene Clustering-Ansätze, je nach Zielsetzung und Datenstruktur.","Diese vier Methoden unterscheiden sich z. B. darin, ob sie scharfe Grenzen ziehen, Hierarchien aufbauen oder Wahrscheinlichkeiten verwenden.","In den nächsten Aufgaben werden sie jeweils im Detail erklärt."],"explanationJa":["クラスタリングにはさまざまな手法がありますが、主に4つの代表的なアプローチがあります。","それぞれ、データの扱いやクラスタの構成方法が異なります（例：明確な境界を引くか、階層構造をとるか、確率で分類するかなど）。","次の問題でこれらを一つずつ詳しく見ていきます。"],"originalSlideText":"- Divisive Clustering\\n- Hierarchisches Clustering\\n- Überlappendes Clustering\\n- Probabilistisches Clustering"},{"id":3,"questionDe":"(s.29) Erklären Sie das Divisive Clustering.","questionJa":"分割型クラスタリング（Divisive Clustering）とは何かを説明せよ。","answerDe":["Unterteilt den Raum in Teilbereiche","Zieht klare Grenzen zwischen den Clustern","Beispiel: k-means Clustering"],"answerJa":["空間を複数の領域に分割する","クラスタ間に明確な境界線を引く","例：k-meansクラスタリング"],"explanationDe":["Beim divisiven Clustering wird der Raum direkt in mehrere Cluster unterteilt.","Das Verfahren beginnt oft mit allen Punkten in einem Cluster und trennt diesen rekursiv weiter.","Typisches Beispiel ist k-means, bei dem die Datenpunkte auf k Cluster aufgeteilt werden, sodass die Punkte innerhalb eines Clusters möglichst ähnlich sind.","Im Bild auf der Folie sieht man, wie durch das Ziehen von Trennlinien (Grenzen) die Punkte d, e, j usw. zu einem Cluster gruppiert werden."],"explanationJa":["分割型クラスタリングは、データ空間を直接いくつかの領域に分けてクラスタを作る方法です。","すべてのデータをまず1つにまとめ、そこから繰り返し分割していく手法なども含まれます。","代表的な例がk-meansクラスタリングで、これはあらかじめ指定したk個のクラスタにデータを分ける方法です。","スライドの図では、直線を引いてデータを分割することで、それぞれのクラスタ（d,e,jなど）に分類している様子が示されています。"],"originalSlideText":"- Divisive Clustering\\n- Unterteilung des Raumes\\n- Zeichne Grenzen zwischen Clustern\\n- Beispiel: k-means clustering"},{"id":4,"questionDe":"(s.29) Was ist hierarchisches Clustering?","questionJa":"階層的クラスタリング（Hierarchisches Clustering）とは何かを説明せよ。","answerDe":["Erzeugt eine Baumstruktur","Verwendet Dendrogramme zur Visualisierung","Beispiele: Agglomerativ oder Divisiv"],"answerJa":["木構造（ツリー構造）を生成する","デンドログラム（樹形図）で表現される","例：凝集型（Agglomerative）、分割型（Divisive）"],"explanationDe":["Beim hierarchischen Clustering wird eine Struktur in Form eines Baums (Dendrogramm) aufgebaut.","Dabei können zwei Vorgehensweisen unterschieden werden: agglomerativ (von unten nach oben) oder divisiv (von oben nach unten).","In der Grafik sieht man ein Beispiel für ein Dendrogramm, das zeigt, wie Datenpunkte schrittweise zusammengefasst werden (z. B. g und a, dann c dazugenommen, usw.)."],"explanationJa":["階層的クラスタリングでは、クラスタを木構造（ツリー構造）として表現します。","データポイントを一つずつまとめていく「凝集型（Agglomerative）」や、大きなクラスタを分割していく「分割型（Divisive）」の2つのアプローチがあります。","スライドのデンドログラムでは、例えばgとaが最初に一緒にされ、それにcが追加されるなど、データが段階的にまとめられる様子が示されています。"],"originalSlideText":"- Hierarchisches Clustering\\n- Erzeugt einen Baum\\n- Verwende Dendrogramme zur Darstellung\\n- Beispiele:\\n  - Hierarchical Agglomerative Clustering\\n  - Divisive Hierarchical Clustering"},{"id":5,"questionDe":"(s.30) Was versteht man unter überlappendem Clustering?","questionJa":"重なり合うクラスタリングとは何かを説明せよ。","answerDe":["Ein Objekt kann zu mehreren Clustern gleichzeitig gehören","Darstellung häufig mit Venn-Diagrammen"],"answerJa":["1つのデータ点が複数のクラスタに属することがある","ベン図などで視覚的に表現される"],"explanationDe":["Beim überlappenden Clustering dürfen sich Cluster überlappen, d. h. ein Datenpunkt kann in mehreren Clustern gleichzeitig vorkommen.","Dies ist sinnvoll, wenn sich bestimmte Merkmale überlagern, z. B. bei Textdaten oder sozialen Netzwerken.","Im Beispielbild sieht man, dass Punkte wie \'j\' und \'g\' in mehreren überlappenden Bereichen vorkommen."],"explanationJa":["重なり合うクラスタリングでは、1つのデータが複数のクラスタに同時に属することができます。","これは例えば、ある人が『スポーツ好き』でも『本好き』でもあるようなケースに使われます（ソーシャルネットワークやテキスト分析など）。","スライドのベン図では、点jやgなどが2つ以上のクラスタ領域に含まれていることが視覚的に示されています。"],"originalSlideText":"- Überlappendes Clustering\\n- Darstellung: Venn-Diagramm"},{"id":6,"questionDe":"(s.30) Was ist probabilistisches Clustering?","questionJa":"確率的クラスタリングとは何かを説明せよ。","answerDe":["Jedes Objekt wird mit Wahrscheinlichkeiten verschiedenen Clustern zugewiesen","Die Wahrscheinlichkeiten pro Objekt summieren sich zu 1"],"answerJa":["各データ点は複数のクラスタに属する確率を持つ","各行の確率の合計は1になる"],"explanationDe":["Beim probabilistischen Clustering gehört ein Punkt nicht fest zu einem Cluster, sondern wird mit Wahrscheinlichkeiten verschiedenen Clustern zugewiesen.","Typische Verfahren sind z. B. das EM-Verfahren oder Gaussian Mixture Models.","Im Beispiel ist z. B. für Punkt \'a\' angegeben: 0.4 für Cluster 1, 0.1 für Cluster 2, 0.5 für Cluster 3 – insgesamt ergibt das 1."],"explanationJa":["確率的クラスタリングでは、データ点は必ずしも1つのクラスタに厳密に属するわけではなく、複数のクラスタに属する確率を持ちます。","例えばEMアルゴリズムやガウス混合モデル（GMM）などが代表的な手法です。","スライドの表では、例えばaという点はクラスタ1に属する確率が0.4、クラスタ2に0.1、クラスタ3に0.5で、合計は1になります。"],"originalSlideText":"- Probabilistisches Clustering\\n- Weise jedem Element eine Wahrscheinlichkeit zu, mit der es zu den Clustern gehört\\n- Die Wahrscheinlichkeiten summieren sich zu 1"},{"id":7,"questionDe":"(s.31) Erklären Sie den k-means Clustering Algorithmus ausführlich.","questionJa":"★k-meansクラスタリングアルゴリズムの手順を丁寧に説明せよ。","answerDe":["1. Lege die gewünschte Anzahl an Clustern k fest.","2. Wähle zufällig k Datenpunkte als Startzentren (Cluster-Zentren).","3. Berechne für jeden Datenpunkt den Abstand zu allen k Zentren.","4. Weise jeden Datenpunkt dem nächstgelegenen Zentrum zu (z. B. mit euklidischem Abstand).","5. Berechne für jedes Cluster das neue Zentrum als Mittelwert der zugewiesenen Punkte.","6. Wiederhole Schritte 3–5, bis sich die Cluster-Zuordnung nicht mehr ändert (stabiler Zustand*).","* Ein stabiler Zustand bedeutet, dass die Zuweisung der Instanzen zu den Clustern unverändert bleibt."],"answerJa":["★1. 分類したいクラスタの数 k を決める。","★2. データからランダムに k 個の点を選び、それらを初期クラスタ中心（クラスタの代表点）とする。","★3. 各データ点について、全てのクラスタ中心との距離を計算する。","★4. 各データ点を、最も近いクラスタ中心に割り当てる（ユークリッド距離などを使用）。","★5. 各クラスタに属する点の平均を計算し、新しいクラスタ中心とする。","★6. 割り当てが変化しなくなるまで（＝クラスタが安定するまで*）、手順3〜5を繰り返す。","★* クラスタが安定するとは、各データ点の所属が変わらず、中心も変化しないことを意味します。"],"explanationDe":["Der k-means Algorithmus ist ein häufig genutztes Verfahren zum Clustern von Daten.","Er arbeitet iterativ: Anfangs werden k Startpunkte zufällig gewählt. Diese gelten als die aktuellen Cluster-Zentren.","Dann wird jeder Datenpunkt dem Zentrum zugewiesen, dem er am nächsten liegt. Dabei wird meist der euklidische Abstand verwendet.","Sobald alle Punkte einem Cluster zugeordnet sind, werden die neuen Zentren als Mittelwert der Punkte innerhalb jedes Clusters berechnet.","Dieser Vorgang wird so lange wiederholt, bis sich die Zuordnung der Punkte zu Clustern nicht mehr ändert. Dann spricht man von einem stabilen Zustand.","Ein stabiler Zustand bedeutet, dass die Zuweisung der Instanzen zu den Clustern unverändert bleibt.","Die rechnerische Komplexität pro Iteration liegt bei Schritt 3: O(k·n·m), und bei Schritt 5: O(k·n·m)."],"explanationJa":["k-meansクラスタリングは、もっともよく使われるクラスタリングアルゴリズムの1つです。","最初に、全データからランダムに k 個の点を選び、それらをクラスタの中心として設定します（初期化）。","次に、各データ点について、どのクラスタ中心が最も近いかを調べ、もっとも近いクラスタに割り当てます。","割り当てが終わったら、各クラスタに属する点の平均（重心）を計算して、新しいクラスタ中心に置き換えます。","この手順（割り当て→中心更新）を繰り返し、クラスタの割り当てが変わらなくなる（＝安定）まで継続します。","クラスタが安定するとは、各データ点の所属が変わらず、中心も変化しないことを意味します。","このアルゴリズムの各繰り返しにかかる計算量は、手順3と手順5でそれぞれ O(k·n·m)（クラスタ数 k、データ数 n、次元数 m に比例）です。"],"originalSlideText":"- k-means Clustering Algorithmus\\n1. Spezifiziere die Anzahl der Cluster: k\\n2. Wähle zufällig k Punkte z1, ..., zk: Cluster-Zentren\\n3. Weise alle Instanzen ihrem nächsten Cluster-Zentrum zu\\n4. Metrik: z. B. Euklid’sche Distanz\\n5. Berechne das neue Zentrum (mean)\\n6. Falls Cluster nicht stabil → Schritt 3\\n- Cluster sind stabil: Die Zuweisung der Instanzen zu den Clustern bleibt unverändert\\n- Komplexität Schritt 3: O(k · n · m)\\n- Komplexität Schritt 5: O(k · n · m)"},{"id":8,"questionDe":"(s.31) Wie ist die Komplexität des k-means Algorithmus?","questionJa":"★k-meansクラスタリングの計算量はどのように見積もられるか？","answerDe":["Die Komplexität des k-means Algorithmus hängt von der Anzahl der Cluster k, der Anzahl der Datenpunkte n und der Dimensionalität m ab.","Pro Iteration: ","– Schritt 3 (Zuweisung der Punkte zu Clustern): O(k · n · m)","– Schritt 5 (Berechnung der neuen Zentren): O(k · n · m)","Bei T Iterationen ergibt sich die Gesamtkosten: O(T · k · n · m)"],"answerJa":["★k-meansアルゴリズムの計算量は、クラスタ数 k、データ数 n、次元数 m に依存します。","各繰り返しごとに：","★– ステップ3（各データ点を最も近いクラスタ中心に割り当てる）: O(k・n・m)","★– ステップ5（クラスタ中心の更新）: O(k・n・m)","★反復回数を T とすると、全体の計算量は O(T・k・n・m) となります。"],"explanationDe":["Bei k-means wird in jeder Iteration zunächst für jeden der n Datenpunkte der Abstand zu jedem der k Cluster-Zentren berechnet.","Da jeder Punkt m-dimensionale Merkmale hat, ergibt das eine Kostenabschätzung von O(k · n · m) für Schritt 3.","Anschließend werden in Schritt 5 für jedes Cluster die neuen Zentren als Mittelwerte der zugewiesenen Punkte berechnet, was ebenfalls O(k · n · m) kostet.","Die Gesamtkomplexität hängt linear von der Anzahl der Iterationen T ab, also: O(T · k · n · m)."],"explanationJa":["k-meansでは、各データ点についてすべてのクラスタ中心との距離を計算する必要があります（ステップ3）。","データ数が n、クラスタ数が k、次元が m のとき、このステップには O(k・n・m) の時間がかかります。","また、クラスタ中心の更新（ステップ5）でも同様に O(k・n・m) の計算量が必要です。","この処理を T 回繰り返すため、最終的な計算量は O(T・k・n・m) となります。"],"originalSlideText":"- Komplexität Schritt 3: O(k · n · m)\\n- Komplexität Schritt 5: O(k · n · m)"},{"id":9,"questionDe":"(s.32) Wie kann man die Qualität des k-means Clusterings messen?","questionJa":"★k-meansクラスタリングの品質はどのように評価されるか？","answerDe":["Die Qualität wird durch die Summe der quadrierten Abstände der Datenpunkte zu ihren jeweiligen Clusterzentren gemessen.","Formel: q = ∑(i=1 bis k) ∑(j=1 bis nᵢ) (dᵢⱼ − zᵢ)²","Dabei ist:","– zᵢ: das Zentrum des i-ten Clusters","– dᵢⱼ: der j-te Punkt im i-ten Cluster","– nᵢ: Anzahl der Punkte im i-ten Cluster"],"answerJa":["クラスタリングの品質は、各クラスタ内のデータ点とそのクラスタ中心との距離の2乗の合計によって評価されます。","数式: q = Σ(i=1〜k) Σ(j=1〜nᵢ) (dᵢⱼ − zᵢ)²","ここで：","– zᵢ：クラスタ i の中心（重心）","– dᵢⱼ：クラスタ i に属する j 番目のデータ点","– nᵢ：クラスタ i に属するデータ点の数"],"explanationDe":["Das Ziel des k-means Algorithmus ist es, die Punkte so zu gruppieren, dass sie möglichst nahe bei ihren Clusterzentren liegen.","Ein kleinerer q-Wert bedeutet, dass die Datenpunkte gut zu ihren Clustern passen.","Beispiel: Wenn du Kunden basierend auf ihrem Kaufverhalten gruppierst, zeigt ein niedriger q-Wert, dass ähnliche Kunden in denselben Gruppen sind."],"explanationJa":["k-meansの目的は、データ点がクラスタの中心にできるだけ近くなるようにグループ分けすることです。","qの値が小さいほど、データ点は自分のクラスタ中心に近く、よいクラスタリングであることを意味します。","例：買い物の傾向で顧客をクラスタに分ける場合、qが小さければ、似た購買行動をする顧客が同じグループにまとまっていると判断できます。"],"originalSlideText":"– Qualität\\n– Berechne\\nq = ∑ ∑ (dᵢⱼ − zᵢ)²\\n– zᵢ: Zentrum des i-ten Clusters\\n– dᵢⱼ: j-ter Punkt des i-ten Clusters\\n– nᵢ = |Cᵢ|","explanationImage":"lecture01/lecture05_ex02.png"},{"id":10,"questionDe":"(s.32) Welche Probleme hat der k-means Algorithmus? Nenne 5 Einflussfaktoren.","questionJa":"★k-meansアルゴリズムの問題点は何か？結果に影響を与える要因を5つ挙げよ。","answerDe":["1. Der Algorithmus berechnet nur ein lokales Minimum von q, kein globales.","2. Die Ergebnisse hängen ab von:","   – der Anzahl der Cluster k","   – der Auswahl der Startzentren","   – der \'empty-cluster\'-Strategie","   – der Größe und Varianz der Cluster","   – der Wahl des entferntesten Punktes vom Zentrum"],"answerJa":["★1. k-meansはqの局所的な最小値しか見つけられず、全体として最良（グローバル最小）な結果を保証できません。","★2. 結果は以下の5つの要因に影響されます：","   ★– クラスタ数kの選び方","   ★– 初期クラスタ中心の選び方","   – 空クラスタ（データが割り当てられないクラスタ）への対応戦略","   – 各クラスタのデータ数やばらつき（分散）","   – クラスタ中心から最も遠い点を次の中心に選ぶかどうか"],"explanationDe":["k-means ist stark abhängig von der Wahl der Anfangswerte. Schlechte Startzentren führen oft zu schlechten Clustern.","Beispiel: Wenn alle Startzentren zufällig nahe beieinander liegen, könnten viele Punkte falsch zugeordnet werden."],"explanationJa":["k-meansは初期の中心点の選び方に大きく依存するため、選び方が悪いと不適切なクラスタができることがあります。","例：初期中心が近すぎると、あるエリアだけにクラスタが集中し、他のエリアが適切に分類されません。"],"originalSlideText":"– Problematik\\n– Berechnet ein lokales Minimum q …\\n– Ergebnis ist abhängig von k, Startzentren, empty-Cluster Strategie usw."},{"id":11,"questionDe":"(s.33) Wie kann man das Ergebnis des k-means Algorithmus verbessern?","questionJa":"★k-meansクラスタリングの結果を改善するにはどうすればよいか？","answerDe":["Führe den k-means Algorithmus mehrfach mit unterschiedlichen Startzentren aus.","Wähle das Ergebnis mit dem geringsten Wert für die Qualitätsfunktion q."],"answerJa":["★k-meansクラスタリングを異なる初期中心で複数回実行し、","★その中で品質評価値qが最も小さい結果を採用する。"],"explanationDe":["Da das Ergebnis stark von den Startzentren abhängt, kann eine zufällige Wahl zu schlechten Clustern führen.","Wenn man k-means zum Beispiel 10-mal ausführt und das beste (kleinste q) nimmt, verbessert sich die Stabilität.","Beispiel: Beim Clustern von Kunden kann man verschiedene Startpunkte ausprobieren und das beste Clustering auswählen."],"explanationJa":["k-meansの結果は初期クラスタ中心に依存するため、1回の実行では不安定な場合があります。","そこで異なる初期値で何度かk-meansを実行し、最もq（クラスタ内距離の合計）が小さい結果を選ぶと良いです。","例：顧客をクラスタに分ける際、10回実行して最もグループが明確な結果を採用する方法がよく使われます。"],"originalSlideText":"– Verbesserung: Lasse k-means mehrfach mit verschiedenen Startzentren laufen\\n– Wähle Ergebnis mit geringstem q"},{"id":12,"questionDe":"(s.33) Was ist k-means++ und wie verbessert es den Algorithmus? Nenne 3 Aspekte.","questionJa":"★k-means++とは何か？アルゴリズムをどのように改善するのか。3つ挙げよ。","answerDe":["1. Wähle den ersten Startpunkt zufällig.","2. Wähle weitere Startpunkte mit einer Wahrscheinlichkeit proportional zur Entfernung zu bestehenden Zentren.","3. Erhöht die Konvergenzgeschwindigkeit und Genauigkeit."],"answerJa":["1. 最初のクラスタ中心はランダムに選ぶ。","2. 次の中心は、既に選ばれた中心から遠い点を高い確率で選ぶ。","3. これにより収束速度と精度が向上する。"],"explanationDe":["k-means++ wählt die Startzentren gezielter aus als das klassische k-means, das alle Punkte rein zufällig wählt.","Punkte, die weit von bestehenden Zentren entfernt sind, haben eine höhere Chance, ausgewählt zu werden.","Das führt zu besser verteilten Startpunkten und schnelleren, genaueren Ergebnissen."],"explanationJa":["通常のk-meansは初期中心を完全にランダムに選ぶが、k-means++はより賢く選びます。","すでに選ばれた中心から遠く離れた点を優先的に次の中心として選ぶことで、クラスタ中心が偏りにくくなります。","その結果、計算の収束が早くなり、分類の精度も向上します。"],"originalSlideText":"– k-means++:\\n– Wähle ersten Startpunkt zufällig\\n– Wähle nächste Punkt mit Wahrscheinlichkeit proportional zur Entfernung\\n– Erhöht: Konvergenzgeschwindigkeit, Genauigkeit"},{"id":13,"questionDe":"(s.34) Wie kann man eine geeignete Anzahl k der Cluster im k-means bestimmen? Nenne 3 Methoden.","questionJa":"★k-meansクラスタリングにおいて、クラスタ数kを適切に決める方法を3つ挙げよ。","answerDe":["★1. Nutze die Faustregel: k = Wurzel(n / 2)","★2. Nutze Expertenwissen aus der Domäne.","★3. Teste verschiedene Werte für k (z. B. von k−2 bis k+2) und vergleiche die Ergebnisse."],"answerJa":["1. 経験則により k = √(n / 2) とする。","2. 専門領域の知識を使って適切なクラスタ数を見積もる。","3. 初期値の周辺でkを変化させ（例：k−2〜k+2）、結果を比較して最適な値を選ぶ。"],"explanationDe":["Eine einfache Faustregel für k ist die Wurzel von n durch 2.","Besser ist jedoch die Nutzung von Vorwissen über die Daten.","Alternativ kann man k in einem Bereich testen und die beste Lösung wählen."],"explanationJa":["クラスタ数kを決めるには、まず簡単な経験則として「データ数nの半分の平方根」があります。","ただし、より信頼できる方法は、対象となる分野の知識（例：医療では患者のタイプ数など）をもとにkを見積もることです。","さらに、kの周辺値でいくつか試し、クラスタの分布や性能指標を比較して決定する方法も実用的です。"],"originalSlideText":"1. Schätzung der Anzahl der Cluster [MKB1979]\\nk = √(n / 2)\\n2. Nutze Domänenwissen zur Abschätzung der Anzahl der Cluster\\n3. Variiere Anzahl der Cluster k"},{"id":14,"questionDe":"(s.34) Wie kann man die Qualität eines Clustering-Ergebnisses bewerten? Nenne 4 mögliche Indizes.","questionJa":"クラスタリングの結果の良さを評価するにはどうすればよいか？4つの指標を挙げよ。","answerDe":["★1. Davies-Bouldin Index","★2. Modified Hubert Γ","3. Dunn Index","4. Silhouette Coefficient"],"answerJa":["1. デービス・ボールディン指数（Davies-Bouldin Index）","2. 修正ハバートΓ（Modified Hubert Γ）","3. ダン指数（Dunn Index）","4. シルエット係数（Silhouette Coefficient）"],"explanationDe":["Ein Problem ist, dass bei größerem k das Ergebnis oft besser aussieht, obwohl es überfitten kann.","Der optimale Fall wäre: k = n ⇒ q = 0, aber das ist nicht sinnvoll.","Deshalb nutzt man Cluster-Indizes wie den Silhouette Coefficient, der interne Qualität bewertet.","Diese helfen zu entscheiden, ob die Cluster gut voneinander getrennt und intern kompakt sind."],"explanationJa":["クラスタ数kを大きくすればするほど、クラスタ内の距離は小さくなり、一見良い結果に見えてしまう（過学習の危険）。","理論上はk=nならクラスタ内距離q=0になるが、それは意味がない。","そこでDavies-Bouldin指数やSilhouette係数など、クラスタのまとまりや分離の良さを測る指標が使われる。","これにより、内部評価で「本当に意味のあるクラスタ分け」ができたかを確認できる。"],"originalSlideText":"Evaluation: Optimal k=n ⇒ q=0, aber nicht sinnvoll\\nMögliche Lösung: Cluster Indices\\n– Davies-Bouldin, Hubert, Dunn, Silhouette","explanationImage":"lecture01/lecture05_ex03.png"},{"id":15,"questionDe":"(s.34) Erklären Sie den Aufbau des Davies-Bouldin Index zur Bewertung von Clustering-Ergebnissen.","questionJa":"★（スライド34）クラスタリング結果を評価するためのDavies-Bouldin Indexの構成を説明せよ。","answerDe":["Der Index besteht aus zwei Hauptkomponenten: Kohäsion und Trennung.","Kohäsion S_i misst die mittlere Abweichung der Punkte im Cluster i von ihrem Zentrum.","Trennung M_{i,j} misst den Abstand zwischen den Zentren der Cluster i und j.","Das Verhältnis R_{i,j} = (S_i + S_j) / M_{i,j} ist ein Maß für die Ähnlichkeit der Cluster i und j.","Für jeden Cluster i wird D_i = max_{j ≠ i} R_{i,j} bestimmt.","Der Davies-Bouldin Index ist der Durchschnitt aller D_i: DB = (1/k) * Σ D_i."],"answerJa":["Davies-Bouldin Indexはクラスタの評価のための指標であり、主に「まとまり」と「分離度」の2要素から構成される。","S_iはクラスタiの内部にある点が、そのクラスタの中心からどれだけ離れているか（平均的な距離）を表す。","M_{i,j}はクラスタiとjの中心点間の距離を表す。","R_{i,j} = (S_i + S_j) / M_{i,j} はクラスタiとjの類似度を表し、小さいほど良い。","各クラスタiについて、最も類似度が高い他クラスタとのR_{i,j}（最悪ケース）をD_iと定義する。","★Davies-Bouldin Indexは全クラスタのD_iの平均で定義され、値が小さいほどクラスタリングの質が良いとされる。"],"explanationDe":["Der Davies-Bouldin Index bewertet, wie gut ein Clustering gelungen ist, indem er misst, wie eng Punkte innerhalb eines Clusters beieinanderliegen (Kohäsion) und wie weit Cluster voneinander entfernt sind (Trennung).","Zum Beispiel: Wenn in einem Cluster alle Punkte dicht um das Zentrum gruppiert sind (kleines S_i), und gleichzeitig die Zentren verschiedener Cluster weit auseinanderliegen (großes M_{i,j}), ergibt sich ein kleiner R_{i,j}-Wert – was wünschenswert ist.","Für jedes Cluster wird dann der \'schlechteste Fall\' (größter R_{i,j}) betrachtet, um D_i zu bestimmen.","Am Ende berechnet man den Durchschnitt aller D_i-Werte. Je kleiner der DB-Wert, desto besser ist das Clustering.","Beispiel: Drei gut getrennte Cluster mit engen internen Punkten liefern typischerweise einen DB-Wert unter 1.0, während überlappende Cluster höhere Werte haben."],"explanationJa":["Davies-Bouldin Index（DBI）は、クラスタリング結果がどれだけ良好かを判断するための指標です。","この評価は、①クラスタ内部のまとまり（コヒージョン）と、②クラスタ間の分離度（セパレーション）を使って行われます。","たとえば、クラスタAの中の点がその中心から近ければS_Aは小さく、クラスタAとBの中心が十分離れていればM_{A,B}は大きくなります。その結果、R_{A,B}の値は小さくなり、「AとBは良く分かれていて、それぞれまとまりがある」と評価できます。","このR_{i,j}をすべてのクラスタペアに対して計算し、各クラスタiごとに最も悪い（最大の）R_{i,j}を選んでD_iとします。","そして最後に、すべてのD_iの平均を取った値がDavies-Bouldin Indexです。","DBIの値が小さいほど、クラスタ内のまとまりが良く、クラスタ同士も良く分離されていると判断できます。実際のクラスタリング評価では、DBIが1.0以下なら良好とされることが多いです。"],"originalSlideText":"• Davies Bouldin Index\\n– Kohäsion\\n  S_i = ((1 / n_i) * Σ_{j=1}^{n_i} |d_{i,j} − z_i|^p)^{1/p}\\n– Trennung\\n  M_{i,j} = ||z_i − z_j||_p = (Σ_{k=1}^m |z_{i,k} − z_{j,k}|^p)^{1/p}\\n– Maß für Qualität des Clusterings für i und j\\n  R_{i,j} = (S_i + S_j) / M_{i,j}\\n– Schlechtester Wert für i\\n  D_i = max_{j≠i} {R_{i,j}}\\n– Davies-Bouldin Index\\n  DB := (1 / k) * Σ_{i=1}^k D_i","explanationImage":"lecture01/lecture05_ex04.png"},{"id":16,"questionDe":"(s.35) Geben Sie die Berechnungskomplexitäten der einzelnen Bestandteile des Davies-Bouldin Index an.","questionJa":"（スライド35）Davies-Bouldin Index の各構成要素における計算量を挙げよ。","answerDe":["S_i: O(n · m)","M_{i,j}: O(m)","R_{i,j}: O(n · m)","D_i: O(k · n · m)","DB: O(k² · n · m)"],"answerJa":["S_i：O(n × m)","M_{i,j}：O(m)","R_{i,j}：O(n × m)","D_i：O(k × n × m)","DB：O(k² × n × m)"],"explanationDe":["Die Komplexität des Davies-Bouldin Index ergibt sich aus der Berechnung mehrerer Komponenten:","- S_i: Die Kohäsion eines Clusters i wird durch die mittlere Distanz seiner n_i Punkte zum Zentrum berechnet. Da jeder Punkt m Dimensionen hat, ergibt sich O(n · m).","- M_{i,j}: Die Trennung zwischen zwei Clustern wird als Abstand der Zentren berechnet. Jedes Zentrum hat m Dimensionen, also ist die Komplexität O(m).","- R_{i,j}: Die Berechnung dieses Wertes benötigt die vorherigen S_i und M_{i,j}-Werte, daher ist der Aufwand O(n · m).","- D_i: Für jeden Cluster muss das Maximum über k Vergleiche gebildet werden, was zu O(k · n · m) führt.","- DB: Da D_i für jedes der k Cluster berechnet werden muss, ergibt sich insgesamt eine Komplexität von O(k² · n · m)."],"explanationJa":["Davies-Bouldin Index の計算にはいくつかのステップがあり、それぞれに計算量（どれだけの計算コストがかかるか）が異なります：","- S_i（クラスタ内の平均距離）：n 個のデータポイントを m 次元空間で計算するので O(n × m)。","- M_{i,j}（クラスタ間の中心点距離）：m 次元で2つの中心点の距離を計算するだけなので O(m)。","- R_{i,j}（クラスタ i と j の類似度）：S_i, S_j, M_{i,j} を使って計算されるため、全体で O(n × m)。","- D_i（クラスタ i における最悪の類似度）：k 個のクラスタ全てとの比較が必要で、それぞれに O(n × m) かかるので O(k × n × m)。","- DB（Davies-Bouldin Index）：全ての D_i（k 個）を計算するので O(k² × n × m)。","例えば、100個のデータ（n=100）、10次元（m=10）、5個のクラスタ（k=5）の場合、DBI全体の計算コストは O(25 × 100 × 10) = O(25,000) となる。"],"originalSlideText":"• Davies Bouldin Index\\n– Komplexität\\n  – S_i: O(n · m)\\n  – M_{i,j}: O(m)\\n  – R_{i,j}: O(n · m + n · m + m) = O(n · m)\\n  – D_i: O(k · n · m)\\n  – DB: O(k · k · n · m) = O(k² · n · m)"},{"id":17,"questionDe":"(s.36) Wie bestimmt man mit Hilfe des Davies-Bouldin Index die optimale Anzahl an Clustern anhand der Tabelle und des Diagramms?","questionJa":"★（スライド36）グラフを用いて、Davies-Bouldin Index による最適なクラスタ数の決定方法を説明せよ。","answerDe":["Berechne den Davies-Bouldin Index für verschiedene k-Werte","Trage die Werte in einer Tabelle und einem Diagramm auf","Wähle das k mit dem kleinsten Indexwert","Im Beispiel ist der kleinste Wert bei k = 6"],"answerJa":["さまざまなクラスタ数 k に対して Davies-Bouldin Index を計算する","得られた値を表やグラフにまとめて比較する","★最も Davies-Bouldin Index が小さい k を選ぶ","★この例では、最小値は k = 6 のときに得られる"],"explanationDe":["Der Davies-Bouldin Index misst die Qualität eines Clusterings – je kleiner der Wert, desto besser ist die Trennung und Kompaktheit der Cluster.","Man berechnet den Index für verschiedene Werte von k (Anzahl der Cluster), z. B. k = 3 bis k = 9.","Die berechneten Werte werden in einer Tabelle dargestellt und zur besseren Visualisierung in einem Diagramm geplottet.","Im Diagramm erkennt man, dass der niedrigste Wert des Index bei k = 6 liegt – dies ist also die optimale Anzahl an Clustern für diesen Datensatz.","Beispiel: Wenn man einen Datensatz mit Kundenprofilen analysiert, kann man anhand des niedrigsten Indexwertes erkennen, dass sich die Kunden am besten in 6 Gruppen segmentieren lassen."],"explanationJa":["Davies-Bouldin Index はクラスタリングの質を測る指標で、値が小さいほど、クラスタ同士がよく分かれており、内部ではまとまりがあることを示します。","このスライドでは、k（クラスタ数）を3〜9まで変えて、各値に対する Davies-Bouldin Index を計算しています。","その結果は左の表に、また右のグラフに視覚的に描かれています。","グラフを見ると、k = 6 のときに Index の値が最小になっているため、このデータに最も適したクラスタ数は6だと判断できます。","たとえば、ある商品の購入履歴から顧客をグルーピングしたいとき、Indexが最小となるk=6を使えば、顧客を6つの代表的なパターンに分けられるという意味になります。"],"originalSlideText":"• Davies Bouldin Index\\n\\nk   | Davies Bouldin Index\\n3   | 1,06957\\n4   | 1,16492\\n5   | 1,02300\\n6   | 1,00282\\n7   | 1,06513\\n8   | 1,02402\\n9   | 1,02164\\n\\n– Wähle k mit kleinstem Index: 6","questionImage":"lecture01/lecture05_q01.png"},{"id":18,"questionDe":"(s.38) Erklären Sie die Berechnung und Bedeutung des Modified Hubert Γ Index.","questionJa":"★（スライド38）Modified Hubert Γ 指数の計算方法とその意味を説明せよ。","answerDe":["Nutzt eine Clusterzuweisung L(i), die jedes Objekt einem Cluster zuordnet","Verwendet zwei Matrizen: X(i,j) misst Abstand der Objekte, Y(i,j) misst Abstand ihrer Clusterzentren","Berechnet Γ als Summe der Produkte von X(i,j) und Y(i,j)","Komplexität: O(n²·m²), unabhängig von k"],"answerJa":["各データ点 i をクラスタ k に割り当てるクラスタインジケータ L(i) を使用","2つの行列 X(i,j)（点同士の距離）と Y(i,j)（クラスタ中心の距離）を用いる","X(i,j) と Y(i,j) の積を全ての i, j 組について足し合わせて Γ を計算","計算量は O(n²·m²)、クラスタ数 k には依存しない"],"explanationDe":["Der Modified Hubert Γ Index ist ein Maß zur Bewertung eines Clusterings, indem er die Beziehung zwischen den Datenpunkten und ihren zugehörigen Clustern analysiert.","Zunächst wird jedem Objekt i ein Cluster k zugewiesen, also L(i) = k.","Dann werden zwei Matrizen erstellt: X(i,j) misst den absoluten Abstand zwischen den Datenpunkten i und j. Y(i,j) misst den Abstand zwischen den Zentroiden (Mittelpunkten) der Cluster, zu denen i und j gehören.","Indem man das Produkt X(i,j) * Y(i,j) für alle Punktpaare aufsummiert, erhält man einen Wert Γ, der angibt, wie gut die Zuordnung mit den Distanzen übereinstimmt.","Ein hohes Γ kann darauf hinweisen, dass ähnliche Punkte tendenziell im gleichen Cluster landen.","Beispiel: Wenn zwei Punkte sehr nah beieinander liegen (kleines X), aber deren Clusterzentren weit auseinander (großes Y), dann deutet dies auf eine schlechte Clustereinteilung hin – der Γ-Wert fällt entsprechend ungünstig aus."],"explanationJa":["Modified Hubert Γ 指数は、データ点の間の距離と、それらが属するクラスタの中心間の距離との関係からクラスタリングの質を評価する指標です。","まず、各データ点 i に対してクラスタ番号 k を割り当てる関数 L(i) を使い、i がどのクラスタに属するかを決定します。","次に2つの行列を作成します。X(i,j) はデータ点 i と j のユークリッド距離（または他の距離）を表します。Y(i,j) は、それぞれの点が属するクラスタの中心（重心）間の距離を表します。","これらの X(i,j) × Y(i,j) の積をすべての i, j 組について足し合わせた値が Γ です。","この値が高いほど、データ点の距離とクラスタ中心の距離がよく一致しており、クラスタリングがデータの構造をよく反映していると解釈されます。","例えば、似た特徴を持つユーザー（距離が近い）が異なるクラスタに分類されていれば、対応する Y(i,j) は大きくなり、Γ の値は悪くなります。"],"originalSlideText":"• Modified Hubert Γ\\n\\n– Clusterindikator: L(i) = k\\n  Objekt i gehört zu Cluster k\\n\\n– Matrizen:\\n  X(i,j) = |d_i – d_j|\\n  Y(i,j) = |z_L(i) – z_L(j)|\\n\\n– Modified Hubert Γ:\\n  Γ = Σ_{i=1}^{n–1} Σ_{j=i+1}^{n} (X(i,j) · Y(i,j))\\n\\n– Komplexität\\n  – O(n · n · m · m) = O(n² · m²)\\n  – unabhängig von k","explanationImage":"lecture01/lecture05_ex04.png","questionImage":""},{"id":19,"questionDe":"(s.39) Wie bestimmt man anhand des Modified Hubert Γ Index die optimale Anzahl an Clustern?","questionJa":"★（スライド39）グラフを用いて、Modified Hubert Γ 指数を使った最適なクラスタ数の決定方法を説明せよ。","answerDe":["Berechne den Hubert Γ Index für verschiedene k-Werte","Trage die Werte in eine Tabelle und ein Diagramm ein","Wähle das k mit dem größten Anstieg des Γ-Wertes"],"answerJa":["複数のクラスタ数 k に対して Modified Hubert Γ 値を計算する","結果を表とグラフで確認する","★Γ 値が最も大きく増加した k を選ぶ"],"explanationDe":["Der Modified Hubert Γ Index hilft bei der Auswahl der optimalen Anzahl von Clustern.","In diesem Beispiel wurden Γ-Werte für k von 3 bis 9 berechnet. Diese Werte wurden in einer Tabelle dargestellt und als Linie im Diagramm geplottet.","Die Strategie: Man sucht den größten Sprung nach oben – also den Punkt, an dem der Index am stärksten ansteigt.","In diesem Fall zeigt sich ein großer Anstieg von k = 4 (0,97873 · 10¹¹) auf k = 5 (1,11428 · 10¹¹). Daher wird k = 5 als optimale Clusteranzahl ausgewählt.","Diese Methode basiert auf der Annahme, dass ein deutlicher Anstieg auf eine starke Verbesserung der Clusterstruktur hinweist."],"explanationJa":["Modified Hubert Γ 指数は、クラスタリングの結果の評価に基づいて、最適なクラスタ数を選ぶために使われます。","このスライドでは、クラスタ数 k = 3〜9 に対して Γ の値を計算し、それを表とグラフにして比較しています。","最も注目すべきなのは、Γ 値が大きく上昇するポイントです。そこが最もクラスタ構造が改善されたタイミングと見なされます。","具体的には、k = 4 の値（0.97873×10¹¹）から k = 5（1.11428×10¹¹）への上昇が最も大きいため、k = 5 が最適とされます。","★これは『エルボー法』に似た考え方で、急激な変化点を最適と見なします。"],"originalSlideText":"• Davies Bouldin Index\\n\\n| k | Modified Hubert Γ |\\n|---|---------------------|\\n| 3 | 1,01844 · 10¹¹       |\\n| 4 | 0,97873 · 10¹¹       |\\n| 5 | 1,11428 · 10¹¹       |\\n| 6 | 1,15922 · 10¹¹       |\\n| 7 | 1,15276 · 10¹¹       |\\n| 8 | 1,08876 · 10¹¹       |\\n| 9 | 1,10869 · 10¹¹       |\\n\\n– Wähle k nach dem größten Anstieg: 5","explanationImage":"","questionImage":"lecture01/lecture05_q02.png"},{"id":20,"questionDe":"(s.40–41) Erklären Sie das grundlegende Prinzip des Dunn-Index.","questionJa":"Dunn Indexの基本的な仕組みを説明せよ。","answerDe":["Dunn-Index misst das Verhältnis zwischen Cluster-Trennung und Cluster-Kohäsion","Großer Wert bedeutet gut getrennte und kompakte Cluster","Formel: minimaler Abstand zwischen Clustern geteilt durch maximale Ausdehnung eines Clusters"],"answerJa":["Dunn Indexは、クラスタ間の分離度とクラスタ内の密集度の比を測る指標である","値が大きいほど、クラスタがよく分かれており、内部がまとまっていることを示す","計算式：クラスタ間の最小距離 ÷ クラスタ内の最大の広がり（凝集度）"],"explanationDe":["Der Dunn-Index bewertet die Qualität einer Clusterstruktur, indem er zwei zentrale Aspekte ins Verhältnis setzt: (1) wie weit die Cluster voneinander entfernt sind (Trennung) und (2) wie kompakt jedes Cluster für sich ist (Kohäsion).","Je höher der Dunn-Index, desto besser ist die Clustering-Ergebnis: Die Cluster sind dann sowohl gut voneinander getrennt als auch intern dicht.","Beispiel: Angenommen, wir haben drei Cluster: A, B und C. Wenn die Abstände zwischen den Clustern groß sind (z. B. 10 Einheiten zwischen A und B) und gleichzeitig die Punkte innerhalb jedes Clusters eng beieinanderliegen (z. B. maximal 2 Einheiten), dann ergibt sich ein hoher Dunn-Index, etwa 10 / 2 = 5.","Im Gegensatz dazu, wenn sich Cluster stark überlappen oder intern sehr weit gestreut sind, wird der Dunn-Index klein – was auf eine schlechte Trennung hinweist."],"explanationJa":["Dunn Indexは、クラスタリング結果の良し悪しを評価するための指標で、2つの要素を比べて計算されます： (1)クラスタどうしの距離（どれだけ離れているか）、(2)クラスタ内部のまとまり（どれだけ密集しているか）です。","この2つの要素の比、つまり「クラスタ間の最小距離 ÷ クラスタ内の最大の広がり」で計算され、値が大きいほど良いクラスタリング結果とされます。","例：3つのクラスタA, B, Cがあり、AとBの間の最小距離が10、A内の最も離れた2点の距離が2だった場合、Dunn Indexは 10 ÷ 2 = 5 になります。これは、クラスタ間の分離が良く、各クラスタが内部でまとまっている（＝良い）ことを意味します。","一方、もしクラスタどうしが近すぎたり、クラスタ内の点がばらけすぎていると、Dunn Indexは小さくなり、クラスタリングの質は悪いと判断されます。"],"originalSlideText":"- Berechnung\\n  DI_k = (min_{1 ≤ i < j ≤ k} δ(C_i, C_j)) / (max_{1 ≤ l ≤ k} Δ_l)\\n- δ(C_i, C_j): Abstand zwischen Clustern i und j\\n- Δ_l: Kohäsion des Clusters l\\n- Komplexität: abhängig von δ(C_i, C_j) und Δ_l","explanationImage":"","questionImage":""},{"id":21,"questionDe":"(s.42) Wie bestimmt man die optimale Clusteranzahl mit dem Dunn-Index anhand der grafischen Darstellung?","questionJa":"★（スライド42）Dunnインデックスとそのグラフを用いて最適なクラスタ数をどのように決定するか？","answerDe":["Berechne den Dunn-Index für verschiedene k-Werte","Wähle den k-Wert mit dem kleinsten Dunn-Index"],"answerJa":["さまざまなクラスタ数 k に対して Dunn インデックスを計算する","★最も Dunn インデックスが小さい k を選択する"],"explanationDe":["Der Dunn-Index ist ein Maß für die Qualität einer Clusterlösung: Er kombiniert Informationen über die Trennung der Cluster und deren Kompaktheit.","In der Grafik sind die berechneten Dunn-Indizes für verschiedene Werte von k (Anzahl der Cluster) dargestellt.","Je kleiner der Dunn-Index, desto besser ist typischerweise die Clusterung – weil die Cluster gut getrennt und intern kompakt sind.","In diesem Beispiel hat k = 6 den kleinsten Wert (~0,00055217), was darauf hinweist, dass 6 Cluster eine besonders gute Trennung und Kohärenz aufweisen.","Beispiel: Wenn du Datenpunkte aus verschiedenen Regionen (z.B. Kunden aus Städten) clustern möchtest, zeigt dir der niedrigste Dunn-Wert, wie du diese am besten aufteilst."],"explanationJa":["Dunnインデックスは、クラスタの分離度（どれだけ離れているか）と凝集度（どれだけ密集しているか）を組み合わせて評価する指標です。","このスライドのグラフには、クラスタ数 k を3から9まで変化させたときのDunnインデックスの値が描かれています。","★一般的に、Dunnインデックスが小さいほど、クラスタが互いに離れていて、各クラスタ内のデータがよくまとまっていると考えられます。","この例では、k=6 のときに Dunnインデックスが最も小さく（約0.00055217）、このクラスタ数が最も良い分割であると考えられます。","例：複数の都市の顧客をクラスタリングしたいとき、Dunnインデックスが最も小さくなるクラスタ数が、顧客の分布の特徴を最もよく表しています。"],"originalSlideText":"• Dunn Index\\n\\n| k | Dunn Index |\\n|---|-------------|\\n| 3 | 3,00291 · 10⁻⁵ |\\n| 4 | 3,67348 · 10⁻⁵ |\\n| 5 | 1,95366 · 10⁻⁵ |\\n| 6 | 0,55217 · 10⁻⁵ |\\n| 7 | 3,67917 · 10⁻⁵ |\\n| 8 | 2,71530 · 10⁻⁵ |\\n| 9 | 3,25993 · 10⁻⁵ |\\n\\n– Wähle k mit dem kleinsten Index: 6","explanationImage":"","questionImage":"lecture01/lecture05_q03.png"},{"id":22,"questionDe":"Wie funktioniert der Algorithmus zum Aufbau eines k-d-Baums zur Nearest-Neighbor-Suche?","questionJa":"★k-d木（k-d-Baum）を用いた最近傍探索の構築アルゴリズムはどのように動作しますか？","answerDe":["1. Sortiere Punkte für jedes Attribut","2. Beginne mit Attribut i = 1","3. Erzeuge Knoten und starte Rekursion","4. Teile die Daten anhand des Medians","5. Speichere den Unterteilungswert im aktuellen Knoten","6. Erzeuge linken und rechten Kindknoten rekursiv, wenn nötig"],"answerJa":["★1. 各属性ごとにデータ点をソートする","★2. 属性iを1に初期化する","★3. ノードを作成し、再帰を開始する","★4. データを中央値で分割する","★5. 分割値を現在のノードに記録する","★6. 必要に応じて、左と右の子ノードを再帰的に生成する"],"explanationDe":["Der Aufbau eines k-d-Baums ist ein rekursiver Algorithmus zur effizienten Suche von nächstgelegenen Nachbarn (Nearest Neighbor).","Zunächst werden die Datenpunkte für jedes Attribut (z. B. x, y, z) sortiert. Dann beginnt man mit dem ersten Attribut (i = 1) und erstellt den Wurzelknoten.","In der Rekursion wird die Datenmenge entlang des aktuellen Attributs i anhand des Medians in zwei gleich große Teilmengen geteilt. Dieser Medianwert wird im Knoten gespeichert – er dient als Entscheidungskriterium für spätere Suchen.","Für jede Teilmenge werden Kindknoten erzeugt: zunächst der linke Kindknoten mit allen Punkten kleiner oder gleich dem Median, dann der rechte mit den größeren.","Falls eine Teilmenge noch mehr Punkte enthält als ein gewisser Schwellenwert, wird die Rekursion aufgerufen, wobei das nächste Attribut verwendet wird (i ← i + 1). Ist i größer als die Anzahl der Attribute, beginnt man wieder bei 1.","So entsteht ein binärer Baum, in dem die Datenpunkte auf Basis ihrer Koordinaten hierarchisch organisiert sind. Dies ermöglicht effiziente Nachbarschaftsabfragen in hohen Dimensionen."],"explanationJa":["k-d木（k-dimensional tree）は、最近傍探索（Nearest Neighbor Search）を効率的に行うための木構造です。このスライドでは、その構築アルゴリズムを説明しています。","まず、すべてのデータ点を各属性（たとえばx軸、y軸、z軸など）ごとにソートします。そして、初期の分割軸（i = 1）で処理を開始します。","再帰処理では、現在の属性iに沿ってデータを中央値で2つに分けます。たとえば、x軸で分割するなら、xの値が中央値より小さいか大きいかで分けます。この分割点（中央値）は、木のノードに記録されます。","その後、左の子ノードには小さいグループ、右の子ノードには大きいグループを再帰的に割り当てていきます。分割対象のデータが多い場合は、次の属性（i ← i + 1）を使って再帰的に同じ手続きを続けます。属性の数を超えたら、1に戻ります。","このようにして、空間を交互に分割していくことで、データ点を効率よく分類する木構造ができ、検索処理を高速化できます。"],"originalSlideText":"Nearest Neighbor\\n\\n• Algorithmus Aufbau k-d-Baum\\n\\n– Initialisierung\\n  1. Sortiere Punkte für jedes Attribut\\n  2. i ← 1\\n  3. Erzeuge Knoten\\n  4. Starte Rekursion\\n\\n– Rekursion\\n  1. Teile alle Datenpunkte bezüglich Attribut i in zwei gleich große Teilmengen (verwende Median)\\n  2. Trage Unterteilungswert t in aktuellen Knoten ein\\n  3. Erzeuge linken Kindknoten\\n     1. Weise ihm seine Datenpunkte zu\\n     2. Falls Anzahl der Datenpunkte größer als Schwellwert\\n         1. i ← i + 1\\n         2. Rekursiver Aufruf\\n  4. Erzeuge rechten Kindknoten\\n     1. Weise ihm seine Datenpunkte zu\\n     2. Falls Anzahl der Datenpunkte größer als Schwellwert\\n         1. i ← i + 1\\n         2. Rekursiver Aufruf","explanationImage":"","questionImage":""},{"id":23,"questionDe":"Was beschreibt der Silhouette Coefficient bei der Clusteranalyse?","questionJa":"★クラスタ分析においてシルエット係数（Silhouette Coefficient）は何を示しますか？","answerDe":["Er misst, wie gut ein Punkt zu seinem eigenen Cluster passt im Vergleich zu anderen Clustern.","Er basiert auf den Abständen zu Punkten im eigenen und im nächstgelegenen fremden Cluster.","Wird für jeden Punkt berechnet mit einem Wert zwischen -1 und 1."],"answerJa":["★シルエット係数は、あるデータ点が自分のクラスタにどれだけ適しているかを測る指標です。","★自分のクラスタ内の点との距離と、最も近い別クラスタとの距離を比較します。","各点について -1〜1 の値で計算されます。"],"explanationDe":["Der Silhouette Coefficient s(d_i) beschreibt für jeden Datenpunkt d_i, wie ähnlich er seinem eigenen Cluster C_i ist im Vergleich zum nächsten benachbarten Cluster C_n.","Die Formel lautet: s(d_i) = (d(d_i, C_n) – d(d_i, C_i)) / max{d(d_i, C_n), d(d_i, C_i)}.","Hierbei ist d(d_i, C) die durchschnittliche Distanz von d_i zu allen Punkten im Cluster C. Je näher s(d_i) bei 1 liegt, desto besser passt der Punkt zu seinem eigenen Cluster.","Ein Wert nahe 0 bedeutet, dass der Punkt sich zwischen zwei Clustern befindet. Ein negativer Wert weist darauf hin, dass der Punkt möglicherweise falsch zugeordnet wurde."],"explanationJa":["シルエット係数 s(d_i) は、各データ点 d_i がどれだけ自分のクラスタにうまく収まっているかを評価するための指標です。","具体的には、d_i が属するクラスタ C_i 内の他の点との平均距離と、最も近い別のクラスタ C_n との平均距離を比較して計算します。","数式：s(d_i) = (d(d_i, C_n) – d(d_i, C_i)) / max{d(d_i, C_n), d(d_i, C_i)}","★s(d_i) が1に近いほど自分のクラスタに適していることを意味し、0に近いと境界にあり、負の値は誤ってクラスタリングされている可能性を示します。"],"originalSlideText":"Erzeugung von Clustern\\n• Silhouette Coefficient\\n– Berechnung\\n  s(d_i) = (d(d_i, C_n) – d(d_i, C_i)) / max{d(d_i, C_i), d(d_i, C_n)}\\n  – Abstand zwischen d_i und einem Cluster C:\\n    d(d_i, C) = (1 / n_C) * ∑_{d_j ∈ C} d(d_i, d_j)\\n  – C_i: Cluster von d_i\\n  – C_n = argmin_{C ≠ C_i} d(d_i, C): Cluster, der Element mit kleinstem Abstand zu d_i enthält","explanationImage":"lecture01/lecture05_ex06.png","questionImage":""},{"id":24,"questionDe":"(s.44) Was beschreibt der Silhouette Coefficient in der Clusteranalyse und wie wird er berechnet?","questionJa":"クラスタ分析におけるシルエット係数とは何で、どのように計算されますか？","answerDe":["Misst, wie gut ein Punkt zu seinem eigenen Cluster passt im Vergleich zu anderen.","Wird mit s(d_i) = (d(d_i, C_n) – d(d_i, C_i)) / max{d(d_i, C_n), d(d_i, C_i)} berechnet.","Verwendet Durchschnittsabstände zu eigenem und nächstem fremdem Cluster."],"answerJa":["ある点が自分のクラスタにどれだけ適しているかを他のクラスタと比較して測る指標。","計算式：s(d_i) = (d(d_i, C_n) – d(d_i, C_i)) / max{d(d_i, C_n), d(d_i, C_i)}","自分のクラスタと最も近い他クラスタへの平均距離を使う。"],"explanationDe":["Der Silhouette Coefficient s(d_i) bewertet für jeden Punkt d_i, wie gut seine Zugehörigkeit zu einem bestimmten Cluster ist.","Dazu wird die durchschnittliche Distanz zu Punkten im eigenen Cluster C_i berechnet sowie zu Punkten im nächsten benachbarten Cluster C_n.","Die Formel s(d_i) = (d(d_i, C_n) – d(d_i, C_i)) / max{d(d_i, C_n), d(d_i, C_i)} ergibt einen Wert zwischen -1 und 1.","Ein hoher Wert bedeutet, dass d_i gut in sein Cluster passt und weit von anderen Clustern entfernt ist."],"explanationJa":["シルエット係数 s(d_i) は、各データ点 d_i がどのクラスタに適切に属しているかを評価する指標です。","具体的には、自分のクラスタ C_i 内の他の点との平均距離と、最も近い別クラスタ C_n との平均距離を用いて、式：s(d_i) = (d(d_i, C_n) – d(d_i, C_i)) / max{…} により算出されます。","値は -1〜1の範囲で、1に近いほどクラスタリングが良好であることを意味します。"],"originalSlideText":"• Silhouette Coefficient\\n– Berechnung\\n  s(d_i) = (d(d_i, C_n) – d(d_i, C_i)) / max{d(d_i, C_i), d(d_i, C_n)}\\n  – Abstand zwischen d_i und einem Cluster C:\\n    d(d_i, C) = (1 / n_C) * ∑_{d_j ∈ C} d(d_i, d_j)\\n  – C_i: Cluster von d_i\\n  – C_n = argmin_{C ≠ C_i} d(d_i, C): Cluster, der Element mit kleinstem Abstand zu d_i enthält","explanationImage":"lecture01/lecture05_ex07.png","questionImage":""},{"id":25,"questionDe":"(s.44) Wie werden Silhouette-Werte interpretiert und zur Bewertung der Clusterqualität genutzt?","questionJa":"★シルエット係数の値はどのように解釈され、クラスタの質の評価にどう使われますか？","answerDe":["s > 0,75 → starke Trennung","0,5 < s ≤ 0,75 → normale Clusterung","0,25 < s ≤ 0,5 → schwach","0 < s ≤ 0,25 → keine sinnvolle Trennung"],"answerJa":["s > 0.75：クラスタが明確に分離されている（強い）","0.5 < s ≤ 0.75：通常レベルのクラスタリング","0.25 < s ≤ 0.5：クラスタがあいまい（弱い）","0 < s ≤ 0.25：クラスタ構造が成立していない"],"explanationDe":["Der Silhouette-Wert s(d_i) kann zwischen -1 und 1 liegen, wird in der Praxis aber meist zwischen 0 und 1 verwendet.","Je höher der Wert, desto besser ist die Zuordnung des Punkts zu seinem Cluster.","Allgemeine Interpretation:\\n• 0,75–1: starke Trennung → gut strukturierte Cluster\\n• 0,5–0,75: akzeptabel\\n• 0,25–0,5: eher schwach → Clustergrenzen unklar\\n• 0–0,25: kaum Struktur → möglicherweise willkürlich"],"explanationJa":["シルエット係数の値 s(d_i) は -1〜1 の間をとりますが、実際には 0〜1 を重視して評価します。","値が高いほど、クラスタへの所属が明確であるとされます。","評価の目安は以下の通りです：\\n・0.75〜1：クラスタが明確に分離されている（強い構造）\\n・0.5〜0.75：普通のクラスタ構造\\n・0.25〜0.5：やや曖昧\\n・0〜0.25：ほとんどクラスタとして機能していない可能性"],"originalSlideText":"– Auswertung\\n  0,75 < s(i) ≤ 1 → strong\\n  0,5 < s(i) ≤ 0,75 → normal\\n  0,25 < s(i) ≤ 0,5 → weak\\n  0 < s(i) ≤ 0,25 → no","explanationImage":"lecture01/lecture05_ex08.png","questionImage":""},{"id":26,"questionDe":"(s.45) Was zeigt der Silhouette Coefficient und wie interpretiert man das gezeigte Diagramm?","questionJa":"シルエット係数とは何か、また図の意味をどのように解釈すればよいか？","answerDe":["Der Silhouette-Koeffizient misst die Qualität der Clusterzuweisung eines Punkts.","Werte nahe 1 bedeuten gute Clusterzugehörigkeit.","Werte nahe 0 oder negativ deuten auf schlechte oder falsche Zuordnung hin.","Im Diagramm erkennt man: die meisten Punkte haben hohe Werte, einige wenige sehr niedrige."],"answerJa":["シルエット係数は、各データ点がどれだけ適切なクラスタに属しているかを評価する指標である。","値が1に近いほど、その点は正しくクラスタに分類されていることを意味する。","値が0または負の場合は、その点が間違ったクラスタに分類されている可能性がある。","図からは、多くの点が高い係数（=良好な分類）を持っていることがわかるが、一部に非常に低い点も見られる。"],"explanationDe":["Der Silhouette Coefficient bewertet, wie gut ein Punkt zu seinem eigenen Cluster passt, im Vergleich zu den anderen Clustern.","Für jeden Punkt wird ein Wert zwischen -1 und 1 berechnet:","- Ein Wert nahe 1 bedeutet, der Punkt ist gut innerhalb seines Clusters positioniert.","- Ein Wert nahe 0 bedeutet, der Punkt liegt auf der Grenze zwischen zwei Clustern.","- Ein negativer Wert bedeutet, der Punkt wäre in einem anderen Cluster besser aufgehoben.","Das Diagramm zeigt die Silhouette-Werte aller Punkte: die meisten liegen bei 0,9 oder höher, was auf eine sehr gute Clusterung hinweist.","Beispiel: Bei der Kundensegmentierung zeigt ein Wert von 0,95, dass der Kunde klar einem bestimmten Profil zugeordnet werden kann."],"explanationJa":["シルエット係数は、あるデータ点が自分のクラスタにどれだけ適切に属しているかを数値で示す評価指標です。","この値は -1 から 1 の間をとり、以下のように解釈します：","- 1 に近い：非常に適切なクラスタに分類されている","- 0 に近い：クラスタの境界に近く、分類が曖昧","- 負の値：他のクラスタに分類すべきだった可能性がある","このスライドのグラフでは、ほとんどの点の値が 0.9 以上と非常に高く、クラスタリングがうまくいっていることを示しています。","例：顧客のクラスタリングにおいて、シルエット係数が 0.95 の顧客は、そのクラスタ（たとえば『高級志向・頻繁な購入者』など）に非常によく合致していると解釈できます。"],"originalSlideText":"• Silhouette Coefficient\\n\\n[グラフ表示]","explanationImage":"","questionImage":"lecture01/lecture05_q04.png"},{"id":27,"questionDe":"(s.46) Wie kann das k-means Clustering effizienter gemacht werden?","questionJa":"★k-meansクラスタリングを効率化する方法にはどのようなものがありますか？","answerDe":["Durch Raumteilungsverfahren wie k-d-Bäume oder Ball-Trees","Durch Parallelisierung der Berechnungen"],"answerJa":["k-d木やBall-Treeなどの空間分割法を使う","計算処理を並列化することで高速化する"],"explanationDe":["Die klassische k-means-Methode kann bei großen Datenmengen langsam werden, da in jeder Iteration alle Punkte mit allen Zentren verglichen werden müssen.","Eine Möglichkeit zur Beschleunigung ist die Nutzung von Raumteilungsverfahren wie k-d-Bäumen (k-dimensional trees) oder Ball-Trees. Diese strukturieren den Raum so, dass unnötige Distanzberechnungen vermieden werden können, indem Punkte effizient nach ihrer Lage im Raum gruppiert werden.","Zum Beispiel erlaubt ein k-d-Baum eine schnelle Suche nach dem nächsten Zentrum durch rekursive Raumteilung.","Ball-Trees arbeiten ähnlich, sind aber besser für hochdimensionale Daten geeignet.","Ein weiterer Beschleunigungsansatz ist die Parallelisierung: verschiedene Threads oder Rechenkerne führen Teile der Berechnungen gleichzeitig aus – z. B. die Distanzberechnungen oder die Zuweisung von Punkten zu Clustern.","Beide Methoden zusammen – Raumteilung und Parallelisierung – können k-means erheblich schneller machen, insbesondere bei großen Datensätzen."],"explanationJa":["k-meansクラスタリングは、大量のデータに対して繰り返し全ての点とクラスタ中心の距離を計算する必要があり、処理が重くなります。","この問題を解決する1つの方法は、「空間分割法」の導入です。k-d木（k-dimensional tree）やBall-Treeのようなデータ構造を使うことで、空間内のデータ点を構造的に分割し、無駄な距離計算を省けます。","k-d木は、空間を再帰的に分割することで最近傍検索を高速に行えます。一方、Ball-Treeは高次元データに適しており、球状の領域に基づいて分割を行います。","もう一つの高速化手法は「並列化（パラレル処理）」です。複数のスレッドやCPUコアで、距離計算やクラスタへの割り当てなどを同時に実行することで、計算時間を大きく短縮できます。","これらの手法を組み合わせることで、大規模なデータに対してもk-meansを高速に実行できます。"],"originalSlideText":"Erzeugung von Clustern\\n• k-means Clustering\\n– Beschleunigung [HD2015]:\\n  – Raumteilungsverfahren\\n    – k-d-Bäume (k-dimensionale Bäume)\\n    – Ball-Trees [WFH2011]\\n  – Parallelisierung","explanationImage":"","questionImage":""},{"id":28,"questionDe":"(s.47) Wie hilft ein k-d-Baum bei der Beschleunigung von k-means Clustering?","questionJa":"k-d木はどのようにして k-meansクラスタリングの高速化に役立ちますか？","answerDe":["k: Anzahl der Dimensionen (entspricht der Anzahl der Attribute m)","Aufbau: O(k · n · log n)","Sortiere n Punkte nach k Attributen"],"answerJa":["k：次元数（属性数 m に相当）","構築：O(k・n・log n)","n個の点をk個の属性でソートする"],"explanationDe":["Ein k-d-Baum (k-dimensionaler Baum) ist eine Datenstruktur, die verwendet wird, um Punkte im k-dimensionalen Raum effizient zu organisieren.","Beim k-means Clustering müssen in jeder Iteration die Datenpunkte dem nächstgelegenen Clusterzentrum zugeordnet werden. Das bedeutet viele Distanzvergleiche.","Ein k-d-Baum hilft, diese Vergleiche effizienter zu gestalten, indem er die Punkte rekursiv nach Attributen sortiert und in einem Baum speichert.","Beim Aufbau des Baums werden n Punkte entlang k Attributen sortiert – das ergibt die Komplexität O(k · n · log n).","Ein Beispiel: Wenn man 10.000 Punkte mit 3 Attributen (x, y, z) hat, werden diese im Baum so organisiert, dass man bei der Clustereinteilung nicht jeden Punkt mit jedem Zentrum vergleichen muss, sondern gezielt suchen kann."],"explanationJa":["k-d木（k-dimensional tree）は、k次元空間内の点を効率的に管理・検索するための構造です。","k-meansクラスタリングでは、各データ点を最も近いクラスタ中心に割り当てる必要があり、そのために多くの距離計算が必要です。","k-d木は、各点を属性（次元）ごとにソートし、再帰的に空間を分割することで、クラスタ中心との距離比較を効率化します。","構築時には、n個の点をk個の属性（たとえばx, y, zなど）で並べ替えて木構造を作るため、計算量は O(k・n・log n) になります。","例えば、3次元空間にある1万点をクラスタリングする場合、全組み合わせを調べるよりも、k-d木を使うことで近傍探索を高速化できるのです。"],"originalSlideText":"– k-d-Bäume (k-dimensionale Bäume)\\n  – k: Anzahl der Dimensionen\\n    – entspricht der Anzahl der Attribute m\\n  – Aufbau: O(k · n · log n)\\n    – Sortiere n Punkte nach k Attributen","explanationImage":"","questionImage":""},{"id":29,"questionDe":"(s.47) Wann lohnt sich der Einsatz von k-d-Bäumen im k-means Clustering zur Nearest-Neighbor-Suche?","questionJa":"k-meansクラスタリングにおいて、k-d木による最近傍探索はどのような場合に有効ですか？","answerDe":["Nearest-Neighbor-Suche: O(n · log n)","Sinnvoll bei vielen Datenpunkten","Langsam für m > 8 [HD2015]"],"answerJa":["最近傍探索：O(n・log n)","★データ点が多い場合に有効","★次元数が8を超えると遅くなる（HD2015）"],"explanationDe":["Die Nearest-Neighbor-Suche ist ein zentraler Bestandteil von k-means, da jeder Punkt dem nächstgelegenen Clusterzentrum zugewiesen werden muss.","Mit einem k-d-Baum kann diese Suche mit einer Komplexität von O(n · log n) durchgeführt werden – das ist wesentlich schneller als eine naive lineare Suche in O(n).","Diese Methode ist besonders vorteilhaft, wenn die Anzahl der Datenpunkte (n) sehr groß ist – also bei Massendaten.","Allerdings ist die Effizienz des k-d-Baums stark von der Dimensionalität abhängig. Für Daten mit hoher Dimension (>8 laut HD2015) wird die Struktur ineffektiv.","Dies liegt am sogenannten \'Fluch der Dimensionalität\', bei dem die Raumaufteilung durch den Baum in höheren Dimensionen kaum noch Vorteile bringt."],"explanationJa":["k-meansクラスタリングでは、各データ点を最近傍のクラスタ中心に割り当てる必要があり、その計算は重要なステップです。","k-d木を使えば、この最近傍探索を O(n・log n) の計算量で実行できます。これは、すべての中心との距離を1つずつ調べる O(n) よりも高速です。","この方法は、特にデータ点の数が多い（n が大きい）場合に有効です。","ただし、次元数（属性数）が増えると、木構造による空間分割の効果が薄れます。HD2015では、次元が8を超えるとこの手法の性能が大きく低下するとされています。","これは「次元の呪い（Curse of Dimensionality）」と呼ばれる現象で、高次元ではデータが空間に拡散し、分割の意味が薄れることに起因します。"],"originalSlideText":"– Nearest-Neighbor-Suche: O(n · log n)\\n– Sinnvoll bei\\n  – Vielen Datenpunkten\\n  – Wenigen Dimensionen\\n    (Langsam für m > 8 [HD2015])","explanationImage":"","questionImage":""},{"id":30,"questionDe":"(s.48) Warum ist es schwer zu sagen, welches Clustering das richtige ist, und welche weiteren Entscheidungen müssen getroffen werden?","questionJa":"どのクラスタリングが正しいかを判断するのが難しいのはなぜで、どんな追加の判断が必要になりますか？","answerDe":["Frage: Welches Clustering ist die richtige?","→ Im Allgemeinen nicht entscheidbar","Zusatzfragen:","– Welches Verfahren soll gewählt werden?","– Welche Parameter sollen für das Verfahren gewählt werden?","– Wie viele Cluster sollen gewählt werden?"],"answerJa":["問い：どのクラスタリングが正しいか？","→ 一般には決めることができない","追加の問い：","– どのクラスタリング手法を選ぶべきか？","– その手法に必要なパラメータはどう設定するか？","– いくつのクラスタに分けるべきか？"],"explanationDe":["In der Clusteranalyse gibt es oft keine objektiv \'richtige\' Lösung. Verschiedene Verfahren können auf denselben Datensatz unterschiedliche Gruppierungen erzeugen.","Ein klassisches Beispiel ist k-means: Mit k = 3 und k = 5 entstehen ganz unterschiedliche Clusterstrukturen. Welche davon besser ist, hängt vom Kontext und dem Ziel ab.","Neben der Verfahrenswahl müssen weitere Entscheidungen getroffen werden: Welcher Algorithmus eignet sich für die Datenstruktur – z. B. k-means für kugelförmige Cluster, DBSCAN für dichtebasierte Cluster?","Auch Parameter spielen eine zentrale Rolle: Bei k-means z. B. die Anzahl k der Cluster. Bei DBSCAN z. B. ε (Radius) und die minimale Punktanzahl.","Die Anzahl der Cluster ist meist unbekannt und muss z. B. durch Validierungsmaße wie den Davies-Bouldin Index abgeschätzt werden.","Diese Vielzahl an Entscheidungen macht Clusteranalyse zu einem explorativen Prozess."],"explanationJa":["クラスタ分析では、どのクラスタリング結果が“正しい”と断定することは一般にはできません。同じデータに対して異なる手法を使うと、異なるクラスタ構造が得られることが多いからです。","典型的な例が k-means クラスタリングです。クラスタ数 k によって結果が大きく変わります。たとえば、k=3 と k=5 では、まったく異なるクラスタが生成されます。どちらが良いかは、分析目的やデータの性質によって異なります。","また、単に手法を選ぶだけでなく、次のような追加判断も必要になります：\\n・どのアルゴリズムがデータ構造に適しているか（例：k-means は球状クラスタ向き、DBSCAN は密度ベース）\\n・その手法にどんなパラメータが必要か（k-meansならクラスタ数k、DBSCANならεと最小点数）","クラスタ数が事前にわからない場合も多く、Davies-Bouldin Index などの評価指標を用いて、どの分割がより妥当かを判断します。","このようにクラスタリングは、様々な選択肢と試行錯誤が伴う探索的プロセスです。"],"originalSlideText":"Erzeugung von Clustern\\n\\n– Frage: Welches Clustering ist die richtige?\\n  – Im Allgemeinen nicht entscheidbar\\n\\n– Zusatzfragen\\n  – Welches Verfahren soll gewählt werden?\\n  – Welche Parameter sollen für das Verfahren gewählt werden?\\n  – Wie viele Cluster sollen gewählt werden?","explanationImage":"","questionImage":""},{"id":31,"questionDe":"(s.49) Wie funktioniert das Evidence Accumulation Clustering nach FJ2002?","questionJa":"★Evidence Accumulation Clustering（FJ2002）はどのような仕組みで動作しますか？","answerDe":["Unterteile: Mehrfaches k-means mit unterschiedlichen Initialisierungen","Verbinde: Berechne co_assoc(i,j) = votes_{i,j} / N","Vereinigung: Nutze minimalen Spannbaum mit Schwellwert t zur Clusterbildung"],"answerJa":["分割（Split）：k-means を複数回（初期値を変えて）実行","結合（Combine）：co_assoc(i,j) = votes_{i,j} / N を計算","統合（Merge）：最小全域木としきい値 t によりクラスタを決定"],"explanationDe":["Das Evidence Accumulation Clustering kombiniert mehrere k-means-Ergebnisse, um robustere Cluster zu erhalten.","Im ersten Schritt (Split) wird k-means N-mal mit verschiedenen Startwerten durchgeführt, sodass unterschiedliche Gruppierungen entstehen.","Im zweiten Schritt (Combine) wird gezählt, wie oft zwei Datenpunkte i und j im selben Cluster liegen. Daraus wird der Co-Association-Wert co_assoc(i,j) berechnet: votes_{i,j} / N.","Diese Co-Association-Matrix ist eine Art Ähnlichkeitsmatrix: Je höher der Wert, desto häufiger wurden i und j gemeinsam gruppiert.","Im letzten Schritt (Merge) wird ein Minimaler Spannbaum (Minimum Spanning Tree) auf dieser Matrix berechnet. Durch Setzen eines Schwellwerts t werden Kanten entfernt, um Cluster zu identifizieren.","Diese Methode ist nützlich, wenn man die Stabilität von Clustern über mehrere Läufe hinweg analysieren möchte."],"explanationJa":["Evidence Accumulation Clustering（証拠蓄積クラスタリング）は、k-meansを複数回行い、安定したクラスタ構造を抽出する手法です。","まず「分割（Split）」段階で、k-meansをN回、異なる初期化で実行します。これにより、複数のクラスタリング結果が得られます。","次に「結合（Combine）」段階では、データ点iとjが何回同じクラスタになったか（votes_{i,j}）を数え、全体の回数Nで割って共起頻度（co_assoc(i,j)）を計算します。","このco-association行列は、データ点同士の“似ている度合い”を表す類似度行列となります。","最後に「統合（Merge）」段階で、この行列を基に最小全域木（Minimum Spanning Tree）を構築し、しきい値t以上の距離を持つエッジを切ることでクラスタを形成します。","この手法は、複数回のクラスタリング結果から一貫性のあるクラスタを抽出したい場合に有効です。"],"originalSlideText":"Erzeugung von Clustern\\n\\n• Evidence Accumulation [FJ2002]\\n\\n– Unterteile (Split)\\n  – Unterteile die Daten in mehrere Kugel-förmige Cluster\\n  – Berechne N-mal k-means mit unterschiedlichen Initialisierungen\\n\\n– Verbinde (Combine)\\n  – Berechne\\n    co_assoc(i,j) = votes_{i,j} / N\\n  – votes_{i,j}: Anzahl der Durchläufe bei denen i und j im gleichen Cluster liegen\\n\\n– Vereinige (Merge)\\n  – Verwende Minimalen Spannbaum mit Schwellwert t um Cluster zu bestimmen","explanationImage":"lecture01/lecture05_ex09.png","questionImage":""},{"id":32,"questionDe":"(s.50) Wie funktioniert der Algorithmus für Evidence Accumulation nach FJ2002 im Detail?","questionJa":"★Evidence Accumulation（FJ2002）のアルゴリズムは具体的にどのように動作しますか？","answerDe":["Führe k-means N-mal aus mit unterschiedlichen Startzentren","Aktualisiere co_assoc(i,j) bei jedem Lauf: +1/N","Vereine Cluster für Paare (i,j), wenn co_assoc(i,j) ≥ t","Nicht zugewiesene Objekte → eigene Einzel-Cluster"],"answerJa":["k-means を N回（初期化を変えて）実行","各回ごとに co_assoc(i,j) に 1/N を加算して更新","co_assoc(i,j) がしきい値 t 以上なら、iとjの属するクラスタを結合","どのクラスタにも属さない点は単独クラスタとする"],"explanationDe":["Der Evidence Accumulation Algorithmus läuft in mehreren Phasen ab.","1. Wiederhole N-mal:\\n  – Wähle k zufällige Startzentren\\n  – Führe k-means Clustering durch\\n  – Für alle Paare (i,j), die im gleichen Cluster sind: co_assoc(i,j) += 1/N","So entsteht eine Co-Assoziationsmatrix, in der jeder Wert zwischen 0 und 1 angibt, wie häufig zwei Punkte zusammen gruppiert wurden.","2. Betrachte alle Punktpaare (i,j). Wenn co_assoc(i,j) ≥ t (Schwellwert), werden ihre Cluster vereinigt.","3. Punkte, die am Ende zu keinem Cluster gehören, bilden ein eigenes 1-Element-Cluster.","Diese Methode hilft, über viele Läufe stabile Clusterbeziehungen zu identifizieren und ist weniger abhängig von einzelnen Initialisierungen."],"explanationJa":["Evidence Accumulation のアルゴリズムは、複数回のクラスタリング結果から“信頼できるクラスタ関係”を抽出することを目的としています。","1. 最初に、k-meansを N 回実行します（毎回、クラスタ中心をランダムに初期化）。\\n　– 各実行で、同じクラスタに入った点 i, j に対して co_assoc(i,j) に 1/N を加算します。\\n　– こうして、ペアごとの共起頻度（0〜1）を蓄積した行列が得られます。","2. 次に、すべての点ペア (i,j) について、co_assoc(i,j) がしきい値 t 以上であれば、iとjのクラスタを統合します。","3. 最後に、どのクラスタにも属さなかった点は、1要素のクラスタとして個別に扱います。","このアルゴリズムは、k-meansのような初期値に敏感な手法から得られる結果のばらつきを補正し、より安定したクラスタを見つけるために有効です。"],"originalSlideText":"Erzeugung von Clustern\\n• Evidence Accumulation [FJ2002]\\n– Algorithmus\\n– N-mal\\n  – Wähle k Cluster-Zentren\\n  – Berechne Clustering mit k-means\\n  – Aktualisiere die Co-Assoziationsmatrix\\n    co_assoc(i,j) ← co_assoc(i,j) + 1/N\\n– Für alle Paare (i,j)\\n  – Falls t ≤ co_assoc(i,j) vereinige die Cluster in denen i und j liegen\\n– Für alle Objekte, die nicht in einem Cluster liegen\\n  – Bilde ein-elementigen Cluster mit diesem Objekt","explanationImage":"lecture01/lecture05_ex10.png","questionImage":""},{"id":33,"questionDe":"(s.51) Was zeigt die Assoziationsmatrix im Rahmen von Evidence Accumulation anhand dieses Beispiels?","questionJa":"★この例において、Evidence Accumulation によるアソシエーション行列は何を示していますか？","answerDe":["Sie basiert auf 4 Durchläufen von k-means mit k = 2.","Jeder Eintrag gibt an, wie oft zwei Objekte gemeinsam im selben Cluster waren.","Werte sind zwischen 0 (nie zusammen) und 1 (immer zusammen)."],"answerJa":["この行列は k = 2 での k-means を4回実行した結果に基づいています。","各セルの値は、対応する2つのオブジェクトが同じクラスタになった割合を示します。","値は 0（常に別クラスタ）から 1（常に同じクラスタ）の間です。"],"explanationDe":["In diesem Beispiel wurde k-means mit k = 2 insgesamt 4-mal ausgeführt, jeweils mit unterschiedlicher Initialisierung.","In jeder Spalte der linken Tabelle ist das Clustering-Ergebnis eines Laufs dargestellt (1 oder 2 = Cluster-ID).","Die rechte Matrix zeigt, wie oft zwei Objekte im selben Cluster waren. Beispiel: d₁ und d₃ waren in 3 von 4 Läufen zusammen → Wert = 0,75.","Diese Matrix ist symmetrisch und hat Einsen auf der Diagonalen, da jedes Objekt immer mit sich selbst im selben Cluster ist.","Sie dient als Grundlage für das finale Clustering: Durch Setzen eines Schwellenwerts t kann man entscheiden, welche Objekte als zusammengehörig betrachtet werden sollen."],"explanationJa":["この例では、k-means（クラスタ数 k = 2）を初期化を変えて4回実行しています。その結果が左の表で、各データ点（d₁〜d₇）が各回にどのクラスタに属したかを示しています。","右側のアソシエーション行列（共起行列）は、任意の2点が4回のうち何回同じクラスタに分類されたかを割合（0〜1）で示したものです。","たとえば、d₁とd₃の値は 0.75 → 4回中3回同じクラスタだったという意味です。","この行列は対称であり、対角要素はすべて1になります（自分自身とは常に同じクラスタ）。","このアソシエーション行列を使い、しきい値 t を設定して、共通度が高いペアを同じクラスタとして最終クラスタリングを構築します。"],"originalSlideText":"Erzeugung von Clustern\\n• Evidence Accumulation [FJ2002]\\n– Ergebnis 4-mal k-means mit k = 2\\n– Assoziationsmatrix","explanationImage":"","questionImage":"lecture01/lecture05_q05.png"},{"id":34,"questionDe":"(s.52) Wie beeinflusst der Schwellenwert t das Clustering-Ergebnis bei Evidence Accumulation?","questionJa":"Evidence Accumulation において、しきい値 t を変えるとクラスタリング結果はどう変化しますか？","answerDe":["0 ≤ t ≤ 0,25: Alle Objekte in einem Cluster","0,25 < t ≤ 0,75: Zwei Cluster {d₁,d₂,d₃,d₄} und {d₅,d₆,d₇}","0,75 < t ≤ 1: Drei Cluster {d₁,d₂}, {d₃,d₄}, {d₅,d₆,d₇}"],"answerJa":["0 ≤ t ≤ 0.25：全データが1つのクラスタにまとめられる","0.25 < t ≤ 0.75：2つのクラスタに分割される（{d₁〜d₄}, {d₅〜d₇}）","0.75 < t ≤ 1：3つのクラスタに分かれる（{d₁,d₂}, {d₃,d₄}, {d₅,d₆,d₇}）"],"explanationDe":["Die Co-Assoziationsmatrix zeigt, wie oft zwei Objekte gemeinsam gruppiert wurden.","Ein Schwellenwert t wird verwendet, um zu entscheiden, ab welchem Wert zwei Objekte als zusammengehörig gelten.","Bei kleinem t (z. B. ≤ 0,25) reicht bereits ein schwacher Zusammenhang, um Objekte zu einem Cluster zu verbinden – Ergebnis: ein großer Cluster.","Bei mittlerem t (z. B. 0,5) werden nur deutlich ähnliche Objekte zusammengefasst – zwei Cluster entstehen.","Bei hohem t (z. B. > 0,75) werden nur sehr stark assoziierte Paare gruppiert – die Cluster werden feiner, es entstehen mehr Gruppen.","Dieses Vorgehen ermöglicht es, durch Variation von t verschiedene Detailstufen im Clustering zu untersuchen."],"explanationJa":["co-association 行列では、各データ点のペアがどれだけ頻繁に同じクラスタになったかを記録しています。","この値に対して、しきい値 t を設定し、それ以上の共起度をもつペア同士を同じクラスタとして扱います。","t が小さい（たとえば t ≤ 0.25）場合は、共通度が低くても同じクラスタとみなされるため、すべての点が1つのクラスタにまとめられます。","t が中程度（たとえば 0.5）の場合は、より強い結びつきが必要となり、クラスタが2つに分かれます。","t が大きい（たとえば > 0.75）場合は、非常に密接なペアしか同じクラスタとされないため、クラスタがさらに細かく分割され、3つのクラスタに分かれます。","このように、しきい値 t を変えることで、クラスタリングの“粒度”を調整できます。"],"originalSlideText":"Erzeugung von Clustern\\n• Evidence Accumulation [FJ2002]\\n– Ergebnis 4-mal k-means mit k = 2\\n– Ergebnis\\n  – 0 ≤ t ≤ 0,25:\\n    {d₁, d₂, d₃, d₄, d₅, d₆, d₇}\\n  – 0,25 < t ≤ 0,75:\\n    {d₁, d₂, d₃, d₄}, {d₅, d₆, d₇}\\n  – 0,75 < t ≤ 1:\\n    {d₁, d₂}, {d₃, d₄}, {d₅, d₆, d₇}","explanationImage":"","questionImage":"lecture01/lecture05_q06.png"},{"id":35,"questionDe":"(s.53) Welche Speicherprobleme entstehen bei Evidence Accumulation und welche Alternativen gibt es?","questionJa":"★Evidence Accumulation ではどのようなメモリ上の問題があり、それに対する代替策は何ですか？","answerDe":["Die Co-Assoziationsmatrix braucht O(n²) Speicher","Alternative: Cluster-Zugehörigkeit pro Lauf speichern (O(n·k))","Verwende Hamming-Distanz zur Gruppierung"],"answerJa":["★co-association 行列は O(n²) のメモリを消費する","代替案：各実行ごとのクラスタ所属情報のみを保存（O(n·k)）","クラスタベクトル間のハミング距離でグループ化を行う"],"explanationDe":["Ein Nachteil der Evidence Accumulation Methode ist der hohe Speicherbedarf: Die Co-Assoziationsmatrix benötigt O(n²) Platz, da sie für jedes Objektpaar (i,j) einen Eintrag enthält.","Dies wird bei großen n (Anzahl der Datenpunkte) unpraktisch.","Eine Alternative besteht darin, für jedes Objekt nur zu speichern, in welchen Clustern es in den verschiedenen Durchläufen lag (Speicherbedarf: O(n·k), wenn k = Anzahl der Durchläufe).","Diese Informationen können als sogenannte Cluster-Vektoren betrachtet werden.","Die Hamming-Distanz zwischen zwei Vektoren gibt dann an, wie ähnlich zwei Objekte bezüglich ihrer Clusterzugehörigkeit sind.","Wenn zwei Objekte in allen Durchläufen immer im selben Cluster waren (Hamming-Distanz = 0), können sie zusammen gruppiert werden."],"explanationJa":["Evidence Accumulation 手法の課題の1つは、co-association 行列のメモリ使用量が O(n²) にもなることです。これは、すべてのオブジェクトのペア (i, j) に対して値を保存する必要があるため、大規模データでは非効率になります。","この問題に対しての代替案として、各データ点が各クラスタリング実行時にどのクラスタに所属していたかという情報（クラスタ所属ベクトル）を保存する方法があります。これは O(n·k) のメモリで済みます（n: データ数、k: 実行回数）。","このベクトル同士の“似ている度合い”を測るために、ハミング距離を使用します。ハミング距離とは、2つのベクトルで異なる位置の数を数える距離のことです。","たとえば、すべての実行で同じクラスタに分類されていた2点のハミング距離は 0 になります。これを基準にして、似たクラスタ履歴をもつ点をグループ化できます。","この方法は、メモリ消費を抑えつつ、Evidence Accumulation の基本的な考え方を活かす実用的な手法です。"],"originalSlideText":"• Evidence Accumulation [FJ2002]\\n– Bemerkungen\\n  – Zur Unterteilung können beliebige Cluster-Verfahren benutzt werden\\n  – Die Matrix benötigt O(n²) Speicher\\n    → ungünstig bei großem n\\n  – Alternative\\n    – Speichere für jeden Durchlauf in welchem Cluster das Objekt liegt (Speicher: O(n · k))\\n    – Berechne Hamming-Distanz der Clustervektoren\\n    – Hamming Distanz 1 gruppiert alle Objekte, welche in allen Durchläufen im jeweils gleichen Cluster waren","explanationImage":"","questionImage":""},{"id":36,"questionDe":"(s.54) Was zeigt das Beispiel für Consensus Clustering mit k-means?","questionJa":"★この k-means を用いた Consensus Clustering の例は何を示していますか？","answerDe":["4 Läufe k-means mit k = 2","Consensus Clustering ergibt 3 Cluster","C₁ = {d₁, d₂}, C₂ = {d₃, d₄}, C₃ = {d₅, d₆, d₇}"],"answerJa":["k = 2 で k-means を4回実行","最終的に3つのクラスタが得られた","C₁ = {d₁, d₂}、C₂ = {d₃, d₄}、C₃ = {d₅, d₆, d₇}"],"explanationDe":["Consensus Clustering kombiniert mehrere Clustering-Ergebnisse, um robuste Gruppen zu identifizieren.","In diesem Beispiel wurde k-means viermal mit k = 2 durchgeführt. Die Tabelle zeigt, zu welchem Cluster (1 oder 2) jedes Objekt in jedem Lauf gehörte.","Obwohl k = 2 verwendet wurde, ergeben sich am Ende durch Analyse der Gruppenkonsistenz drei stabile Cluster:","– C₁ = {d₁, d₂}: Diese zwei Objekte waren immer zusammen in einem Cluster.","– C₂ = {d₃, d₄}: Auch diese waren stets gemeinsam gruppiert.","– C₃ = {d₅, d₆, d₇}: Diese drei wechselten häufiger zwischen Clustern, bildeten aber intern eine stabile Gruppe.","Consensus Clustering erkennt solche stabilen Muster, unabhängig von der Clusteranzahl k im Ausgangsverfahren."],"explanationJa":["コンセンサス・クラスタリングは、複数回のクラスタリング結果から共通性の高いグループ（安定したクラスタ）を見つけ出す手法です。","この例では、k = 2 を指定して k-means を4回実行し、それぞれのデータ点がどのクラスタに属したかを記録しています（左の表）。","その結果を基に、以下のような安定したクラスタ構造が見出されました：","・C₁ = {d₁, d₂}：常に同じクラスタに属していた","・C₂ = {d₃, d₄}：一貫してペアでクラスタ化された","・C₃ = {d₅, d₆, d₇}：頻繁にクラスタが変わるが、この3つの間には一貫性が見られた","このように、k-means での個別の結果よりも、複数回の実行を通して見えてくる“共通のクラスタ構造”を抽出するのがこの手法の目的です。"],"originalSlideText":"Erzeugung von Clustern\\n• Consensus Clustering\\n\\n– k-means\\n  – 4 Läufe\\n  – k = 2\\n\\n– Ergebnis\\n  – 3 Cluster\\n    – C₁ = {d₁, d₂}\\n    – C₂ = {d₃, d₄}\\n    – C₃ = {d₅, d₆, d₇}","explanationImage":"","questionImage":"lecture01/lecture05_q07.png"},{"id":37,"questionDe":"(s.55) Was ist der Unterschied zwischen agglomerativem und divisivem hierarchischem Clustering?","questionJa":"★凝集型（agglomerativ）と分割型（divisiv）の階層的クラスタリングの違いは何ですか？","answerDe":["Agglomerativ: Bottom-up Technik, Start mit m Einzelobjekten","Divisiv: Top-down Technik, Start mit einer Gesamtklasse","Unterschied liegt im Anfangszustand und Richtung der Gruppierung"],"answerJa":["凝集型：Bottom-up（下から上へ）の手法で、各点を個別クラスタとして開始","分割型：Top-down（上から下へ）の手法で、全体を1つのクラスタとして開始","違いはクラスタリングの開始状態と統合・分割の方向にある"],"explanationDe":["Hierarchisches Clustering umfasst zwei Hauptvarianten: agglomerativ (bottom-up) und divisiv (top-down).","Beim agglomerativen Verfahren startet man mit g = m, d. h. jedes Objekt ist seine eigene Klasse: Cᵢ⁰ = {dᵢ}. Dann werden sukzessive die ähnlichsten Cluster zusammengeführt.","Beim divisiven Verfahren startet man mit g = 1, d. h. alle Objekte sind anfangs in einem gemeinsamen Cluster C₁⁰ = {d₁, ..., dₘ}. Dieses Cluster wird dann schrittweise in kleinere Gruppen aufgeteilt.","Die beiden Verfahren erzeugen am Ende beide eine Hierarchie von Clustern (z. B. dargestellt als Dendrogramm), aber sie arbeiten in entgegengesetzter Richtung."],"explanationJa":["階層的クラスタリングには、大きく分けて「凝集型（agglomerative）」と「分割型（divisive）」の2つのアプローチがあります。","凝集型（bottom-up）では、最初に各データ点をそれぞれ1つのクラスタ（Cᵢ⁰ = {dᵢ}）とし、それらを類似度に基づいて徐々に統合していきます。","分割型（top-down）では、全体を1つのクラスタ（C₁⁰ = {d₁, ..., dₘ}）として始め、類似度などに基づいて徐々に細かく分割していきます。","どちらも最終的にはクラスタの階層構造（例：デンドログラム）を作成しますが、クラスタ形成の方向性が正反対です。"],"originalSlideText":"Erzeugung von Clustern\\n• Hierarchical Agglomerative Clustering\\n  – Bottom-Up Technik\\n  – Anfangszustand:\\n    – Jedes Objekt ist in einer eigenen Klasse\\n    – g = m\\n    – ∀i: Cᵢ⁰ = {dᵢ}\\n    – G⁰ := {C₁⁰, ..., Cg⁰}\\n\\n• Divisive Hierarchical Clustering\\n  – Top-Down Technik\\n  – Anfangszustand:\\n    – Alle Objekte sind in einer Klasse\\n    – g = 1\\n    – G⁰ = {C₁⁰} = {{d₁, ..., dₘ}}","explanationImage":"","questionImage":""},{"id":38,"questionDe":"(s.56) Wie funktioniert der nächste Schritt beim agglomerativen bzw. divisiven hierarchischen Clustering?","questionJa":"凝集型・分割型の階層クラスタリングにおける次のステップはどのように行われますか？","answerDe":["Agglomerativ: Wähle Ähnlichkeitsmaß s, vereine ähnlichste Klassen","Divisiv: Wähle Abstandsmaß t, teile eine Klasse in zwei maximal getrennte Gruppen"],"answerJa":["凝集型：類似度指標 s を用いて、最も似ている2クラスを統合する","分割型：距離指標 t を用いて、1つのクラスを2つに最大限離して分割する"],"explanationDe":["Beim agglomerativen Clustering wird in jedem Schritt ein Ähnlichkeitsmaß s verwendet, um die zwei aktuell ähnlichsten Cluster Cⱼ⁰ und Cₖ⁰ zu bestimmen.","Diese beiden Cluster werden zu einem neuen Cluster Cⱼ¹ vereinigt: Cⱼ¹ = Cⱼ⁰ ∪ Cₖ⁰.","Die Clusterliste G¹ wird aktualisiert, indem Cⱼ⁰ und Cₖ⁰ entfernt und Cⱼ¹ hinzugefügt wird.","Beim divisiven Clustering wird in jedem Schritt ein Abstandsmaß t verwendet, um einen bestehenden Cluster C₁⁰ in zwei Teilmengen C₁₁¹ und C₁₂¹ aufzuteilen.","Diese zwei Teilmengen müssen disjunkt sein (keine Überlappung) und die Trennung t(C₁₁¹, C₁₂¹) soll maximal sein – also möglichst stark voneinander getrennt.","Beide Methoden erzeugen hierarchische Clusterstrukturen, arbeiten aber in entgegengesetzter Richtung und nutzen unterschiedliche Kriterien zur Entscheidung, welche Gruppen zusammengeführt oder aufgeteilt werden."],"explanationJa":["凝集型クラスタリング（Agglomerative）では、各ステップで類似度指標 s を用いて、現在最も似ている2つのクラス（Cⱼ⁰とCₖ⁰）を選び、それらを統合します。","統合後の新しいクラスタは Cⱼ¹ = Cⱼ⁰ ∪ Cₖ⁰ として定義され、クラスタ集合 G¹ は更新されます。","一方、分割型クラスタリング（Divisive）では、距離指標 t を用いて、既存の1つのクラス（C₁⁰）を2つの非重複なグループ（C₁₁¹とC₁₂¹）に分割します。","このとき、2つのグループの間の距離（t(C₁₁¹, C₁₂¹)）が最大になるように分割を行います。つまり、最も異なるように分けます。","このように、凝集型と分割型はそれぞれ“最も近いものを統合”または“最も離れたように分割”するという反対の方向から階層構造を構築します。"],"originalSlideText":"Erzeugung von Clustern\\n• Hierarchical Agglomerative Clustering\\n  – Wähle ein Ähnlichkeitsmaß s\\n  – Vereinige die beiden ähnlichsten Klassen:\\n    – Seien j ≠ k: s(Cⱼ⁰, Cₖ⁰) maximal\\n    – Cⱼ¹ = Cⱼ⁰ ∪ Cₖ⁰\\n    – G¹ = G⁰ \\\\ {Cⱼ⁰, Cₖ⁰} ∪ Cⱼ¹\\n\\n• Divisive Hierarchical Clustering\\n  – Wähle ein Abstandsmaß t\\n  – Unterteile C₁⁰ in zwei Klassen:\\n    – G¹ := {C₁₁¹, C₁₂¹}\\n    – C₁₁¹ ∪ C₁₂¹ = D\\n    – C₁₁¹ ∩ C₁₂¹ = ∅\\n    – t(C₁₁¹, C₁₂¹) ist maximal","explanationImage":"","questionImage":""},{"id":39,"questionDe":"(s.57) Wie sehen die Endzustände beim agglomerativen und divisiven hierarchischen Clustering aus?","questionJa":"凝集型および分割型の階層クラスタリングにおける終了状態はどのようになりますか？","answerDe":["Agglomerativ: Endzustand ist eine einzige Klasse","Divisiv: Endzustand sind m Einzelklassen","Endzustände entsprechen jeweils dem Anfangszustand des anderen Verfahrens"],"answerJa":["凝集型：すべてのデータが1つのクラスタに統合された状態で終了","分割型：各データ点が1つずつのクラスタに分かれた状態で終了","両者の終了状態は、それぞれ相手の開始状態と一致する"],"explanationDe":["Beim Hierarchical Agglomerative Clustering werden die Datenpunkte schrittweise zu größeren Gruppen zusammengeführt, bis alle Objekte in einer einzigen Klasse enthalten sind.","Dieser Endzustand besteht also aus genau einem Cluster Gᵐ = {C₁ᵐ} = {{d₁, ..., dₘ}}.","Interessanterweise entspricht dieser Endzustand dem Anfangszustand des Divisive Hierarchical Clustering, das genau umgekehrt funktioniert.","Beim Divisive Clustering startet man mit einer einzigen Klasse und teilt sie schrittweise auf, bis jede Klasse genau ein Objekt enthält: Gᵐ = {C₁ᵐ, ..., Cgᵐ}, wobei Cᵢᵐ = {dᵢ}.","Dieser Endzustand entspricht wiederum dem Anfangszustand des Agglomerative Clustering.","Beide Verfahren durchlaufen also die gleiche Menge an Zwischenzuständen, aber in entgegengesetzter Reihenfolge."],"explanationJa":["階層的クラスタリングにおいて、凝集型と分割型ではプロセスの方向が逆であるため、終了状態も対照的です。","凝集型クラスタリング（Hierarchical Agglomerative）は、すべてのデータを1つのクラスタに統合するまで繰り返します。最終的なクラスタ集合は Gᵐ = {C₁ᵐ} = {{d₁, ..., dₘ}} となり、これは分割型の開始状態と同じです。","一方、分割型クラスタリング（Divisive）は、最初は全体が1つのクラスタで、そこから徐々に分割していきます。最終的には、各データ点が1つのクラスタに属する形になり、Gᵐ = {C₁ᵐ, ..., Cgᵐ}（各 Cᵢᵐ = {dᵢ}）になります。","この終了状態は、凝集型クラスタリングの開始状態と一致しています。","つまり、両者は異なる方向から出発して、同じ階層構造を異なる順序で構築していると言えます。"],"originalSlideText":"Erzeugung von Clustern\\n• Hierarchical Agglomerative Clustering\\n  – Wiederhole bis nur noch eine Klasse vorhanden ist:\\n    – Gᵐ = {C₁ᵐ} = {{d₁, ..., dₘ}}\\n  – Endzustand entspricht Ausgangszustand des „Divisive Hierarchical Clustering”\\n\\n• Divisive Hierarchical Clustering\\n  – Wiederhole bis jede Klasse genau ein Objekt enthält:\\n    – g = m\\n    – ∀i: Cᵢᵐ = {dᵢ}\\n    – Gᵐ := {C₁ᵐ, ..., Cgᵐ}\\n  – Endzustand entspricht Ausgangszustand des „Hierarchical Agglomerative Clustering”","explanationImage":"","questionImage":""},{"id":40,"questionDe":"(s.58) Welche Abstandsmaße werden beim agglomerativen Clustering verwendet?","questionJa":"★凝集型クラスタリングで使われる距離（リンク関数）はどのようなものですか？","answerDe":["Oft wird die euklidische Distanz verwendet","★Single Linkage: minimale Distanz zwischen Punkten zweier Cluster","Complete Linkage: maximale Distanz zwischen Punkten zweier Cluster","Achtung: euklidische Distanz ist nicht skalierungsinvariant"],"answerJa":["よく使われるのはユークリッド距離（ただしスケーリングに不変ではない）","Single Linkage：2つのクラスタ間で最も近い点同士の距離","Complete Linkage：2つのクラスタ間で最も遠い点同士の距離"],"explanationDe":["Beim Hierarchical Agglomerative Clustering ist die Wahl des Abstandsmaßes entscheidend für die Bildung der Clusterstruktur.","Die euklidische Distanz ist ein häufig genutztes Maß. Sie misst den Luftlinienabstand zwischen zwei Punkten im Merkmalsraum. Allerdings ist sie nicht invariant gegenüber Skalierung – d.h., wenn ein Attribut stärker gewichtet (z. B. in anderen Einheiten) vorliegt, kann dies die Clusterbildung verzerren.","Zur Bestimmung des Abstands zwischen zwei Clustern werden sogenannte Linkage-Methoden eingesetzt:","- Single Linkage: t_SL(Cₖ, Cₗ) ist die kürzeste Distanz zwischen einem Punkt aus Cluster Cₖ und einem Punkt aus Cluster Cₗ (‚nächstes Paar‘). Dadurch entstehen oft längliche, kettenartige Cluster.","- Complete Linkage: t_CL(Cₖ, Cₗ) ist die größte Distanz zwischen allen Punktpaaren zweier Cluster. Diese Methode erzeugt tendenziell kompakte, runde Cluster.","Je nach Problemstellung und Datenverteilung kann die Wahl der Linkage-Methode das Ergebnis stark beeinflussen."],"explanationJa":["階層的凝集クラスタリングでは、クラスタ間の距離の定義（リンク関数）が結果に大きな影響を与えます。","よく使われるユークリッド距離は、2点間の直線距離（通常の長さ）を測るものですが、各次元（特徴量）のスケールの違いに敏感である点に注意が必要です（例：年齢と収入など単位の異なるデータ）。","クラスタ間の距離を決める方法として、以下の2つが紹介されています：","- Single Linkage（単一連結法）：2つのクラスタ間で最も近い2点の距離をクラスタ間距離とする。細長く連なったようなクラスタ構造になる傾向があります。","- Complete Linkage（完全連結法）：2つのクラスタ間で最も遠い2点の距離をクラスタ間距離とする。よりコンパクトで丸みを帯びたクラスタを作りやすくなります。","どのリンク関数を使うかは、データの特性や目的に応じて選択することが重要です。"],"originalSlideText":"Erzeugung von Clustern\\n• Hierarchical Agglomerative Clustering\\n  – Oft: Euklid’sche Distanz\\n  – Beachte: nicht invariant bezüglich Skalierung\\n\\n– Maße:\\n  – Single Linkage:\\n    t_SL(Cₖ, Cₗ) := min_{dᵢ ∈ Cₖ, dⱼ ∈ Cₗ} (||dᵢ − dⱼ||₂)\\n  – Complete Linkage:\\n    t_CL(Cₖ, Cₗ) := max_{dᵢ ∈ Cₖ, dⱼ ∈ Cₗ} (||dᵢ − dⱼ||₂)","explanationImage":"","questionImage":""},{"id":41,"questionDe":"(s.59) Wie lässt sich eine Clusterhierarchie darstellen und welche Unterschiede zeigen sich zwischen Linkage-Methoden?","questionJa":"クラスタの階層構造はどのように可視化され、リンク法（結合法）によってどのような違いが見られますか？","answerDe":["Die Clusterhierarchie wird als Dendrogramm dargestellt","Single Linkage führt zu langen, kettenartigen Clustern","Complete Linkage ergibt kompaktere Cluster"],"answerJa":["クラスタの階層構造はデンドログラム（樹形図）で表現される","Single Linkageでは細長く連なったクラスタ構造になる","Complete Linkageではよりコンパクトなクラスタが形成される"],"explanationDe":["Ein Dendrogramm ist eine Baumstruktur, die die Hierarchie der Clusterbildung bei einem hierarchischen Verfahren darstellt.","Auf der horizontalen Achse sieht man die Datenpunkte, auf der vertikalen Achse den Abstands- bzw. Ähnlichkeitswert, bei dem Cluster zusammengeführt wurden.","Im linken Bild sieht man das Dendrogramm für Single Linkage: Hier werden Objekte sukzessive zu einem Cluster verbunden, sobald ein einzelnes Paar nahe beieinander liegt – das führt zu langen, kettenartigen Clustern.","Im rechten Bild ist das Dendrogramm für Complete Linkage dargestellt: Hier bestimmt das maximal entfernte Punktpaar den Clusterabstand, was zu gleichmäßigeren, runderen Clustern führt.","Die Wahl der Linkage-Methode beeinflusst also maßgeblich die resultierende Clusterstruktur und deren Interpretierbarkeit."],"explanationJa":["デンドログラム（樹形図）は、階層的クラスタリングによって生成されたクラスタ階層を可視化する方法です。","横軸にはデータ点、縦軸にはクラスタが結合された距離や類似度の値が示され、どのクラスタがどの順で結びつけられたかが視覚的に把握できます。","左図はSingle Linkage（単一連結法）によるデンドログラムで、2点のうち最も近い距離に基づいてクラスタを形成するため、細長く連なったようなクラスタ（チェーン構造）が生まれやすくなります。","右図はComplete Linkage（完全連結法）によるデンドログラムで、最も遠い点同士の距離でクラスタ間距離を定義するため、全体によりバランスの取れた、丸みを帯びたクラスタになります。","このように、同じデータでも使用するリンク関数によってクラスタ構造が大きく変わるため、可視化して確認することは非常に重要です。"],"originalSlideText":"Erzeugung von Clustern\\n• Darstellung der erzeugtem Clusterhierarchie als Dendrogramm [Mar1986]","explanationImage":"lecture01/lecture05_ex11.png","questionImage":""}]');const m={class:"container py-4"},g={class:"text-center mb-5"},h={class:"display-5 fw-bold text-primary"},o={class:"fs-5 text-muted"},c={class:"text-dark"};var k={__name:"Lecture05Page",setup(e){const n=(0,a.lq)(),i=(0,s.KR)(""),k=(0,s.KR)(""),C=(0,s.KR)(""),b=(0,s.KR)([]);return(0,t.sV)(()=>{const e="lecture01",t=parseInt(n.name.split("_")[1]),r=l[e];i.value=r.title,C.value=t.toString().padStart(2,"0");const s=r.lectures.find(e=>e.number===t);k.value=s?s.title:"",b.value=d}),(e,n)=>((0,t.uX)(),(0,t.CE)("div",m,[(0,t.Lk)("div",g,[(0,t.Lk)("h1",h,(0,r.v_)(i.value),1),(0,t.Lk)("p",o,[(0,t.eW)(" Lecture "+(0,r.v_)(C.value)+": ",1),(0,t.Lk)("span",c,(0,r.v_)(k.value),1)]),n[0]||(n[0]=(0,t.Lk)("hr",{class:"w-25 mx-auto border-primary"},null,-1))]),((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(b.value,e=>((0,t.uX)(),(0,t.Wv)(u.A,{key:e.id,question:e},null,8,["question"]))),128))]))}};const C=k;var b=C}}]);
//# sourceMappingURL=5123.fada922b.js.map