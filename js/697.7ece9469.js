"use strict";(self["webpackChunkstudy_site"]=self["webpackChunkstudy_site"]||[]).push([[697],{495:function(e,n,i){i.d(n,{A:function(){return V}});var t=i(6768),r=i(4232),a=i(144);const s={class:"card mb-4 shadow-sm"},l={class:"card-body"},u={class:"card-title"},d={class:"text-muted fst-italic"},o={key:0},h=["src"],g={key:1,class:"mt-3"},m={class:"alert alert-success"},c={key:0},k={key:1},p={class:"alert alert-info mt-2"},D={key:0},b={key:1},w={class:"mt-3"},z={key:0},f={key:1},A={key:2},v={key:3},S={key:4},E=["src"],x={class:"mt-4"},M={class:"border rounded p-3 bg-white text-secondary",style:{"white-space":"pre-wrap","font-family":"inherit"}};var P={__name:"QuestionItem",props:{question:Object},setup(e){const n=(0,a.KR)(!1);return(i,a)=>((0,t.uX)(),(0,t.CE)("div",s,[(0,t.Lk)("div",l,[(0,t.Lk)("h5",u,"Q"+(0,r.v_)(e.question.id)+": "+(0,r.v_)(e.question.questionJa),1),(0,t.Lk)("p",d,"("+(0,r.v_)(e.question.questionDe)+")",1),e.question.questionImage?((0,t.uX)(),(0,t.CE)("div",o,[(0,t.Lk)("img",{src:`./images/${e.question.questionImage}`,class:"img-fluid rounded my-2 border",alt:"question image"},null,8,h)])):(0,t.Q3)("",!0),(0,t.Lk)("button",{class:"btn btn-outline-primary mt-2",onClick:a[0]||(a[0]=e=>n.value=!n.value)},(0,r.v_)(n.value?"Hide Answer":"Check Answer"),1),n.value?((0,t.uX)(),(0,t.CE)("div",g,[(0,t.Lk)("div",m,[a[1]||(a[1]=(0,t.Lk)("strong",null,"Antwort (De):",-1)),a[2]||(a[2]=(0,t.Lk)("br",null,null,-1)),Array.isArray(e.question.answerDe)?((0,t.uX)(),(0,t.CE)("ul",c,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.answerDe,(e,n)=>((0,t.uX)(),(0,t.CE)("li",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",k,(0,r.v_)(e.question.answerDe),1))]),(0,t.Lk)("div",p,[a[3]||(a[3]=(0,t.Lk)("strong",null,"Übersetzung (Ja):",-1)),a[4]||(a[4]=(0,t.Lk)("br",null,null,-1)),Array.isArray(e.question.answerJa)?((0,t.uX)(),(0,t.CE)("ul",D,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.answerJa,(e,n)=>((0,t.uX)(),(0,t.CE)("li",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",b,(0,r.v_)(e.question.answerJa),1))]),(0,t.Lk)("div",w,[a[6]||(a[6]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"Erklärung (De):",-1)),Array.isArray(e.question.explanationDe)?((0,t.uX)(),(0,t.CE)("div",z,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.explanationDe,(e,n)=>((0,t.uX)(),(0,t.CE)("p",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",f,(0,r.v_)(e.question.explanationDe),1)),a[7]||(a[7]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"解説 (Ja):",-1)),Array.isArray(e.question.explanationJa)?((0,t.uX)(),(0,t.CE)("div",A,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.explanationJa,(e,n)=>((0,t.uX)(),(0,t.CE)("p",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",v,(0,r.v_)(e.question.explanationJa),1)),e.question.explanationImage?((0,t.uX)(),(0,t.CE)("div",S,[(0,t.Lk)("img",{src:`./images/${e.question.explanationImage}`,class:"img-fluid rounded my-2 border",alt:"explanation image"},null,8,E)])):(0,t.Q3)("",!0),(0,t.Lk)("div",x,[a[5]||(a[5]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"原文（スライド抜粋）:",-1)),(0,t.Lk)("div",M,(0,r.v_)(e.question.originalSlideText),1)])])])):(0,t.Q3)("",!0)])]))}};const y=P;var V=y},7697:function(e,n,i){i.r(n),i.d(n,{default:function(){return D}});i(8111),i(116);var t=i(6768),r=i(4232),a=i(144),s=i(1387),l=i(495),u=i(3529),d=JSON.parse('[{"id":1,"questionDe":"Was umfasst das Thema „Dimensionsreduktion“ in dieser Vorlesung?","questionJa":"この講義における「次元削減」はどのような内容を含んでいますか？","answerDe":["Einleitung zum Thema","Faktorenanalyse","Hauptkomponentenanalyse (PCA)","Multi-Dimensional Scaling (MDS)","t-distributed Stochastic Neighbor Embedding (t-SNE)","Uniform Manifold Approximation and Projection (UMAP)"],"answerJa":["次元削減の導入（概要）","因子分析（Factor Analysis）","主成分分析（PCA）","多次元尺度構成法（MDS）","t-SNE（t分布型確率的近傍埋め込み）","UMAP（一様多様体近似と射影）"],"explanationDe":["Das Thema „Dimensionsreduktion“ behandelt Methoden, um hochdimensionale Daten in eine niedrigere Dimension zu überführen – oft zur Visualisierung oder zur Vorverarbeitung für maschinelles Lernen.","In der Vorlesung werden sowohl klassische lineare Verfahren (z. B. PCA, Factor Analysis) als auch moderne, nichtlineare Techniken (z. B. t-SNE, UMAP) vorgestellt.","Diese Methoden helfen, die Struktur der Daten besser zu erkennen, Überfitting zu vermeiden oder Rechenaufwand zu reduzieren.","Beispielsweise erlaubt PCA eine Interpretation der Hauptachsen der Varianz, während t-SNE oder UMAP eher die lokalen Nachbarschaften im Datenraum erhalten."],"explanationJa":["「次元削減」とは、高次元データをより少ない次元に変換する手法の総称で、データの可視化や機械学習の前処理によく使われます。","この講義では、線形な古典的手法（例：PCAや因子分析）と、非線形で近年注目されている手法（例：t-SNEやUMAP）の両方が取り上げられます。","次元削減は、データの本質的な構造を理解しやすくしたり、過学習を防いだり、計算量を削減したりするのに役立ちます。","例えば、PCAは分散の大きい方向を基に軸を定めるのに対し、t-SNEやUMAPはデータの局所的な関係性（近さ）を保ちつつ、2次元や3次元に落とし込むのが得意です。"],"originalSlideText":"Dimensionsreduktion\\n- Einleitung\\n- Factor Analysis\\n- Principal Component Analysis (PCA)\\n- Multi-Dimensional Scaling (MDS)\\n- T-distributed stochastic neighborhood embedding (t-SNE)\\n- Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)","explanationImage":"","questionImage":""},{"id":2,"questionDe":"(s.3) Welches Problem entsteht bei vielen Attributen und Datenpunkten?","questionJa":"多数の属性やデータ点があると、どのような問題が生じますか？","answerDe":["Bei einer Tabelle mit vielen Datenpunkten und Attributen entsteht das Problem, dass mehr Daten vorhanden sind als Darstellungsfläche."],"answerJa":["多数のデータ点や属性を持つ表では、表示できる領域よりもデータ量のほうが多くなってしまうという問題が発生します。"],"explanationDe":["In datengetriebenen Anwendungen, etwa bei Sensor- oder Bilddaten, entstehen häufig sehr große Tabellen mit vielen Spalten (Attributen) und Zeilen (Datenpunkten).","Ein zentrales Problem besteht darin, dass die vorhandene Darstellungsfläche – zum Beispiel ein Bildschirm oder Diagramm – nicht ausreicht, um die gesamte Datenstruktur verständlich darzustellen.","Dies erschwert die Analyse, insbesondere die visuelle Interpretation oder die Anwendung von Algorithmen, die in niedriger Dimensionalität effektiver arbeiten."],"explanationJa":["センサーや画像データなどの実用的なデータでは、多くの列（属性）と行（データ点）からなる大規模な表が生成されます。","このとき、「画面」や「図」といった表示領域が足りず、すべてのデータ構造を視覚的に表現するのが難しくなります。","この問題により、特に可視化や、低次元で効果的に働くアルゴリズムの適用が困難になります。"],"originalSlideText":"– Ausgangssituation\\n  – Tabelle mit n Datenpunkten und m Attributen\\n– Problem\\n  – Mehr Daten als Darstellungsfläche","explanationImage":"","questionImage":"lecture01/lecture06_q01.png"},{"id":3,"questionDe":"(s.3) Welche Lösungsansätze gibt es zur Datenreduktion?","questionJa":"データ量の削減にはどのような解決法がありますか？","answerDe":["Zur Lösung kann man entweder eine Datenauswahl durch Einschränkung der Attribute oder Attributwerte treffen oder eine Projektion zur Dimensionsreduktion durchführen."],"answerJa":["この問題への対処法としては、属性や属性値の制限によるデータの選択、または次元削減を目的とした射影（プロジェクション）があります。"],"explanationDe":["Ein Ansatz zur Datenreduktion ist die Datenauswahl. Dabei werden entweder nur bestimmte Attribute berücksichtigt (z. B. jene mit hoher Relevanz) oder Attributwerte eingeschränkt (z. B. durch Filterung).","Ein anderer Ansatz ist die Projektion: Hierbei werden die Daten mathematisch auf einen niedrigeren dimensionalen Raum abgebildet, wobei wichtige Strukturen erhalten bleiben.","Die Dimensionsreduktion (engl. Dimensionality Reduction) ist ein Spezialfall dieser Projektion und wird häufig eingesetzt, um Daten visuell darzustellen oder Rechenaufwand zu reduzieren.","Ein Beispiel: Bei der PCA werden die Hauptachsen der größten Varianz genutzt, um die Dimension der Daten zu verringern."],"explanationJa":["データ量を削減する方法のひとつは「データ選択」で、重要な属性だけを選んだり、属性値の範囲を制限したりします（例：フィルター処理）。","もうひとつは「射影（プロジェクション）」で、データを数学的に低次元の空間に写し、重要な構造を保ちながら次元を減らします。","この射影の一種が「次元削減（Dimensionality Reduction）」であり、視覚化のためや、計算負荷の軽減などによく用いられます。","例としてPCA（主成分分析）では、最も分散の大きな軸に沿ってデータを写すことで、次元を縮小します。"],"originalSlideText":"– Lösung\\n  – Datenauswahl\\n    – Einschränkung der Attribute\\n    – Einschränkung der Attributwerte\\n  – Projektion / Datenreduktion\\n    – Dimensionsreduktion („Dimensionality Reduction“)\\n  – Clustering","explanationImage":"","questionImage":""},{"id":4,"questionDe":"(s.4) Was ist unter Projektion im Kontext der Datenvisualisierung zu verstehen und welche Probleme entstehen dabei?","questionJa":"データの可視化におけるプロジェクションとは何ですか？また、どのような問題がありますか？","answerDe":["Bei der Projektion wählt man typischerweise 2 oder 3 Attribute aus, um Daten visuell darzustellen.","Dies kann zu vielen gleichen Datenpunkten führen (Overplotting).","Es stellt sich die Frage, welche Attribute ausgewählt werden und ob die Verteilung der Werte berücksichtigt wird."],"answerJa":["プロジェクションでは、通常2つか3つの属性を選び、データを視覚的に表示します。","しかし、多くのデータ点が同じ位置に重なってしまう（オーバープロッティング）という問題が発生します。","また、どの属性を選ぶべきか、値の分布が考慮されているかなどの課題もあります。"],"explanationDe":["In der Datenvisualisierung beschränkt man sich oft auf 2 oder 3 Dimensionen, da man diese einfach auf einem Bildschirm oder Plot darstellen kann.","Dazu wählt man einige Attribute aus, z. B. Alter und Einkommen, und stellt die Datenpunkte in einem 2D-Scatterplot dar.","Ein Problem ist das sogenannte Overplotting: Viele Datenpunkte haben in den ausgewählten Dimensionen identische Werte und überdecken sich visuell.","Ein weiteres Problem ist die Attributwahl: Wenn man wichtige Merkmale nicht berücksichtigt, geht relevante Information verloren.","Außerdem werden bei dieser einfachen Auswahl keine Korrelationen oder Verteilungen der Daten beachtet – man nutzt also nur einen sehr begrenzten Ausschnitt der gesamten Datenstruktur."],"explanationJa":["データの可視化では、通常2次元または3次元で表示する必要があるため、いくつかの属性を選び出して描画します。","たとえば、「年齢」と「収入」などをx軸とy軸にして散布図を描くといった形です。","しかし、この方法では同じ属性値を持つデータ点が重なりやすく、視覚的に区別できなくなる『オーバープロッティング』が発生します。","また、どの属性を選ぶかによって表現される情報が大きく変わり、重要な特徴を見逃してしまう可能性もあります。","さらに、選ばれた属性間の相関や、データの分布が考慮されていないため、元のデータ構造の一部しか反映されません。"],"originalSlideText":"– Projektion\\n  – Wähle d ∈ {2,3} Attribute\\n  – Führt in der Regel zu vielen „gleichen“ Datenpunkten → Overplotting\\n  – Probleme\\n    – Welche Attribute werden betrachtet?\\n    – Berücksichtigt die Werte und die Verteilung der Werte der Datenpunkte nicht","explanationImage":"","questionImage":""},{"id":5,"questionDe":"(s.4) Was versteht man unter Dimensionsreduktion und welche Varianten gibt es?","questionJa":"次元削減とは何ですか？また、その代表的な種類にはどのようなものがありますか？","answerDe":["Bei der Dimensionsreduktion ersetzt man die ursprünglichen Attribute durch eine kleinere Menge neuer Attribute.","Oft gilt: d ≪ m und d ∈ {2, 3}.","Es gibt verschiedene Varianten: Projektion (D ⊆ A), Berechnung neuer Attribute (D ∩ A = ∅), oder eine Kombination."],"answerJa":["次元削減では、もとの属性をより少ない数の新しい属性に置き換えます。","通常、d ≪ m で、d は2や3など小さい値が多いです。","方法には、プロジェクション（D ⊆ A）、新しい属性の生成（D ∩ A = ∅）、およびその両者の組み合わせがあります。"],"explanationDe":["Dimensionsreduktion ist eine Methode, um die Anzahl der Merkmale (Attribute) eines Datensatzes zu verringern, ohne die wesentliche Struktur der Daten zu verlieren.","Anstatt einfach einige existierende Attribute zu wählen (wie bei Projektion), berechnet man oft neue Merkmale, die die ursprüngliche Information möglichst gut zusammenfassen.","Beispiel: Bei der Hauptkomponentenanalyse (PCA) werden neue Achsen (Hauptkomponenten) so gewählt, dass sie die Varianz der Daten maximieren.","Man unterscheidet zwischen drei Varianten:\\n1. Projektion: man wählt eine Teilmenge der Originalattribute (D ⊆ A).\\n2. Transformation: man bildet völlig neue Attribute, berechnet aus den alten (D ∩ A = ∅).\\n3. Kombination: man mischt beide Ansätze.","Diese Methoden helfen, Daten übersichtlicher darzustellen und Overfitting oder Rechenlast zu reduzieren."],"explanationJa":["次元削減は、データの重要な構造を保ちつつ、属性（特徴量）の数を減らす方法です。","単に元の属性の一部を選ぶのではなく、元の情報を集約して、新たな属性を計算することが多いです。","たとえば主成分分析（PCA）では、データの分散を最もよく表す軸（主成分）を見つけ、それを新しい特徴量として使います。","方法には次の3つがあります。\\n1. プロジェクション：元の属性の一部をそのまま使う（D ⊆ A）。\\n2. 変換：元の属性から全く新しい属性を作る（D ∩ A = ∅）。\\n3. 両者の組み合わせ。","このような方法は、データの可視化や処理の効率化、過学習の防止に役立ちます。"],"originalSlideText":"– Dimensionsreduktion\\n  – Ersetze gegebene Attribute A = {A₁, ..., Aₘ} durch eine Menge von weniger Attributen D = {D₁, ..., D_d}, d ≪ m\\n    – Meist: d ∈ {2,3}\\n  – Alternativen:\\n    – D ⊆ A: entspricht Projektion\\n    – D ∩ A = ∅\\n      – Die neuen Attribute werden aus den alten berechnet\\n    – Kombination aus beidem","explanationImage":"lecture01/lecture06_ex01.png","questionImage":""},{"id":6,"questionDe":"(s.5) Welche Verfahren zur Dimensionsreduktion werden vorgestellt?","questionJa":"どのような次元削減の手法が紹介されていますか？","answerDe":["Vorgestellt werden: Factor Analysis, Principal Component Analysis (PCA), Multi-Dimensional Scaling (MDS).","Weitere Möglichkeiten: Self-Organizing Maps, visuelle Metaphern zur Anordnung von Dimensionen."],"answerJa":["紹介された手法は以下の通りです：因子分析（Factor Analysis）、主成分分析（PCA）、多次元尺度構成法（MDS）","その他の方法として、自己組織化マップ（SOM）や視覚的なメタファーによる次元の配置などもあります。"],"explanationDe":["In der Dimensionsreduktion gibt es verschiedene Ansätze, um die Anzahl der Merkmale zu reduzieren, ohne zu viel Information zu verlieren.","Klassische Verfahren sind:\\n- **Factor Analysis**: Modelliert beobachtete Variablen als lineare Kombinationen latenter (nicht direkt beobachtbarer) Faktoren.\\n- **PCA** (Principal Component Analysis): Findet neue Achsen (Hauptkomponenten), die die meiste Varianz erklären.\\n- **MDS** (Multi-Dimensional Scaling): Platziert Objekte so im Raum, dass die Distanzen möglichst den gegebenen Unterschieden entsprechen.","Zusätzliche Optionen sind:\\n- **Self-Organizing Maps (SOMs)**: Neuronale Netze, die eine nichtlineare Projektion liefern.\\n- **Visuelle Metaphern**: Grafische Ansätze, die dem Nutzer helfen, die Struktur der Daten intuitiv zu erfassen."],"explanationJa":["次元削減にはさまざまなアプローチがあり、情報をなるべく失わずにデータの特徴量を減らすことを目的とします。","代表的な手法は次のとおりです：\\n- **因子分析（Factor Analysis）**：観測された変数を、観測できない「因子」の線形結合としてモデル化します。\\n- **主成分分析（PCA）**：データの分散を最も説明する新しい軸（主成分）を見つけ出します。\\n- **多次元尺度構成法（MDS）**：データ間の距離関係を保つように、低次元空間に配置します。","その他の方法には：\\n- **自己組織化マップ（SOM）**：ニューラルネットワークを用いた非線形なマッピング手法です。\\n- **視覚的メタファー**：データの構造を視覚的に理解しやすくするための図示的アプローチです。"],"originalSlideText":"– Verfahren:\\n  – Factor Analysis\\n  – Principal Component Analysis (PCA)\\n  – Multi-Dimensional Scaling (MDS)\\n– Andere Möglichkeiten\\n  – Self-Organizing Maps\\n  – Visuelle Metaphern, um die Dimensionen anzuordnen","explanationImage":"","questionImage":""},{"id":7,"questionDe":"(s.5) Welche allgemeinen Hinweise gelten für alle Verfahren der Dimensionsreduktion?","questionJa":"次元削減のすべての手法に共通する注意点は何ですか？","answerDe":["Eine Analyse kann keine Informationen hinzufügen, die nicht vorhanden sind.","Es gilt das Prinzip: „Garbage in, garbage out“.","Faktoren werden immer gefunden – daher ist Domänenwissen zur Bewertung notwendig."],"answerJa":["分析では、元々存在しない情報を新たに加えることはできません。","『Garbage in, garbage out（ゴミを入れればゴミが出る）』という原則が当てはまります。","手法は常に何らかの『因子』を見つけますが、それが有効かどうかはドメイン知識で判断する必要があります。"],"explanationDe":["Ein wichtiger Hinweis bei allen Verfahren zur Dimensionsreduktion: Sie arbeiten nur mit den vorliegenden Daten.","Das bedeutet: Wenn z. B. ein wichtiges Merkmal nicht erhoben wurde, kann es auch durch keine mathematische Methode ersetzt oder ergänzt werden.","Der Spruch „Garbage in, garbage out“ warnt davor, dass schlechte oder irrelevante Daten zwangsläufig zu schlechten Ergebnissen führen.","Zudem identifizieren Verfahren wie PCA oder Factor Analysis stets bestimmte Strukturen oder Faktoren – selbst wenn diese möglicherweise keinen echten Informationsgehalt besitzen.","Deshalb ist es entscheidend, zusätzliches Wissen über den Anwendungsbereich (Domänenwissen) zu haben, um zu beurteilen, ob die gefundenen Muster sinnvoll sind."],"explanationJa":["次元削減を含むすべての分析手法は、与えられたデータの範囲内でしか処理を行えません。","つまり、もし重要な情報（例えばある属性）がそもそも収集されていなければ、それを後からどんな分析を使っても補うことはできません。","このような状況を表す言葉が「Garbage in, garbage out（ゴミを入れればゴミが出る）」です。","また、PCAや因子分析などの手法は、必ず何らかの構造や因子を抽出しますが、それが本当に意味あるものかどうかは自動的には判断できません。","したがって、分析結果の妥当性を判断するためには、データの背景知識や専門的な知見（ドメイン知識）が不可欠です。"],"originalSlideText":"– Achtung!\\n– Für alle Verfahren gilt:\\n  – Information, die nicht erhoben wurde, wird durch eine Analyse nicht hinzugefügt\\n  – Beware of „garbage in, garbage out“\\n    Vorsicht vor „Müll hinein, Müll heraus“:\\n    – Das Verfahren findet immer Faktoren\\n    – Hintergrund-/Domänen-Wissen ist notwendig, um zu entscheiden, ob die Faktoren einen Wert haben","explanationImage":"","questionImage":""},{"id":7,"questionDe":"(s.5) Nennen Sie vier wichtige Hinweise („Achtung!“), die für alle Verfahren der Dimensionsreduktion gelten.","questionJa":"次元削減のすべての手法に共通する重要な注意点を4つ挙げなさい。","answerDe":["1. Eine Analyse kann keine Informationen hinzufügen, die nicht erhoben wurden.","2. Das Prinzip „Garbage in, garbage out“ gilt: Schlechte Daten führen zu schlechten Ergebnissen.","3. Jedes Verfahren liefert mathematisch immer Faktoren, auch wenn diese bedeutungslos sein können.","4. Um die gefundenen Faktoren sinnvoll zu interpretieren, ist Domänenwissen erforderlich."],"answerJa":["1. 収集されていない情報は、分析であとから追加することはできない。","2. 「Garbage in, garbage out（ゴミを入れればゴミが出る）」の原則が当てはまる：質の低いデータでは、分析結果も信用できない。","3. 数理的な処理として、分析手法は必ず因子（パターン）を見つけ出すが、それが意味のあるものとは限らない。","4. 見つかった因子を正しく解釈するには、対象分野の専門知識（ドメイン知識）が必要である。"],"explanationDe":["**1. Keine Informationen hinzufügbar:** Wenn bestimmte Merkmale (wie Alter oder Einkommen) gar nicht erhoben wurden, können sie durch Analyseverfahren auch nicht ersetzt oder erschlossen werden.","**2. Garbage in, garbage out:** Wenn z. B. fehlerhafte, unvollständige oder unskalierte Daten verwendet werden, wird das Ergebnis entsprechend unzuverlässig. Beispiel: Bei uneinheitlichen Maßeinheiten (cm vs. m) liefert PCA irreführende Ergebnisse.","**3. Verfahren liefern immer Faktoren:** Algorithmen wie PCA oder Factor Analysis produzieren zwangsläufig mathematische Faktoren – auch dann, wenn keine sinnvolle Struktur in den Daten steckt. Dies kann zu falscher Interpretation führen, wenn man den Ergebnissen zu viel Bedeutung beimisst.","**4. Bedeutung durch Domänenwissen:** Nur mit Fachwissen lässt sich entscheiden, ob ein Faktor tatsächlich etwas wie \'Kundenzufriedenheit\' widerspiegelt oder nur ein statistisches Artefakt ist."],"explanationJa":["**1. 情報は後から足せない：** たとえば「年齢」や「収入」といった属性が最初からデータに含まれていない場合、分析手法でそれを補うことはできません。","**2. Garbage in, garbage out（ゴミを入れればゴミが出る）：** 測定ミスや不完全なデータ、単位が統一されていないデータを用いると、主成分分析なども意味のある結果を返しません。例：身長をcmとmで混在させたままPCAを行うと、主成分の方向が歪みます。","**3. 手法は必ず「因子」を出力する：** PCAや因子分析などの数理手法は、たとえデータに意味のある構造がなくても、必ず数学的に「因子」を作り出します。これが過信につながる危険があります。","**4. 専門知識が必要：** たとえば「第1主成分が顧客満足度に対応している」かどうかを判断するには、統計ではなく業務や対象領域の知識が不可欠です。そうでなければ、意味のない構造をあたかも意味があるように見なしてしまいます。"],"originalSlideText":"- Achtung!\\n- Für alle Verfahren gilt:\\n  – Information, die nicht erhoben wurde, wird durch eine Analyse nicht hinzugefügt\\n  – Beware of „garbage in, garbage out“\\n    Vorsicht vor „Müll hinein, Müll heraus“:\\n    – Das Verfahren findet immer Faktoren\\n    – Hintergrund-/Domänen-Wissen ist notwendig, um zu entscheiden, ob die Faktoren einen Wert haben","explanationImage":"","questionImage":""},{"id":8,"questionDe":"(s.6) Erklären Sie die zwei Typen der Faktorenanalyse und benennen Sie ihre jeweiligen Merkmale.","questionJa":"因子分析の2つのタイプ（Typ Q と Typ R）について説明し、それぞれの特徴を挙げなさい。","answerDe":["Typ Q fasst Datenpunkte zusammen, ist rechenaufwändig und wird oft durch Clustering ersetzt.","Typ R identifiziert verborgene Dimensionen, also Gruppen korrelierter Variablen."],"answerJa":["Typ Q はデータポイントをまとめる手法で、計算コストが高く、代わりにクラスタリングが使われることが多い。","Typ R は相関のある変数をまとめ、隠れた次元（因子）を見つける手法である。"],"explanationDe":["Die Faktorenanalyse wird in zwei Varianten unterteilt:","**Typ Q** betrachtet die Zeilen der Datenmatrix, also einzelne Datenpunkte. Ziel ist es, Gruppen von ähnlichen Individuen zu bilden. Da dies jedoch oft rechenintensiv ist, wird stattdessen häufig Clustering verwendet.","**Typ R** hingegen analysiert die Spalten der Datenmatrix, also die Variablen. Ziel ist es, sogenannte \'verborgene Dimensionen\' (Faktoren) zu finden, die mehrere korrelierte Variablen zusammenfassen. Zum Beispiel könnte ein \'sozialer Faktor\' die Variablen „Freundschaft“, „Geselligkeit“ und „Kommunikation“ zusammenfassen.","Typ R ist der klassische Anwendungsfall der Faktorenanalyse in der Psychologie, Soziologie und Marktforschung."],"explanationJa":["因子分析（Factor Analysis）には2つのタイプがあります：","**Typ Q** はデータ行（つまり個々のデータ点）を対象とし、似た個体（データポイント）をまとめようとします。たとえば、似たような購買行動をする顧客をグループ化するような場合です。ただし、計算量が大きいため、実務では代わりにクラスタリングが使われることが多いです。","**Typ R** は変数（列）を対象とし、相関のある変数をまとめて、より少ない“因子（潜在変数）”に置き換えます。たとえば、「友人の数」「SNSでのやりとり」「会話の頻度」などがすべて「社交性」という1つの因子で説明できる場合です。","Typ R は心理学やマーケティング調査などでよく使われる方法です。"],"originalSlideText":"FACTOR ANALYSIS\\n– Arten\\n  – Typ Q\\n    – Zusammenfassung der Datenpunkte\\n    – Aufwändige Berechnung\\n    – Meist wird stattdessen Clustering verwendet\\n  – Typ R\\n    – Finde „verborgene Dimensionen“ (Gruppen von korrelierten Variablen)\\n– Ziele:\\n  – Identifikation von Strukturen\\n  – Zusammenfassung der Daten\\n  – Reduktion der Daten\\n– Eigenschaften:\\n  – Gruppen sind disjunkt\\n  – Gruppen können von ihren Mitgliedern repräsentiert werden","explanationImage":"","questionImage":""},{"id":9,"questionDe":"(s.6) Nennen Sie drei typische Ziele der Faktorenanalyse.","questionJa":"因子分析の代表的な目的を3つ挙げなさい。","answerDe":["Identifikation von Strukturen","Zusammenfassung der Daten","Reduktion der Daten"],"answerJa":["構造（パターン）の識別","データの要約","データの次元削減"],"explanationDe":["Ein zentrales Ziel der Faktorenanalyse ist es, in den Daten zugrunde liegende Strukturen zu erkennen – etwa, ob bestimmte Variablen in Gruppen zusammengehören.","Sie dient außerdem dazu, große Datenmengen zu verdichten und durch wenige aussagekräftige Faktoren zu beschreiben.","Dies führt gleichzeitig zu einer Reduktion der Anzahl an Variablen und damit zu besserer Interpretierbarkeit und Effizienz bei der Analyse."],"explanationJa":["因子分析の主な目的は、観測された変数の背後にある構造（パターン）を明らかにすることです。たとえば、変数同士がどのように関係し、どのグループに属するかなどを探ります。","また、情報の要約手法としても使われ、多くの変数を少数の因子に集約することで、データの解釈が容易になります。","その結果、次元の削減が可能となり、計算の効率化や視覚的な理解の向上にもつながります。"],"originalSlideText":"– Ziele:\\n  – Identifikation von Strukturen\\n  – Zusammenfassung der Daten\\n  – Reduktion der Daten"},{"id":10,"questionDe":"(s.6) Welche zwei Eigenschaften zeichnen Gruppen in der Faktorenanalyse aus?","questionJa":"因子分析におけるグループの特徴を2つ挙げなさい。","answerDe":["Gruppen sind disjunkt.","Gruppen können von ihren Mitgliedern repräsentiert werden."],"answerJa":["グループは互いに重複しない（排他的）","グループはその構成要素によって代表される"],"explanationDe":["Disjunkt bedeutet, dass jede Variable oder jedes Objekt nur zu einer Gruppe gehört – es gibt keine Überschneidungen.","Zudem lassen sich die Gruppen durch typische Merkmalsausprägungen ihrer Mitglieder charakterisieren, was die Interpretation erleichtert."],"explanationJa":["『排他的（disjunkt）』とは、1つの変数やオブジェクトが複数のグループにまたがることがないことを意味します。","また、グループはその構成メンバー（変数やデータ点）の性質によって代表されるため、解釈しやすくなるという特徴もあります。"],"originalSlideText":"– Eigenschaften:\\n  – Gruppen sind disjunkt\\n  – Gruppen können von ihren Mitgliedern repräsentiert werden"},{"id":11,"questionDe":"(s.7) Vergleichen Sie anhand der dargestellten Grafik die Gruppierungsergebnisse von Typ Q Faktor-Analyse und Clustering.","questionJa":"図をもとに、Typ Q 因子分析とクラスタリングによるグループ分けの違いを比較しなさい。","answerDe":["Bei der Typ Q Faktor-Analyse gehören A und C sowie B und D zur gleichen Gruppe.","Beim Clustering gehören A und B sowie C und D zusammen."],"answerJa":["Typ Q 因子分析では、A と C、B と D がそれぞれ同じグループに属している。","一方、クラスタリングでは、A と B、C と D が同じグループに分類されている。"],"explanationDe":["Die Grafik zeigt vier Objekte (A–D) mit drei Variablen (V1–V3). Die Liniengrafik verdeutlicht die Ähnlichkeit der Wertverläufe.","Bei der Typ Q Faktor-Analyse werden Objekte gruppiert, die über alle Variablen hinweg ähnliche Verläufe zeigen. A und C folgen einem ähnlichen Muster, obwohl ihre absoluten Werte unterschiedlich sind – deshalb werden sie gruppiert. Ebenso verhalten sich B und D ähnlich in ihrer Kurvenform.","Beim Clustering hingegen werden Objekte eher nach absoluter Nähe im Merkmalsraum gruppiert. A und B haben ähnliche hohe Werte, daher werden sie zusammen gruppiert. C und D liegen beide in einem niedrigeren Wertebereich.","Die beiden Methoden erfassen also unterschiedliche Arten von Ähnlichkeit: Faktor-Analyse betrachtet Formmuster, Clustering absolute Nähe."],"explanationJa":["この図には、3つの変数 (V1〜V3) に対して4つのデータ（A〜D）の値が示されており、線グラフがそれぞれの傾向を視覚化しています。","Typ Q の因子分析では、全体の形（変数間の推移の傾向）に注目します。たとえば、AとCは全体の数値は異なるものの、増減のパターンが似ており、同じ因子とみなされます。同様に、BとDも同じ形状の傾向を持つため、一緒に分類されます。","一方クラスタリングは、値の絶対的な近さに基づいてグループ化を行います。AとBは3変数すべての値が高く、CとDは低いため、前者と後者でグループが分かれます。","つまり、因子分析（Typ Q）は『傾向の類似性』、クラスタリングは『数値の近さ』をもとにしているという違いがあります。"],"originalSlideText":"FACTOR ANALYSIS\\n– Typ Q Faktor-Analyse – Clustering\\n– Faktor-Analyse\\n  – Gruppe 1: A, C\\n  – Gruppe 2: B, D\\n– Clustering\\n  – Gruppe 1: A, B\\n  – Gruppe 2: C, D","explanationImage":"","questionImage":"lecture01/lecture06_q02.png"},{"id":12,"questionDe":"(s.8) Welche Voraussetzungen müssen für die Anwendung der Faktorenanalyse erfüllt sein? Nennen Sie mindestens 3.","questionJa":"因子分析を適用するために必要な前提条件を3つ以上挙げて説明しなさい。","answerDe":["Die Anzahl der Datenpunkte sollte deutlich größer sein als die Anzahl der Variablen.","Es sollten möglichst wenige Variablen verwendet werden.","Es sollten möglichst viele Datenpunkte verwendet werden.","Die Eignung der Daten kann mit Bartletts Test auf Sphärizität überprüft werden."],"answerJa":["データ点の数が変数の数よりも十分に多いこと。","使用する変数の数はできるだけ少なくすること。","できるだけ多くのデータ点を使用すること。","バートレットの球面性検定（Bartlett\'s Test）により、因子分析に適しているかどうかを確認すること。"],"explanationDe":["Für eine stabile Faktorenanalyse muss die Anzahl der Datenpunkte deutlich größer als die der Variablen sein – mindestens 50–100 Datenpunkte werden empfohlen.","In der Praxis wird oft die Regel verwendet: 5–10 Datenpunkte pro Variable oder sogar 20 Datenpunkte pro Variable.","Bartlett\'s Test auf Sphärizität prüft, ob die Korrelationsmatrix signifikant von der Einheitsmatrix abweicht.","Die Nullhypothese besagt, dass die Korrelationsmatrix gleich der Einheitsmatrix ist – d.h. keine Korrelationen zwischen den Variablen.","Wenn die Nullhypothese bei p < 0,05 abgelehnt wird, ist eine Faktorenanalyse sinnvoll, da es ausreichende Korrelationen gibt.","Voraussetzung für den Test ist multivariate Normalverteilung."],"explanationJa":["因子分析を正しく行うためには、いくつかの前提条件を満たす必要があります。","まず、データ点（サンプル）の数が変数の数よりも十分に多くなければなりません。一般には50〜100件のサンプル、または1変数あたり5〜10件（理想的には20件）のデータが必要とされます。","また、使用する変数の数はできるだけ絞るべきです。変数が多いと結果が不安定になる可能性があります。","バートレットの球面性検定（Bartlett\'s Test）は、変数間に有意な相関があるか（因子分析が妥当か）を判断するための統計的検定です。","この検定の帰無仮説は『相関行列が単位行列と同じである』というもので、p値が0.05未満なら帰無仮説は棄却され、因子分析を行う意義があるとされます。","この検定を使うには、多変量正規分布しているという前提も必要です。"],"originalSlideText":"FACTOR ANALYSIS\\n– Anzahl der Datenpunkte ≫ Anzahl der Variablen\\n  – Minimum 50 – 100 Datenpunkte\\n  – #Datenpunkte = 5–10 · #Variablen\\n  – #Datenpunkte = 20 · #Variablen\\n– Verwendung von möglichst wenig Variablen\\n– Verwendung von möglichst vielen Datenpunkten\\n– Anwendbarkeit\\n  – Bartlett‘s Test auf Sphärizität\\n    – Nullhypothese: die Korrelationsmatrix ist gleich der Einheitsmatrix\\n    – Signifikanz: p < 0,05\\n    – Nullhypothese wird abgelehnt → Faktorenanalyse möglich\\n    – Voraussetzung: multivariate Normalverteilung\\n    – χ² = − (n − 1 − (2·m+5)/6) · log(det(R))\\n      R: Korrelationsmatrix","explanationImage":"","questionImage":""},{"id":13,"questionDe":"Was prüft der Bartlett-Test auf Sphärizität und wann ist eine Faktorenanalyse zulässig?","questionJa":"Bartlettの球面性検定では何を検定しますか？また、どのような場合に因子分析を行うことができますか？","answerDe":["Der Bartlett-Test prüft, ob die Korrelationsmatrix einer Einheitsmatrix entspricht.","Wenn die Nullhypothese abgelehnt wird (p < 0,05), kann eine Faktorenanalyse durchgeführt werden."],"answerJa":["Bartlett検定は、相関行列が単位行列に等しいかどうかを検定します。","p値が0.05未満で帰無仮説が棄却された場合、因子分析を行うことができます。"],"explanationDe":["In der Faktorenanalyse ist es wichtig, dass zwischen den Variablen genügend Korrelationen bestehen, damit gemeinsame Faktoren identifiziert werden können.","Der Bartlett-Test auf Sphärizität prüft genau diese Voraussetzung. Er testet die Nullhypothese, dass die Korrelationsmatrix einer Einheitsmatrix entspricht – also keine Korrelationen zwischen den Variablen vorliegen.","Ein signifikantes Testergebnis (p < 0,05) bedeutet, dass die Korrelationen ausreichend stark sind, um eine Faktorenanalyse durchzuführen.","Die Teststatistik basiert auf der Determinante der Korrelationsmatrix und folgt einer Chi-Quadrat-Verteilung. Voraussetzung ist eine multivariate Normalverteilung der Daten.","Beispiel: Wenn viele Variablen in einem Fragebogen ähnlich beantwortet werden, deutet dies auf gemeinsame latente Faktoren hin – der Bartlett-Test kann zeigen, ob diese Annahme gerechtfertigt ist."],"explanationJa":["因子分析では、観測された変数群の背後に共通する因子（潜在変数）があると仮定します。そのため、変数同士にある程度の相関が存在することが前提条件です。","Bartlettの球面性検定は、この前提が成り立っているかを確認するための検定で、相関行列が単位行列（すべての変数が無相関）と等しいかを検証します。","p値が0.05未満であれば、帰無仮説は棄却され、変数間に有意な相関があると判断され、因子分析が可能になります。","この検定は相関行列の行列式（determinant）を利用し、カイ二乗分布に従う検定統計量を用います。前提条件として、データが多変量正規分布に従う必要があります。","例：アンケート調査で「価格」「価値」「満足度」などの質問項目の回答に似た傾向があれば、それらに共通する『購買意欲』のような因子があると仮定され、この検定で因子分析の適用が妥当かを判断できます。"],"originalSlideText":"Bartlett‘s Test auf Sphärizität\\n- Nullhypothese: die Korrelationsmatrix ist gleich der Einheitsmatrix\\n- Signifikanz: p < 0,05\\n- Nullhypothese wird abgelehnt → Faktorenanalyse möglich\\n- Voraussetzung: multivariate Normalverteilung\\n- χ² = − (n − 1 − 2·m+5 / 6) log(det(R))\\n- R: Korrelationsmatrix","explanationImage":"lecture01/lecture06_ex02.png","questionImage":""},{"id":14,"questionDe":"Was misst das Measure of Sampling Adequacy (MSA) und wie wird es interpretiert?","questionJa":"MSA（サンプリング適性尺度）とは何を測定する指標で、どのように評価されますか？","answerDe":["MSA misst, ob eine Variable für die Faktorenanalyse geeignet ist.","Ein Wert kleiner als 0,5 bedeutet: ungeeignet.","Ein Wert zwischen 0,6 und 0,8: brauchbar.","Ein Wert größer als 0,8: gut geeignet."],"answerJa":["MSAは変数が因子分析に適しているかどうかを示す指標です。","MSAが0.5未満の場合：因子分析に不適切。","0.6から0.8の間：使用可能な変数。","0.8より大きい：因子分析に非常に適した変数。"],"explanationDe":["Das Measure of Sampling Adequacy (MSA) zeigt, wie stark eine Variable mit anderen Variablen zusammenhängt und ob sie für eine gemeinsame Faktorenstruktur geeignet ist.","Eine hohe MSA bedeutet, dass die Variable stark mit anderen korreliert und nur geringe partielle Korrelationen aufweist.","Die MSA für eine Variable j wird folgendermaßen berechnet:","MSA_j = Summe der quadrierten Korrelationen r_jk mit anderen Variablen geteilt durch (Summe der quadrierten Korrelationen r_jk plus Summe der quadrierten partiellen Korrelationen p_jk).","Sind die partiellen Korrelationen hoch, deutet das auf eine geringe Eignung für Faktorenanalyse hin.","Beispiel: Eine Variable, die mit vielen anderen stark korreliert, aber nach Herausrechnen der restlichen Einflüsse (durch partielle Korrelation) weiterhin starke Zusammenhänge zeigt, erhält eine hohe MSA und ist gut geeignet."],"explanationJa":["MSA（サンプリング適性尺度）は、ある変数が因子分析に適しているかどうかを判断するための指標です。","この値は、変数が他の変数とどれだけ強く相関しており、かつ偏相関（他の変数の影響を除いた後の相関）が小さいかどうかを見ます。","MSAの計算式は、相関係数r_jkの二乗の合計を、相関係数の二乗の合計と偏相関係数p_jkの二乗の合計の和で割ったものです。","つまり、MSA_j = r^2の合計 / (r^2の合計 + p^2の合計)","偏相関が大きい（他の変数の影響が大きい）場合、MSAは低くなり、因子分析には適さないとされます。","例えば、ある設問が他の設問と強く関連していて、さらに他の変数の影響を除いてもその関係が残る場合、その設問はMSAが高く、因子分析に適しています。"],"originalSlideText":"Anwendbarkeit: Measure of Sampling Adequacy (MSA)\\n- Gesamt und für jede der m Variablen\\n- Variablen mit kleineren Werten werden von der Faktorenanalyse ausgenommen\\n- ∀1 ≤ j ≤ m: MSA_j := ∑_{k≠j} r^2_{jk} / (∑_{k≠j} r^2_{jk} + ∑_{k≠j} p^2_{jk})\\n- r_jk: Korrelation zwischen j und k\\n- p_jk: partielle Korrelation zwischen j und k\\n- Auswertung\\n  - < 0,5: Variable ungeeignet\\n  - 0,6 < MSA_j ≤ 0,8: Variable brauchbar\\n  - > 0,8: Variable gut geeignet\\n- Partielle Korrelation: p_jk,l = (r_jk - r_jl r_kl) / sqrt((1 - r^2_jl)(1 - r^2_kl))","explanationImage":"lecture01/lecture06_ex03.png","questionImage":""},{"id":15,"questionDe":"(s13) Was ist das Ziel der Principal Component Analysis (PCA) und wie wird sie zur Dimensionsreduktion verwendet?","questionJa":"主成分分析（PCA）の目的は何であり、それはどのように次元削減に利用されるのですか？","answerDe":["Ziel: Reduktion der Dimension durch unkorrelierte Hauptkomponenten.","Ergebnis: p < m neue Variablen (principal components), die möglichst viel Varianz erklären.","Geeignet für Visualisierung (p = 2 oder 3) oder Datenkompression.","Man wählt p so, dass möglichst große Varianz abgedeckt wird."],"answerJa":["目的：無相関な主成分によって次元を削減すること。","結果：mより小さいp個の新しい変数（主成分）を得る。これらはできるだけ多くの分散を説明する。","主に可視化（2次元や3次元）やデータの圧縮に用いられる。","分散の多くをカバーするようにpを選ぶ。"],"explanationDe":["Die Principal Component Analysis (PCA) ist ein Verfahren, um die Anzahl von Variablen zu reduzieren, ohne zu viel Information zu verlieren.","Dazu transformiert man die korrelierten Ursprungsvariablen in eine kleinere Anzahl unkorrelierter Hauptkomponenten.","Diese neuen Variablen basieren auf den Richtungen mit der höchsten Varianz.","Beispiel: In einem Datensatz mit Körpergröße, Gewicht und Arm-/Beinlänge lassen sich die Variablen oft zu einer Hauptkomponente wie ‚Körpergröße insgesamt‘ zusammenfassen."],"explanationJa":["PCA（主成分分析）は、情報をあまり失わずに変数の数を減らすための手法です。","相関のある元の変数を、相関のない少数の主成分（新しい変数）に変換します。","これらの主成分は、データのばらつき（分散）が大きい方向を表しており、情報を多く含みます。","例：身長、体重、腕や足の長さといった身体情報の変数を、PCAによって「全体的な体の大きさ」といった1つの主成分に要約することができます。"],"originalSlideText":"PRINCIPAL COMPONENT ANALYSIS (PCA)\\n– Gegeben: m möglicherweise korrelierte beobachtete Variablen x₁, ..., xₘ\\n– Ergebnis: p < m unkorrelierte Variablen (principal components)\\n– Anforderung: p soll so klein wie möglich sein\\n– Visualisierung: p ∈ {2, 3}\\n– Alternativ: Wähle p, so dass eine möglichst große Varianz abgedeckt wird und verwende Scatterplot-Matrizen","explanationImage":"","questionImage":""},{"id":16,"questionDe":"(s14) Was sind die mathematischen Ziele der Principal Component Analysis (PCA) und wie wird Varianz dabei verwendet?","questionJa":"主成分分析（PCA）の数学的な目的は何ですか？また、そこでは分散はどのように使われますか？","answerDe":["Das Ziel der PCA ist es, die Anzahl der Dimensionen zu reduzieren.","Die neuen Dimensionen (Hauptkomponenten) sind orthogonal und linear.","Die Hauptkomponenten werden so geordnet, dass sie möglichst viel Varianz enthalten.","Die Varianz dient als Maß für den Informationsgehalt der Daten.","Mathematisch bedeutet dies, eine neue Basis des Vektorraumes zu finden."],"answerJa":["PCAの目的は、次元（変数）の数を減らすことです。","新しく作られる主成分は互いに直交していて、線形な組み合わせでできています。","主成分は、できるだけ多くの分散（ばらつき）を含むように順に並べられます。","分散は、どれだけの情報がその主成分に含まれているかを示す指標です。","数学的には、データ空間の新しい基底（座標軸）を求める操作になります。"],"explanationDe":["Bei der PCA versucht man, eine große Anzahl von Variablen durch eine kleinere Anzahl neuer Variablen zu ersetzen, die möglichst viel Information enthalten.","Diese neuen Variablen nennt man Hauptkomponenten. Sie sind so konstruiert, dass sie zueinander orthogonal (also rechtwinklig) sind – das verhindert doppelte Information.","Die erste Hauptkomponente zeigt die Richtung, in der die Daten am stärksten streuen – das ist die Richtung mit der höchsten Varianz.","Die zweite Hauptkomponente zeigt die Richtung mit der zweithöchsten Varianz und ist senkrecht zur ersten, usw.","Ein Beispiel: Stelle dir vor, du misst bei 100 Personen die Körpergröße und das Gewicht. Die erste Hauptkomponente könnte dann die allgemeine Körperstatur (groß und schwer vs. klein und leicht) darstellen. Die zweite Hauptkomponente könnte den Unterschied zwischen muskulös und schlank erklären.","Diese Transformation erlaubt es, die wichtigsten Informationen in weniger Dimensionen darzustellen – z. B. 2D statt 10D – was besonders für die Visualisierung hilfreich ist."],"explanationJa":["PCAでは、たくさんの変数（たとえば10個）を、情報をなるべく保ったまま、より少ない変数（たとえば2個）に変換します。","このときに作られる新しい変数を「主成分」と呼び、もとの変数の直線的な組み合わせで作ります。主成分同士は直交（90度）していて、情報が重ならないようになっています。","第1主成分は、データが一番ばらついている方向（つまり一番情報が多い方向）です。第2主成分は、ばらつきが2番目に大きく、しかも第1主成分と直角になるように決まります。","例えば、100人の「身長」と「体重」のデータがあるとします。このとき、第1主成分は『体の大きさ』を表す方向（大きくて重い vs 小さくて軽い）かもしれません。第2主成分は、同じ身長でも「重い or 軽い」など、体型の違いを表すかもしれません。","このようにして、たとえば10個の変数から2つの主成分を使って、元のデータの構造をうまく表すことができます。これはデータの可視化やノイズの除去に役立ちます。"],"originalSlideText":"Ziele\\n– Berechne eine reduzierte Menge von Dimensionen\\n   – Orthogonal\\n   – Linear\\n– Ordne die Dimensionen absteigend bezüglich der Varianz\\nVarianz\\n– Maß für den Informationsgehalt einer Variable\\nMathematisch\\n– Suche eine neue Basis des Vektorraumes","explanationImage":"","questionImage":""},{"id":17,"questionDe":"(s15) Wie wird die Principal Component Analysis (PCA) mathematisch konstruiert?","questionJa":"主成分分析（PCA）は数学的にどのように構成されますか？","answerDe":["Gegeben: Datenmatrix X mit m Variablen und n Beobachtungen.","Berechne den Mittelwert für jede Variable.","Zentriere die Daten, indem du den Mittelwert abziehst.","Berechne die Kovarianzmatrix C.","Führe eine spektrale Zerlegung der Kovarianzmatrix durch: C = UΛU^T.","Λ ist die Diagonalmatrix mit Eigenwerten, U enthält die Eigenvektoren.","Sortiere die Eigenwerte absteigend: λ₁ > λ₂ > ... > λₘ."],"answerJa":["与えられたのは、n個の観測値とm個の変数からなるデータ行列X。","各変数について平均値を計算する。","各データから平均を引いて中心化（ゼロ平均）する。","共分散行列Cを計算する。","Cを固有値分解する（スペクトル分解）：C = UΛU^T。","Λは固有値を並べた対角行列、Uは対応する固有ベクトルを含む。","固有値は大きい順に並べ替える：λ₁ > λ₂ > ... > λₘ。"],"explanationDe":["Die PCA beginnt mit einer Datenmatrix X, in der die Zeilen Beobachtungen und die Spalten Variablen sind.","Zunächst wird der Mittelwert jeder Spalte (Variable) berechnet und anschließend von allen Werten dieser Spalte abgezogen. Dadurch erhält man zentrierte Daten – wichtig, weil PCA nur die Streuung (Varianz) betrachtet.","Dann wird die Kovarianzmatrix berechnet. Sie misst, wie stark zwei Variablen gemeinsam variieren.","Da die Kovarianzmatrix symmetrisch ist, kann man sie durch eine spektrale Zerlegung in Eigenwerte und Eigenvektoren zerlegen: C = UΛUᵀ.","Die Eigenvektoren geben die Richtung der Hauptkomponenten an, die Eigenwerte sagen, wie viel Varianz (also Information) in dieser Richtung steckt.","Beispiel: Wenn λ₁ der größte Eigenwert ist, zeigt der zugehörige Eigenvektor u₁ in die Richtung, in der die Daten am stärksten streuen.","Die PCA nutzt die ersten paar dieser Eigenvektoren (mit den höchsten Eigenwerten), um eine neue, reduzierte Darstellung der Daten zu schaffen."],"explanationJa":["PCAは、行が観測データ（例えば100人）、列が変数（例えば身長・体重など）であるデータ行列Xから始まります。","まず、各変数（列）の平均値を求め、データからその平均を引きます。これは『中心化』と呼ばれ、平均が0になるように整えます。PCAではばらつき（分散）を分析するため、中心化は重要です。","次に共分散行列を計算します。これは、2つの変数がどれだけ一緒に変化するかを表します。","この共分散行列は対称行列なので、『固有値分解』ができます。固有値分解では、固有値（λ）と固有ベクトル（u）を求めます。","固有ベクトルは新しい座標軸（主成分）の方向を示し、固有値はその方向にどれだけ情報（分散）があるかを表します。","たとえば、最も大きな固有値λ₁に対応する固有ベクトルu₁が、第1主成分になります。この方向には一番多くのばらつき（＝情報）が含まれています。","固有値が大きい順に並べて、上位の主成分だけを使って、データの次元を削減しながら、できるだけ多くの情報を保持します。"],"originalSlideText":"Konstruktion\\n– Gegeben: X = (X₁, ..., Xₘ), Xⱼ = (x₁ⱼ, ..., xₙⱼ)ᵀ\\n– Mittelwert: μⱼ = (1/n) ∑ᵢ xᵢⱼ\\n– Zentrierte Werte: Y = xᵢⱼ - μⱼ\\n– Kovarianz-Matrix: C = (1/(n-1)) YᵀY\\n– Symmetrisch → spektrale Zerlegung möglich\\n– Spektrale Zerlegung: C = UΛUᵀ, UᵀU = Iₘ\\n– Eigenwert Matrix: Λ = diag(λ₁, ..., λₘ)\\n– Eigenvektor Matrix: U = (u₁, ..., uₘ)\\n– Ordne die Eigenwerte absteigend: λᵢ > λⱼ für i > j","explanationImage":"","questionImage":""},{"id":18,"questionDe":"(s16) Welche Eigenschaften haben die Hauptkomponenten (principal components) in der PCA?","questionJa":"PCAにおける主成分（Principal Components）の特徴は何ですか？","answerDe":["λ₁, u₁ beschreibt die größte Varianz in den Daten.","λ₂, u₂ beschreibt die zweitgrößte Varianz usw.","Die ersten p Eigenwerte beschreiben den Großteil der Varianz.","Hauptkomponenten mit sehr kleiner Varianz können helfen, Ausreißer zu erkennen.","PCA wird oft in der wissenschaftlichen Visualisierung verwendet."],"answerJa":["λ₁とu₁はデータの中で最も大きな分散（ばらつき）を表す。","λ₂とu₂は次に大きな分散を表す、以下同様。","最初のp個の固有値がデータの大部分の情報を含む。","分散が非常に小さい主成分は外れ値の検出に使える。","PCAは科学的な可視化に頻繁に使用される。"],"explanationDe":["Jede Hauptkomponente (PC) entspricht einer Richtung im Datenraum, in der die Daten variieren. Die erste PC (u₁) ist die Richtung mit der größten Streuung – also da, wo am meisten Information steckt.","Je mehr PCs man hinzufügt (z. B. u₂, u₃ …), desto mehr Struktur der Daten wird erklärt, aber der Zugewinn wird kleiner.","Beispiel: Wenn man 100 Dimensionen hat, können die ersten 3 PCs bereits 90 % der Varianz erklären.","Wenn eine PC fast keine Varianz enthält, könnte das ein Hinweis auf Ausreißer sein – etwa ein Sensorfehler, der nur bei einem Messpunkt auftritt.","Wegen ihrer Eigenschaft, Daten zu komprimieren und gleichzeitig visuell interpretierbar zu machen, wird PCA auch häufig in der wissenschaftlichen Visualisierung genutzt."],"explanationJa":["主成分（PC）は、データ空間の中でばらつきが最も大きい方向を表します。第一主成分（u₁）は、データが最も広がっている方向です。","第2主成分（u₂）、第3主成分（u₃）と続きますが、それぞれの方向は前のものと直交しており、情報（分散）の多い順に並んでいます。","例：100次元のデータでも、最初の3つの主成分だけで全体の90％の情報を説明できることがあります。","一方、分散がほとんどない主成分は、ノイズや外れ値（例：誤作動したセンサーの値）を見つけるのに役立ちます。","また、次元削減により2次元や3次元に圧縮して視覚化できるため、科学的なデータ可視化によく使われます。"],"originalSlideText":"Eigenschaften:\\n- λ₁, u₁: größte Varianz\\n- λ₂, u₂: zweitgrößte Varianz\\n- …\\n- Die ersten p Eigenwerte beschreiben einen Großteil der Varianz\\n- Principal components mit einer Varianz nahe 0 können verwendet werden, um Ausreißer zu identifizieren\\n- Wird auch häufig in der Scientific Visualization verwendet","explanationImage":"","questionImage":""},{"id":19,"questionDe":"(s16) Welche Einschränkungen hat die Principal Component Analysis (PCA)?","questionJa":"主成分分析（PCA）にはどのような制約がありますか？","answerDe":["Das Ergebnis hängt stark von der Skalierung der Variablen ab.","Ausreißer können das Ergebnis stark beeinflussen.","PCA ist eine lineare Methode.","Die Interpretation der neuen Achsen (Basis) ist oft schwierig."],"answerJa":["変数のスケーリング（単位）によって結果が大きく変わる。","外れ値があると結果に大きな影響を与える。","PCAは線形な方法である。","新しい軸（主成分）の解釈が難しいことがある。"],"explanationDe":["PCA basiert auf der Varianz. Wenn eine Variable in Zentimetern und eine andere in Kilometern gemessen ist, dominiert die größere Einheit das Ergebnis – daher ist Standardisierung wichtig.","Ein einziger Ausreißer kann die Richtung der Hauptkomponenten stark verschieben.","Da PCA nur lineare Beziehungen betrachtet, erkennt sie keine komplexen, nichtlinearen Muster in den Daten.","Die neuen Achsen (PCs) sind oft schwer zu interpretieren, da sie eine Kombination aus vielen Ursprungsvariablen sind – z. B. ist PC1 vielleicht eine Mischung aus 40 % Temperatur, 30 % Luftdruck und 30 % Luftfeuchtigkeit."],"explanationJa":["PCAは変数のばらつき（分散）をもとにしているため、単位が異なると不公平になります。例えば、1つの変数がcmで、もう1つがkmだと、kmの方が大きく影響します。標準化が必要です。","また、たった1つの異常値（外れ値）が全体の主成分方向を変えてしまう可能性があります。","PCAは直線的（線形）な方法なので、曲線的な構造（非線形）には対応できません。","主成分は複数の変数が混ざったものになるため、『PC1は何を意味するか』を明確に説明するのが難しい場合があります。例えば、PC1が温度40％＋湿度30％＋気圧30％のような混合になっていることもあります。"],"originalSlideText":"Einschränkungen:\\n- Das Ergebnis ist abhängig von der Skalierung der einzelnen Variablen (Attribute)\\n- Ausreißer haben einen großen Einfluss auf das Ergebnis\\n- Lineare Methode\\n- Interpretation der Basis (Ergebnisse)","explanationImage":"","questionImage":""},{"id":20,"questionDe":"(s12–s16) Erklären Sie den Ablauf der Principal Component Analysis (PCA) und wie sie für die Dimensionsreduktion angewendet wird. (5 Punkte)","questionJa":"（試験）主成分分析（PCA）の手順と、それがどのように次元削減に用いられるかを説明しなさい。（5点）","answerDe":["1. Zentriere die Daten: Berechne den Mittelwert jeder Variable und ziehe ihn ab.","2. Berechne die Kovarianzmatrix der zentrierten Daten.","3. Führe eine Eigenwertzerlegung der Kovarianzmatrix durch.","4. Wähle die p größten Eigenwerte und die zugehörigen Eigenvektoren (Hauptkomponenten).","5. Projiziere die ursprünglichen Daten auf den neuen Raum der Hauptkomponenten, um die Dimension zu reduzieren."],"answerJa":["1. データを中心化する：各変数の平均を求め、すべてのデータからその平均を引く。","2. 中心化したデータの共分散行列を求める。","3. 共分散行列に対して固有値分解を行う。","4. 固有値が大きい順にp個を選び、それに対応する固有ベクトル（主成分）を選ぶ。","5. 元のデータを主成分空間に射影することで、次元数をpに削減する。"],"explanationDe":["Die Principal Component Analysis (PCA) ist ein Verfahren, das korrelierte Variablen durch eine kleinere Anzahl unkorrelierter Variablen ersetzt.","Zuerst werden die Daten zentriert, um den Ursprung der Hauptachsen korrekt zu setzen.","Dann wird die Kovarianzmatrix berechnet, um die Varianzstruktur zu analysieren.","Durch Eigenwertzerlegung erhält man die Hauptkomponenten, die Richtungen der größten Varianz anzeigen.","Man wählt typischerweise die ersten p Hauptkomponenten, die den Großteil der Varianz abdecken, und reduziert somit die Dimension.","Beispiel: Aus einem Datensatz mit 10 Variablen können durch PCA 2 Hauptkomponenten extrahiert werden, die 90 % der Varianz erklären."],"explanationJa":["PCA（主成分分析）は、相関のある複数の変数を、より少ない数の相関のない変数（主成分）に置き換える手法です。","最初にデータを平均0に中心化し、その後共分散行列を用いて、どの方向にデータが広がっているか（分散）を調べます。","その行列を固有値分解し、最も大きな固有値に対応する固有ベクトル（主成分）を抽出します。","その後、元のデータをこれらの主成分の空間に射影して、新しい軸で表現します。","例：10変数のデータをPCAで処理した結果、最初の2主成分で90％以上の情報が保持され、2次元での可視化が可能になります。"],"originalSlideText":"s12–s16: Ablauf (Zentrierung, Kovarianz, Spektrale Zerlegung), Auswahl der Hauptkomponenten, Anwendung zur Dimensionsreduktion","explanationImage":"","questionImage":""},{"id":21,"questionDe":"(s17) Was ist das Ziel von Multi-Dimensional Scaling (MDS)?","questionJa":"多次元尺度構成法（MDS）の目的は何ですか？","answerDe":["Ziel ist es, Datenpunkte aus einem Ähnlichkeitssystem in ein Koordinatensystem zu überführen.","Dabei soll die Struktur der Ähnlichkeiten zwischen den Punkten erhalten bleiben."],"answerJa":["類似度の情報に基づいて、データ点を座標空間上に配置すること。","元の類似度関係が、変換後の位置関係にも反映されるようにすること。"],"explanationDe":["Multi-Dimensional Scaling (MDS) wandelt abstrakte Ähnlichkeitsinformationen zwischen Objekten in konkrete Positionen in einem Koordinatensystem um.","Das Ziel besteht darin, ein räumliches Modell zu erstellen, in dem die Distanzen zwischen den Punkten die ursprünglichen Ähnlichkeiten so gut wie möglich widerspiegeln.","Zum Beispiel kann MDS verwendet werden, um Musikgeschmack von Personen, auf Basis ihrer Ähnlichkeit, visuell darzustellen."],"explanationJa":["MDS（多次元尺度構成法）は、物体間の類似度情報をもとに、座標空間上の位置として表現する方法です。","目的は、類似度の構造を保ったまま、視覚的に理解しやすい空間配置を得ることです。","例えば、音楽の好みが似ている人たちを、2次元の図に並べて表示するような使い方ができます。"],"originalSlideText":"Gegeben: eine Tabelle mit Ähnlichkeiten\\nGesucht: Abbildung der Datenpunkte von einem Koordinatensystem in ein anderes\\nMDS: Menge von Systemen","explanationImage":"","questionImage":""},{"id":22,"questionDe":"(s17) Welche Bedingungen müssen beim Multi-Dimensional Scaling (MDS) erfüllt sein?","questionJa":"多次元尺度構成法（MDS）で満たすべき条件は何ですか？","answerDe":["Monotonie: Kleinere Distanzen im Originalsystem führen zu kleineren Distanzen im Zielsystem.","Minimum: Die Zieldimension soll möglichst gering sein."],"answerJa":["単調性：元の空間で近い点同士は、変換後の空間でも近くにある必要がある。","最小性：できるだけ少ない次元で表現するようにする。"],"explanationDe":["MDS versucht, ein niedrigdimensionales Koordinatensystem zu finden, in dem die relativen Distanzen der Objekte möglichst gut den Originaldaten entsprechen.","Dabei bedeutet Monotonie, dass Objekte, die im Originalsystem näher beieinander liegen, auch im neuen Raum näher beieinander platziert werden.","Außerdem soll die Ziel-Dimension (z. B. 2D oder 3D) so gering wie möglich gehalten werden, um die Visualisierung und Interpretation zu erleichtern."],"explanationJa":["MDSでは、元の距離情報に基づき、次元削減後の空間でも同様の距離関係を保つ必要があります。","「単調性」は、元の空間で距離が短いものは、新しい空間でも近くに配置されるという性質です。","また、次元数はできるだけ少なく抑えることで、可視化や理解がしやすくなります（例：2次元や3次元）。"],"originalSlideText":"Bedingungen\\nMonotonie: die Distanz zwischen zwei Datenpunkten im Zielsystem ist kleiner, wenn die Distanz der Datenpunkte im Originalsystem kleiner ist\\nMinimum: die Dimension des Zielsystems soll so klein wie möglich sein","explanationImage":"","questionImage":""},{"id":23,"questionDe":"(s18) Was ist der mathematische Ablauf von Multi-Dimensional Scaling (MDS)?","questionJa":"Multi-Dimensional Scaling（MDS）の数学的な手順を説明しなさい。","answerDe":["Gegeben: Hochdimensionale Datenpunkte X = (x₁, ..., xᵣ), wobei xᵢ ∈ ℝᵐ.","Gesucht: Eine niedrigdimensionale Darstellung Y = (y₁, ..., yᵣ), yᵢ ∈ ℝⁿ mit n ≪ m.","Bedingung: Die Distanzen ‖yᵢ − yⱼ‖ in der neuen Darstellung sollen möglichst gut zu den Originaldistanzen tᵢⱼ passen.","Für Visualisierungen wird häufig n = 2 oder n = 3 gewählt.","Optimierung: Minimierung der Kostenfunktion ∑_{i<j} (‖yᵢ − yⱼ‖ − tᵢⱼ)²."],"answerJa":["入力として、高次元空間（例えば m 次元）上のデータ点 X = (x₁, ..., xᵣ) が与えられる。","これを低次元（n 次元、通常は 2 または 3）の空間の点 Y = (y₁, ..., yᵣ) に写像することを目指す。","このとき、元の点 xᵢ と xⱼ 間の距離 tᵢⱼ と、写像後の点 yᵢ と yⱼ 間の距離 ‖yᵢ − yⱼ‖ ができるだけ一致するようにする。","可視化を目的とする場合は、n = 2 または n = 3 が選ばれることが多い。","目的は、コスト関数 ∑_{i<j} (‖yᵢ − yⱼ‖ − tᵢⱼ)² を最小化することである。"],"explanationDe":["MDS ist ein Verfahren zur Dimensionsreduktion, das besonders dann verwendet wird, wenn man nur die Ähnlichkeiten oder Distanzen zwischen Objekten kennt.","Ziel ist es, eine neue, niedrigdimensionale Darstellung der Daten zu finden, in der die geometrischen Abstände zwischen den Punkten möglichst den ursprünglichen Ähnlichkeiten entsprechen.","Die Ähnlichkeitsmatrix oder Distanzmatrix dient als Ausgangspunkt. Diese Matrix enthält die Distanzen zwischen allen Punktpaaren.","MDS berechnet neue Koordinaten für die Punkte in 2D oder 3D, sodass die Euklidischen Distanzen dieser neuen Punkte den ursprünglichen Distanzen möglichst nahekommen.","Beispiel: In der Psychologie wird MDS verwendet, um Begriffe, die in Umfragen ähnlich beurteilt werden, auf einer Karte anzuordnen – z.B. werden \'Apfel\' und \'Birne\' näher beieinander liegen als \'Apfel\' und \'Auto\'."],"explanationJa":["MDS（多次元尺度構成法）は、データ間の距離や類似度のみが与えられているときに、低次元の空間にデータを写像する手法です。","この手法では、元の高次元空間でのデータ間の関係（距離）を保ちながら、2次元や3次元の空間で再配置を行います。","出発点として、すべてのデータ点のペア間の距離（または類似度）を示す距離行列を使用します。","目的は、新しい空間での点同士の距離が、元の距離にできるだけ近くなるように、座標を最適化することです。","例としては、心理学の分野で、複数の語句（例えば『りんご』と『なし』、『りんご』と『自動車』）の類似性に基づいて、語句を2次元マップ上に配置する際に用いられます。"],"originalSlideText":"Gegeben: X = (x₁, ..., xᵣ), xᵢ ∈ ℝᵐ\\nGesucht: Y = (y₁, ..., yᵣ), yᵢ ∈ ℝⁿ, n ≪ m\\nBedingung: ‖yᵢ − yⱼ‖ ≈ tᵢⱼ, ∀ i,j\\nIn der Visualisierung: n = 2 oder n = 3\\nMinimierung der Kostenfunktion: min_{x₁,...,xᵣ} ∑_{i<j} (‖yᵢ − yⱼ‖ − tᵢⱼ)²","explanationImage":"","questionImage":""},{"id":24,"questionDe":"(s19) Wie funktioniert das Metric Multi-Dimensional Scaling (MDS) rechnerisch?","questionJa":"Metric Multi-Dimensional Scaling（MDS）は計算的にどのように行われますか？","answerDe":["Gegeben ist eine Distanzmatrix t_ij.","Berechne A = (a_ik), wobei a_ik = -1/2 * t_ik^2.","Berechne B = (b_ij) durch Zentrierung von A.","Bestimme Eigenwerte λ_i und zugehörige Eigenvektoren e_i von B.","Wähle die m größten Eigenwerte und entsprechenden Eigenvektoren für die neue Darstellung.","Dieses Verfahren ist vergleichbar mit der PCA."],"answerJa":["入力として距離行列 t_ij が与えられる。","まず A 行列を計算する。各要素 a_ik は −1/2 × t_ik² で与えられる。","次に A を基に B 行列を作成する（これは A の中心化に対応）。","B 行列の固有値 λ_i と対応する固有ベクトル e_i を求める。","最も大きい m 個の固有値に対応する固有ベクトルを選び、新しい座標空間を構成する。","この方法は主成分分析（PCA）と類似している。"],"explanationDe":["Metric MDS ist eine rechnerische Methode zur Dimensionsreduktion, bei der die Eingabe eine Distanzmatrix ist.","Zunächst wird die Distanzmatrix in eine quadratische Matrix A umgewandelt, in der jede Distanz quadriert und skaliert wird.","Dann wird die Matrix A zentriert, um die Matrix B zu erhalten – das entspricht einer Verschiebung in einen neuen Ursprung.","Aus der Matrix B werden mittels Eigenwertzerlegung die Hauptachsen (Dimensionen) extrahiert.","Die größten Eigenwerte repräsentieren die Richtungen mit der meisten Information, ähnlich wie bei der PCA.","Beispiel: Wenn man Ähnlichkeitsdaten zwischen Tieren (z. B. Katze, Hund, Fisch) hat, kann MDS diese auf einer 2D-Karte platzieren, sodass ähnliche Tiere näher beieinander liegen."],"explanationJa":["Metric MDS は、与えられた距離行列を基に、元のデータの幾何的構造を低次元空間で再現する方法です。","最初に、距離行列から各要素を −1/2 倍した2乗値で構成された A 行列を計算します。","次に、A 行列から行と列の平均を用いて中心化を行い、B 行列を作成します。","B 行列に対して固有値分解を行い、固有値と対応する固有ベクトルを得ます。","大きな固有値に対応する固有ベクトルを選び、それを使って低次元空間にデータを配置します。","このプロセスは PCA（主成分分析）と非常に似ており、両者とも固有値分解に基づいて重要な軸を抽出します。","例：動物同士の類似性を示す距離データから、MDS により2次元マップ上で動物を視覚的に配置できる。"],"originalSlideText":"Metric Multi-Dimensional Scaling\\n- Gegeben: Distanzmatrix (t_ij)\\n- A = (a_ik), a_ik = -1/2 * t_ik^2\\n- B = (b_ij), b_ij = a_ij − (1/m) ∑_k a_ik − (1/m) ∑_k a_kj + (1/m²) ∑_k ∑_l a_kl\\n- Bestimme die Eigenwerte λ_i und die zugehörigen Eigenvektoren e_i von B mit ∑_j γ_ij² = λ_i\\n- Wähle die m Eigenvektoren mit den größten Eigenwerten\\n- Vergleichbar mit der PCA","explanationImage":"","questionImage":""},{"id":25,"questionDe":"(s19) Erklären Sie den Ablauf des Metric Multi-Dimensional Scaling (MDS). (Prüfung, 3 Punkte)","questionJa":"（試験類題）Metric MDS の処理手順（Ablauf）を説明しなさい（3点）","answerDe":["1. Gegeben ist eine Distanzmatrix zwischen Datenpunkten.","2. Berechne Matrix A: a_ik = -1/2 * t_ik^2.","3. Zentriere A zu Matrix B.","4. Bestimme Eigenwerte und Eigenvektoren von B.","5. Wähle die m größten Eigenvektoren zur Darstellung."],"answerJa":["1. データ点間の距離行列が与えられる。","2. A 行列を計算する（要素 a_ik = -1/2 × t_ik²）。","3. A を中心化して B 行列を作る。","4. B の固有値と固有ベクトルを求める。","5. 最も大きな m 個の固有ベクトルを選び、新しい空間に写像する。"],"explanationDe":["Beim Metric MDS beginnt man mit einer Distanzmatrix, die die Abstände zwischen allen Datenpunkten angibt.","Diese wird zunächst in eine neue Matrix A überführt, wobei die quadrierten Distanzen verwendet werden.","Danach wird A zentriert (Mittelwert-Korrektur), sodass man Matrix B erhält.","Aus B bestimmt man dann die Eigenwerte und Eigenvektoren.","Die Richtung der größten Eigenvektoren entspricht den neuen Achsen des Zielraums."],"explanationJa":["Metric MDS は距離行列（各データ間の距離）を入力として使用します。","まずその距離を二乗して −1/2 を掛けた A 行列を作ります。","次に、行列 A を行・列の平均を使って中心化し、B 行列を作成します。","この B 行列に固有値分解を行って、固有値と固有ベクトルを求めます。","最も大きな固有値に対応する固有ベクトルを取り出して、それを使って新しい軸（座標空間）にデータを写します。"],"originalSlideText":"Metric Multi-Dimensional Scaling\\n- Gegeben: Distanzmatrix (t_ij)\\n- A = (a_ik), a_ik = -1/2 * t_ik^2\\n- B = (b_ij), b_ij = a_ij − ...\\n- Bestimme die Eigenwerte λ_i ...\\n- Wähle die m Eigenvektoren mit den größten Eigenwerten\\n- Vergleichbar mit der PCA","explanationImage":"","questionImage":""},{"id":26,"questionDe":"(s19) Wie wird Metric MDS für Dimensionsreduktion angewendet? (Prüfung, 2 Punkte)","questionJa":"（試験類題）Metric MDS はどのように次元削減に応用されるか説明しなさい（2点）","answerDe":["MDS transformiert hochdimensionale Distanzen in einen Raum mit niedriger Dimension.","Dabei wird versucht, die relativen Abstände zwischen den Datenpunkten möglichst gut zu erhalten.","So können z. B. 100-dimensionale Daten in 2D abgebildet werden."],"answerJa":["MDS は高次元のデータ間の距離関係を保ったまま、低次元空間に変換する方法である。","この際、データ点同士の相対的な距離（近い・遠い）をなるべく保つようにする。","例えば、100 次元のデータを 2 次元に縮約して可視化することができる。"],"explanationDe":["Das Ziel von MDS in der Dimensionsreduktion ist, die Struktur der Daten – also die Ähnlichkeiten – in einem Raum mit weniger Dimensionen darzustellen.","Wenn zum Beispiel Kundenprofile in 50 Dimensionen gegeben sind, kann MDS sie in 2D oder 3D darstellen, sodass ähnliche Profile nahe beieinander liegen.","Dadurch wird eine visuelle Analyse und Interpretation möglich."],"explanationJa":["MDS の目的は、元の高次元空間でのデータの類似性（距離）を保ったまま、より少ない次元の空間で表現することです。","例えば、顧客の50項目の特徴量を持つデータを、2次元空間に写すことで、類似した顧客を近くに配置できます。","こうすることで、視覚的な分析やパターンの発見がしやすくなります。"],"originalSlideText":"Vergleichbar mit der PCA\\nBestimme die Eigenwerte λ_i und die zugehörigen Eigenvektoren ...\\nWähle die m größten Eigenwerte ...","explanationImage":"","questionImage":""},{"id":27,"questionDe":"(s20) Erklären Sie die Funktionsweise und den Anwendungsbereich von t-SNE.","questionJa":"t-SNEのしくみと利用される場面について説明しなさい。","answerDe":["t-SNE ist eine Methode zur Dimensionsreduktion.","Es bettet Daten in eine 2-dimensionale Ebene ein.","Dabei wird versucht, die lokale Verteilung der Daten zu erhalten.","t-SNE wird häufig im Bereich des Maschinellen Lernens eingesetzt."],"answerJa":["t-SNEは次元削減の手法である。","データを2次元平面に埋め込む（低次元化する）。","この際、元のデータの局所的な分布を保つようにしている。","機械学習の分野でよく利用されている。"],"explanationDe":["t-SNE (t-distributed Stochastic Neighbor Embedding) reduziert hochdimensionale Daten auf meist zwei Dimensionen, um sie visuell analysieren zu können.","Der Algorithmus achtet besonders darauf, dass Datenpunkte, die im Originalraum nahe beieinander liegen, auch im Zielraum nahe bleiben.","Ein typisches Beispiel ist die Darstellung von hochdimensionalen Bilderkennungsdaten: Ähnliche Bilder erscheinen nebeneinander.","Da t-SNE auf lokale Nachbarschaften fokussiert, ist es besonders nützlich zur Cluster-Erkennung."],"explanationJa":["t-SNE（t分布型確率的近傍埋め込み）は、主に視覚化を目的として高次元データを2次元に縮約する次元削減のアルゴリズムです。","この方法は、元の空間で近くにあったデータ点が、縮約後も近くに配置されるように調整されます。","たとえば、画像の特徴量など高次元なデータを扱う際に、似た画像が近くに配置されるようにマッピングされます。","そのため、クラスタの発見や構造の理解などに有用であり、機械学習や深層学習の分野で広く利用されています。"],"originalSlideText":"T-DISTRIBUTED STOCHASTIC NEIGHBORHOOD EMBEDDING (T-SNE)\\n- t-SNE ist eine weitere Methode zur Dimensionsreduktion\\n- Daten werden in eine 2 dimensionale Ebene eingebettet\\n- T-SNE versucht die lokale Verteilung der Daten zu bewahren\\n- Häufig im Bereich des Maschinellen Lernens eingesetzt","explanationImage":"","questionImage":""},{"id":28,"questionDe":"(s21) Beschreiben Sie die Methode, mit der t-SNE die Wahrscheinlichkeiten zur Darstellung der Ähnlichkeit zwischen Punkten berechnet.","questionJa":"t-SNEがデータ点間の類似性を表現するためにどのように確率を計算しているかを説明しなさい。","answerDe":["Berechne für jede Distanz eine bedingte Wahrscheinlichkeit p_{j|i}.","Nutze dafür einen Gaußschen Kernel mit punktabhängiger Breite σ_i.","Wandle p_{j|i} in symmetrische Wahrscheinlichkeit p_{ij} um.","Die Breite σ_i wird für jeden Punkt i individuell angepasst."],"answerJa":["各点 i と他の点 j との距離に基づき、条件付き確率 p_{j|i} を計算する。","このとき、点 i ごとに異なる幅 σ_i のガウスカーネルを使用する。","得られた p_{j|i} と p_{i|j} から対称的な確率 p_{ij} を作る。","カーネルの幅 σ_i は点ごとに適切に調整される。"],"explanationDe":["t-SNE beginnt damit, die Ähnlichkeit zwischen zwei Punkten i und j durch eine bedingte Wahrscheinlichkeit p_{j|i} zu modellieren.","Diese Wahrscheinlichkeit wird mit Hilfe einer Gaußverteilung berechnet, deren Breite (σ_i) für jeden Punkt so gewählt wird, dass die Verteilung gut zur lokalen Nachbarschaft passt.","Dadurch bekommt jeder Punkt seine eigene Sicht auf die Nachbarschaft.","Um die Asymmetrie zu vermeiden, werden die p_{j|i} und p_{i|j} kombiniert, sodass p_{ij} eine symmetrische Wahrscheinlichkeit ergibt.","Ein Beispiel: Wenn ein Punkt viele nahe Nachbarn hat, wird die Verteilung enger (kleines σ_i), während sie bei isolierten Punkten breiter wird."],"explanationJa":["t-SNEでは、各点 i と他の点 j の間の類似度を、条件付き確率 p_{j|i} によって表します。","これは、点 i にガウス分布（正規分布）を配置し、その幅（σ_i）を調整して計算します。","例えば、点 i の近くに多くの点があれば、σ_i は小さくなり、周囲がまばらな場合はσ_i は大きくなります。","その後、p_{j|i} と p_{i|j} を使って、対称的な確率 p_{ij} を導出します。","この処理により、各点のローカルな密度に応じて、柔軟に類似性を表現できるようになります。"],"originalSlideText":"METHODE\\n- Berechne für jede Distanz zwischen Punkt i und j eine abhängige Wahrscheinlichkeit, welche die Ähnlichkeit repräsentiert\\n- Platziere einen Gausschen Kernel über dem Punkt i und berechne die Wahrscheinlichkeiten aller Nachbarn.\\n- Wandle die abhängigen Wahrscheinlichkeiten in Wahrscheinlichkeiten um\\n- Für jeden Punkt i muss die Breite des Kernels angepasst werden","explanationImage":"","questionImage":""},{"id":29,"questionDe":"(s22) Wie funktioniert die Einbettung bei t-SNE und welche Rolle spielt die Kullback-Leibler Divergenz?","questionJa":"t-SNEではどのようにデータが2次元に埋め込まれ、KLダイバージェンスがどのように使われるか説明しなさい。","answerDe":["Platziere Punkte in eine 2D-Ebene.","Berechne Wahrscheinlichkeiten basierend auf der Student-t-Verteilung mit einem Freiheitsgrad.","Optimiere die Einbettung mit einem Gradientenverfahren.","Nutze die Kullback-Leibler-Divergenz als Kostenfunktion."],"answerJa":["すべての点を2次元平面に初期配置する。","Studentのt分布（自由度1）に基づいて、点と点の間の確率を計算する。","確率分布が元の空間の分布と一致するように、勾配法で配置を最適化する。","Kullback-Leiblerダイバージェンスが配置の評価指標として使われる。"],"explanationDe":["In t-SNE wird jeder Datenpunkt zufällig in eine zweidimensionale Ebene platziert.","Zwischen diesen Punkten wird dann eine Wahrscheinlichkeit q_ij berechnet, die auf der Student-t-Verteilung mit einem Freiheitsgrad basiert. Diese Verteilung erlaubt es, dass auch weiter entfernte Punkte eine kleine, aber nicht vernachlässigbare Wahrscheinlichkeit haben.","Danach wird die Platzierung so lange angepasst, bis die Wahrscheinlichkeiten q_ij in der 2D-Ebene möglichst gut zu den Wahrscheinlichkeiten p_ij im Hochraum passen.","Das Maß für diese Übereinstimmung ist die Kullback-Leibler Divergenz, welche die Differenz zwischen den beiden Verteilungen misst.","Beispiel: Wenn zwei Punkte im Hochraum sehr ähnlich sind (hohes p_ij), aber in der 2D-Einbettung weit auseinander liegen (kleines q_ij), dann wird die KL-Divergenz groß und die Positionen werden angepasst."],"explanationJa":["t-SNEではまず、全てのデータ点を2次元平面上に仮置きします。","その後、点と点の間の距離に基づき、t分布（自由度1）を用いて確率 q_ij を計算します。t分布は正規分布よりも裾が広く、遠く離れた点の影響も考慮できます。","この2次元の確率分布 q_ij が、元の高次元空間で計算された確率分布 p_ij にできるだけ近づくように、配置を徐々に調整していきます。","この“近さ”の指標として、Kullback-Leibler（KL）ダイバージェンスが用いられます。KLダイバージェンスが小さいほど、2つの確率分布は似ていると判断されます。","例えば、元の空間で似ている2つの点が2次元上で遠くに配置されていると、KLダイバージェンスが大きくなり、その差を減らすように点の位置が調整されます。"],"originalSlideText":"METHODE\\n- Platziere jeden Punkt in eine 2D Ebene\\n- Berechne für jeden Punkt eine Wahrscheinlichkeit basierend auf der Student t-Verteilung\\n  - Ein Freiheitsgrad\\n- Optimiere die Einbettung durch einen Gradient Walk, bis die Wahrscheinlichkeiten “passen”\\n- Kullback-Leibler Divergenz als Metrik für die Platzierung","explanationImage":"","questionImage":""},{"id":30,"questionDe":"(s23) Welche Parameter können das Verhalten von t-SNE beeinflussen?","questionJa":"t-SNEの動作に影響を与えるパラメータにはどのようなものがありますか？","answerDe":["Perplexity: Anzahl der Nachbarn im Gauss-Kernel","Maximale Anzahl von Iterationen","Theta: Parameter für die Barnes-Hut-Optimierung","Zielanzahl der Dimensionen"],"answerJa":["Perplexity（パープレキシティ）: ガウスカーネル内に含める近傍データ点の数","最大イテレーション数","theta（シータ）: Barnes-Hut最適化のためのパラメータ","次元数の設定（通常は2次元や3次元）"],"explanationDe":["t-SNE verwendet verschiedene Parameter, um die Qualität und Effizienz der Einbettung zu beeinflussen:","- Die Perplexity bestimmt die effektive Anzahl der Nachbarn eines Punktes. Kleine Werte fokussieren auf lokale Struktur, große auf globale.","- Die maximale Iteration legt fest, wie lange das Optimierungsverfahren läuft.","- Theta beeinflusst die Geschwindigkeit und Genauigkeit bei der Barnes-Hut-Optimierung (ein Näherungsverfahren für große Datenmengen).","- Die Zielanzahl der Dimensionen bestimmt, wie viele Dimensionen die eingebetteten Daten am Ende haben – meist 2 oder 3 für Visualisierung."],"explanationJa":["t-SNEは以下のような複数のパラメータによって動作が調整されます：","- Perplexity（パープレキシティ）は各点の近傍として考慮するデータ点の数を表し、小さいと局所構造、大きいと大域構造を重視します。","- 最大イテレーション数は、配置をどこまで最適化するか（繰り返し回数）を決定します。","- theta（シータ）は、Barnes-Hut近似という高速化手法で用いられ、精度と計算効率のトレードオフを調整します。","- 次元数の設定では、最終的に何次元に埋め込むかを決めます（多くは2次元または3次元）。"],"originalSlideText":"METHODE\\n- t-SNE hat mehrere Parameter\\n  - Perplexity: Wie viele Datenpunkte sollen im Gauss Kernel liegen\\n  - Maximale Iterationen\\n  - theta: Parameter für die Barnes-Hut Optimierung\\n  - Anzahl der Dimensionen","explanationImage":"","questionImage":""},{"id":31,"questionDe":"(s24) Welche praktischen Einschränkungen und Eigenschaften hat t-SNE?","questionJa":"t-SNEの実用上の制約や特徴にはどのようなものがありますか？","answerDe":["Komplexität von Laufzeit und Speicher ist quadratisch (n²)","Verwendet Quadtree zur Optimierung des zweiten Schritts","Geeignet nur für weniger als 10.000 Datenpunkte","Ergebnisse sind nicht deterministisch"],"answerJa":["計算時間とメモリの複雑さはデータ数nの2乗（n²）に比例する","第2ステップではクアッドツリーによる最適化を使用","10,000点未満のデータに適している","結果が毎回異なり、決定的でない（再現性がない）"],"explanationDe":["t-SNE hat einige praktische Einschränkungen:","- Da die Komplexität quadratisch zur Anzahl der Datenpunkte steigt, ist es für große Datensätze rechnerisch teuer.","- Ein Quadtree hilft, die Berechnung in einem Schritt effizienter zu machen (besonders bei vielen Punkten).","- In der Praxis wird t-SNE oft nur bei kleineren Datensätzen (< 10.000) verwendet.","- Aufgrund zufälliger Initialisierungen und Optimierungsschritte liefert t-SNE keine stabilen, reproduzierbaren Ergebnisse – jeder Lauf kann anders aussehen."],"explanationJa":["t-SNEの実用的な制約について説明します：","- 時間とメモリの消費は、データ点の数nに対してn²のオーダーで増加するため、大規模データには不向きです。","- クアッドツリーという空間分割構造を用いることで、第2ステップ（近傍点との関係計算）を効率化します。","- 通常、1万点以下の比較的小規模なデータセットに限定して使われます。","- 初期配置や確率的最適化によって毎回異なる結果を生成するため、再現性がありません。"],"originalSlideText":"METHODE\\n- Die Komplexität ist für Laufzeit und Speicher ist n²\\n- Optimierung des 2. Schrittes mit einem Quadtree\\n- Nur für Datensätze mit weniger als 10.000 Datenpunkten\\n- Erzeugt keine deterministischen Ergebnisse","explanationImage":"","questionImage":""},{"id":32,"questionDe":"(s22–24) Erklären Sie den Ablauf von t-SNE.","questionJa":"t-SNEの手順を説明しなさい（試験）","answerDe":["Platziere alle Punkte zunächst in einer 2D-Ebene zufällig.","Berechne Wahrscheinlichkeiten für alle Punktpaare im Ursprungsraum mit Gauß-Verteilung.","Berechne Wahrscheinlichkeiten im Zielraum mit t-Verteilung (ein Freiheitsgrad).","Optimiere die Einbettung durch Minimierung der Kullback-Leibler-Divergenz.","Nutze einen Gradientenabstieg für die Optimierung."],"answerJa":["すべてのデータ点をまず2次元平面にランダムに配置する。","元の高次元空間での各点のペアについて、ガウス分布に基づいた類似度（確率）を計算する。","2次元空間では、t分布（自由度1）を使って同様に確率を計算する。","元と変換後の確率分布の差（KLダイバージェンス）を最小化するように、配置を更新する。","最小化には勾配降下法（gradient descent）を用いる。"],"explanationDe":["t-SNE ist ein nichtlineares Verfahren zur Reduktion von hochdimensionalen Daten in 2D oder 3D.","Zuerst wird jeder Datenpunkt zufällig auf einer Fläche platziert.","Dann wird die Ähnlichkeit zwischen Punktpaaren im Originalraum berechnet, indem eine Gauß-Verteilung verwendet wird.","Im Zielraum wird eine t-Verteilung genutzt, die weiter streut und lokale Nachbarschaften besser abbildet.","Die Kullback-Leibler-Divergenz wird als Kostenfunktion verwendet, um die Ähnlichkeitsverteilung möglichst gut zu erhalten.","Beispiel: Zwei Kunden mit ähnlichen Kaufverhalten sollen auch im 2D-Bild nahe beieinander liegen."],"explanationJa":["t-SNEは、高次元データを2次元や3次元に変換する非線形な次元削減手法です。","まず、すべてのデータ点を2次元空間にランダムに配置します。","次に、元の空間（例えば100次元など）で各データ点の距離に基づいて類似度をガウス分布により確率として計算します。","2次元空間ではt分布（自由度1）を使って確率を計算し、元の分布と似るように配置を最適化します。","この最適化では、KLダイバージェンスという指標を使い、確率の差を最小にするよう配置を繰り返し修正します。","例：似た購買履歴を持つ顧客同士が、2次元図上で近くにプロットされるようになります。"],"originalSlideText":"METHODE\\n- Platziere jeden Punkt in eine 2D Ebene\\n- Berechne für jeden Punkt eine Wahrscheinlichkeit basierend auf der Student t-Verteilung\\n- Ein Freiheitsgrad\\n- Optimiere die Einbettung durch einen Gradient Walk\\n- Kullback-Leibler Divergenz als Metrik für die Platzierung","explanationImage":"","questionImage":""},{"id":33,"questionDe":"(s20–24) Wie wird t-SNE für Dimensionsreduktion angewendet?","questionJa":"t-SNEがどのように次元削減に使われるかを説明しなさい（試験）","answerDe":["t-SNE reduziert hochdimensionale Daten auf 2 oder 3 Dimensionen.","Es bewahrt die lokale Struktur der Daten (Nachbarschaften).","Es wird verwendet, um komplexe Zusammenhänge visuell darzustellen.","Wird häufig im maschinellen Lernen verwendet, z.B. zur Visualisierung von Clustern."],"answerJa":["t-SNEは、高次元（例えば50次元など）のデータを2次元または3次元に変換して可視化する。","この変換により、データの局所的な構造（近い点同士の関係）が保たれるようにする。","複雑な関係性を視覚的に理解しやすくするために利用される。","クラスタ（群）の可視化など、機械学習の文脈でよく使用される。"],"explanationDe":["t-SNE dient nicht nur der Visualisierung, sondern ist auch ein Werkzeug der Dimensionsreduktion.","Dabei wird versucht, die komplexe Struktur von z.B. 100-dimensionalen Daten in 2D abzubilden.","Die Nähe zwischen Punkten im hochdimensionalen Raum wird in die Nähe auf der Fläche übertragen.","So können Cluster, Ausreißer oder Muster erkannt werden, die vorher nicht sichtbar waren.","Beispiel: Bilder von Ziffern (0–9) können mit t-SNE so dargestellt werden, dass jede Ziffer eine Gruppe in der Fläche bildet."],"explanationJa":["t-SNEは、単に可視化のためだけでなく、次元削減の有効な手段でもあります。","元の空間での類似性を保ちつつ、複雑な高次元データ（例：100次元）を2次元に変換します。","これにより、似ているデータ点同士が平面上でも近くに表示され、クラスタや外れ値が視覚的に分かるようになります。","例：手書き数字（0～9）の画像データをt-SNEで変換すると、各数字が固まって表示され、グループごとに分かれるように見えます。"],"originalSlideText":"T-DISTRIBUTED STOCHASTIC NEIGHBORHOOD EMBEDDING (T-SNE)\\n- t-SNE ist eine weitere Methode zur Dimensionsreduktion\\n- Daten werden in eine 2 dimensionale Ebene eingebettet\\n- T-SNE versucht die lokale Verteilung der Daten zu bewahren\\n- Häufig im Bereich des Maschinellen Lernens eingesetzt","explanationImage":"","questionImage":""}]');const o={class:"container py-4"},h={class:"text-center mb-5"},g={class:"display-5 fw-bold text-primary"},m={class:"fs-5 text-muted"},c={class:"text-dark"};var k={__name:"Lecture06Page",setup(e){const n=(0,s.lq)(),i=(0,a.KR)(""),k=(0,a.KR)(""),p=(0,a.KR)(""),D=(0,a.KR)([]);return(0,t.sV)(()=>{const e="lecture01",t=parseInt(n.name.split("_")[1]),r=u[e];i.value=r.title,p.value=t.toString().padStart(2,"0");const a=r.lectures.find(e=>e.number===t);k.value=a?a.title:"",D.value=d}),(e,n)=>((0,t.uX)(),(0,t.CE)("div",o,[(0,t.Lk)("div",h,[(0,t.Lk)("h1",g,(0,r.v_)(i.value),1),(0,t.Lk)("p",m,[(0,t.eW)(" Lecture "+(0,r.v_)(p.value)+": ",1),(0,t.Lk)("span",c,(0,r.v_)(k.value),1)]),n[0]||(n[0]=(0,t.Lk)("hr",{class:"w-25 mx-auto border-primary"},null,-1))]),((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(D.value,e=>((0,t.uX)(),(0,t.Wv)(l.A,{key:e.id,question:e},null,8,["question"]))),128))]))}};const p=k;var D=p}}]);
//# sourceMappingURL=697.7ece9469.js.map