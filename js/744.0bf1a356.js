"use strict";(self["webpackChunkstudy_site"]=self["webpackChunkstudy_site"]||[]).push([[744],{495:function(e,n,i){i.d(n,{A:function(){return y}});var r=i(6768),a=i(4232),t=i(144);const s={class:"card mb-4 shadow-sm"},d={class:"card-body"},l={class:"card-title"},u={class:"text-muted fst-italic"},o={key:0},m=["src"],g={key:1,class:"mt-3"},h={class:"alert alert-success"},c={key:0},b={key:1},k={class:"alert alert-info mt-2"},p={key:0},x={key:1},f={class:"mt-3"},w={key:0},z={key:1},D={key:2},S={key:3},M={key:4},T=["src"],V={class:"mt-4"},v={class:"border rounded p-3 bg-white text-secondary",style:{"white-space":"pre-wrap","font-family":"inherit"}};var K={__name:"QuestionItem",props:{question:Object},setup(e){const n=(0,t.KR)(!1);return(i,t)=>((0,r.uX)(),(0,r.CE)("div",s,[(0,r.Lk)("div",d,[(0,r.Lk)("h5",l,"Q"+(0,a.v_)(e.question.id)+": "+(0,a.v_)(e.question.questionJa),1),(0,r.Lk)("p",u,"("+(0,a.v_)(e.question.questionDe)+")",1),e.question.questionImage?((0,r.uX)(),(0,r.CE)("div",o,[(0,r.Lk)("img",{src:`./images/${e.question.questionImage}`,class:"img-fluid rounded my-2 border",alt:"question image"},null,8,m)])):(0,r.Q3)("",!0),(0,r.Lk)("button",{class:"btn btn-outline-primary mt-2",onClick:t[0]||(t[0]=e=>n.value=!n.value)},(0,a.v_)(n.value?"Hide Answer":"Check Answer"),1),n.value?((0,r.uX)(),(0,r.CE)("div",g,[(0,r.Lk)("div",h,[t[1]||(t[1]=(0,r.Lk)("strong",null,"Antwort (De):",-1)),t[2]||(t[2]=(0,r.Lk)("br",null,null,-1)),Array.isArray(e.question.answerDe)?((0,r.uX)(),(0,r.CE)("ul",c,[((0,r.uX)(!0),(0,r.CE)(r.FK,null,(0,r.pI)(e.question.answerDe,(e,n)=>((0,r.uX)(),(0,r.CE)("li",{key:n},(0,a.v_)(e),1))),128))])):((0,r.uX)(),(0,r.CE)("p",b,(0,a.v_)(e.question.answerDe),1))]),(0,r.Lk)("div",k,[t[3]||(t[3]=(0,r.Lk)("strong",null,"Ãœbersetzung (Ja):",-1)),t[4]||(t[4]=(0,r.Lk)("br",null,null,-1)),Array.isArray(e.question.answerJa)?((0,r.uX)(),(0,r.CE)("ul",p,[((0,r.uX)(!0),(0,r.CE)(r.FK,null,(0,r.pI)(e.question.answerJa,(e,n)=>((0,r.uX)(),(0,r.CE)("li",{key:n},(0,a.v_)(e),1))),128))])):((0,r.uX)(),(0,r.CE)("p",x,(0,a.v_)(e.question.answerJa),1))]),(0,r.Lk)("div",f,[t[6]||(t[6]=(0,r.Lk)("p",{class:"fw-bold mb-1"},"ErklÃ¤rung (De):",-1)),Array.isArray(e.question.explanationDe)?((0,r.uX)(),(0,r.CE)("div",w,[((0,r.uX)(!0),(0,r.CE)(r.FK,null,(0,r.pI)(e.question.explanationDe,(e,n)=>((0,r.uX)(),(0,r.CE)("p",{key:n},(0,a.v_)(e),1))),128))])):((0,r.uX)(),(0,r.CE)("p",z,(0,a.v_)(e.question.explanationDe),1)),t[7]||(t[7]=(0,r.Lk)("p",{class:"fw-bold mb-1"},"è§£èª¬ (Ja):",-1)),Array.isArray(e.question.explanationJa)?((0,r.uX)(),(0,r.CE)("div",D,[((0,r.uX)(!0),(0,r.CE)(r.FK,null,(0,r.pI)(e.question.explanationJa,(e,n)=>((0,r.uX)(),(0,r.CE)("p",{key:n},(0,a.v_)(e),1))),128))])):((0,r.uX)(),(0,r.CE)("p",S,(0,a.v_)(e.question.explanationJa),1)),e.question.explanationImage?((0,r.uX)(),(0,r.CE)("div",M,[(0,r.Lk)("img",{src:`./images/${e.question.explanationImage}`,class:"img-fluid rounded my-2 border",alt:"explanation image"},null,8,T)])):(0,r.Q3)("",!0),(0,r.Lk)("div",V,[t[5]||(t[5]=(0,r.Lk)("p",{class:"fw-bold mb-1"},"åŸæ–‡ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰æŠœç²‹ï¼‰:",-1)),(0,r.Lk)("div",v,(0,a.v_)(e.question.originalSlideText),1)])])])):(0,r.Q3)("",!0)])]))}};const E=K;var y=E},1744:function(e,n,i){i.r(n),i.d(n,{default:function(){return p}});i(8111),i(116);var r=i(6768),a=i(4232),t=i(144),s=i(1387),d=i(495),l=i(3529),u=JSON.parse('[{"id":1,"questionDe":"(s2) Was ist eine Support Vector Machine (SVM)?","questionJa":"Support Vector Machine (SVM)ã¨ã¯ã©ã®ã‚ˆã†ãªæ‰‹æ³•ã‹ï¼Ÿ","answerDe":["Eine Klassifikationsmethode, welche auf StÃ¼tzvektoren basiert","Teilt den Raum mit einer Hyperebene","Multiple Hyperebenen sind mÃ¶glich, deshalb wird die Hyperebene mit der grÃ¶ÃŸten Breite gewÃ¤hlt (Maximum margin method)"],"answerJa":["ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ï¼ˆæ”¯æŒãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«åŸºã¥ãåˆ†é¡æ‰‹æ³•","ãƒ‡ãƒ¼ã‚¿ã‚’è¶…å¹³é¢ï¼ˆHyperebeneï¼‰ã§åˆ†å‰²ã—ã¦åˆ†é¡ã™ã‚‹","è¤‡æ•°ã®è¶…å¹³é¢ãŒå¯èƒ½ãªãŸã‚ã€æœ€ã‚‚å¹…ã®åºƒã„è¶…å¹³é¢ã‚’é¸ã¶ï¼ˆæœ€å¤§ãƒãƒ¼ã‚¸ãƒ³æ³•ï¼‰"],"explanationDe":["Die Support Vector Machine (SVM) ist eine hÃ¤ufig verwendete Methode zur Klassifikation, bei der es darum geht, zwei oder mehr Klassen (zum Beispiel zwei Arten von Objekten) mÃ¶glichst optimal voneinander zu trennen. Die Trennung geschieht mithilfe einer sogenannten Hyperebene. Im zweidimensionalen Raum (wie in der Abbildung dargestellt) ist dies einfach eine gerade Linie, im dreidimensionalen Raum wÃ¤re es eine FlÃ¤che, und in hÃ¶heren Dimensionen spricht man allgemein von einer Hyperebene.","Da es oft mehrere Hyperebenen gibt, die die Datenpunkte korrekt trennen, muss entschieden werden, welche die beste ist. Die SVM entscheidet sich fÃ¼r diejenige Hyperebene, bei der der Abstand (die sogenannte â€Marginâ€œ) zu den am nÃ¤chsten liegenden Datenpunkten beider Klassen maximal ist. Diese am nÃ¤chsten liegenden Datenpunkte heiÃŸen StÃ¼tzvektoren (â€Support Vectorsâ€œ).","Ein einfaches Beispiel: Stellen wir uns vor, wir mÃ¶chten Ã„pfel und Orangen basierend auf Farbe und GrÃ¶ÃŸe voneinander unterscheiden. Die SVM sucht dann eine Trennlinie (Hyperebene), bei der der Abstand zu den nÃ¤chsten Ã„pfeln und Orangen mÃ¶glichst groÃŸ ist, um zukÃ¼nftige FrÃ¼chte mÃ¶glichst zuverlÃ¤ssig zu klassifizieren."],"explanationJa":["Support Vector Machineï¼ˆSVMï¼‰ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®åˆ†é¡å•é¡Œã§ã‚ˆãä½¿ã‚ã‚Œã‚‹æ‰‹æ³•ã®ã²ã¨ã¤ã§ã™ã€‚SVMã®ç›®çš„ã¯ã€2ã¤ï¼ˆã¾ãŸã¯ãã‚Œä»¥ä¸Šï¼‰ã®ç•°ãªã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ã«å±ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ã€æœ€ã‚‚åŠ¹ç‡çš„ã«åˆ†ã‘ã‚‹ã“ã¨ã§ã™ã€‚ãã®ãŸã‚ã«ã€ã€Œè¶…å¹³é¢ï¼ˆHyperebeneï¼‰ã€ã¨ã„ã†å¢ƒç•Œç·šã‚’åˆ©ç”¨ã—ã¾ã™ã€‚2æ¬¡å…ƒã§ã¯ç›´ç·šã€3æ¬¡å…ƒã§ã¯å¹³é¢ã€ãã‚Œä»¥ä¸Šã®æ¬¡å…ƒã§ã¯ã€Œè¶…å¹³é¢ã€ã¨ä¸€èˆ¬çš„ã«å‘¼ã³ã¾ã™ã€‚","ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é¡ã™ã‚‹éš›ã«ã€è¤‡æ•°ã®è¶…å¹³é¢ãŒãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãåˆ†é¡ã§ãã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãã®ãŸã‚SVMã§ã¯ã€è¤‡æ•°ã‚ã‚‹å€™è£œã®ä¸­ã‹ã‚‰ã€ä¸¡æ–¹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®æœ€ã‚‚è¿‘ã„ãƒ‡ãƒ¼ã‚¿ç‚¹ã¨ã®è·é›¢ï¼ˆã“ã‚Œã‚’ã€Œãƒãƒ¼ã‚¸ãƒ³ã€ã¨å‘¼ã¶ï¼‰ãŒæœ€ã‚‚å¤§ãããªã‚‹è¶…å¹³é¢ã‚’é¸ã³ã¾ã™ã€‚ã“ã®ã¨ãã®æœ€ã‚‚è¿‘ã„ä½ç½®ã«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒã€Œã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ï¼ˆæ”¯æŒãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã€ã§ã™ã€‚","å…·ä½“ä¾‹ã‚’æŒ™ã’ã¦èª¬æ˜ã™ã‚‹ã¨ã€ãƒªãƒ³ã‚´ã¨ãƒŸã‚«ãƒ³ã‚’è‰²ã¨ã‚µã‚¤ã‚ºã«åŸºã¥ã„ã¦åˆ†é¡ã—ãŸã„å ´åˆã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚SVMã¯ã€Œãƒªãƒ³ã‚´ã‚°ãƒ«ãƒ¼ãƒ—ã€ã¨ã€ŒãƒŸã‚«ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã€ã‚’æœ€ã‚‚ä½™è£•ã‚’ã‚‚ã£ã¦åˆ†ã‘ã‚‰ã‚Œã‚‹å¢ƒç•Œç·šã‚’æ¢ã—ã¾ã™ã€‚ã“ã†ã™ã‚‹ã“ã¨ã§ã€ã¾ã åˆ†é¡ã—ã¦ã„ãªã„æ–°ã—ã„æœç‰©ãŒç¾ã‚ŒãŸéš›ã«ã‚‚ã€ã‚ˆã‚Šæ­£ç¢ºã«åˆ†é¡ã§ãã‚‹å¯èƒ½æ€§ãŒé«˜ããªã‚Šã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Klassifikationsmethode, welche auf StÃ¼tzvektoren basiert\\nâ€“ Teilt dabei den Raum mit einer Hyperebene\\nâ€“ Aber: Multiple Hyperebenen sind gleich gut auf den Trainingsdaten\\nâ†’ Suche nach der Hyperebene mit der grÃ¶ÃŸten Breite\\nâ€“ Maximum margin method","explanationImage":"lecture01/lecture10_ex01.png","questionImage":""},{"id":2,"questionDe":"(s3, s4) Welche Voraussetzungen mÃ¼ssen erfÃ¼llt sein, um eine SVM als linearen Klassifikator verwenden zu kÃ¶nnen?","questionJa":"SVMã‚’ç·šå½¢åˆ†é¡å™¨ã¨ã—ã¦ä½¿ã†ã«ã¯ã©ã®ã‚ˆã†ãªæ¡ä»¶ãŒå¿…è¦ã‹ï¼Ÿ","answerDe":["Klassen mÃ¼ssen linear separierbar sein"],"answerJa":["åˆ†é¡å¯¾è±¡ã®ã‚¯ãƒ©ã‚¹ãŒç·šå½¢åˆ†é›¢å¯èƒ½ï¼ˆç›´ç·šã§åˆ†å‰²å¯èƒ½ï¼‰ã§ã‚ã‚‹ã“ã¨"],"explanationDe":["Damit eine Support Vector Machine (SVM) als linearer Klassifikator genutzt werden kann, mÃ¼ssen die Klassen linear separierbar sein.","Linear separierbar bedeutet, dass sich die Klassen durch eine gerade Linie (in 2D) oder eine Hyperebene (in hÃ¶heren Dimensionen) eindeutig voneinander trennen lassen.","In der Praxis sind Datenpunkte aber oft komplex verteilt, sodass lineare Trennung hÃ¤ufig nicht mÃ¶glich ist.","Daher verwendet man oft den sogenannten \'Kernel-Trick\'.","Dabei werden die Daten in einen hÃ¶herdimensionalen Raum Ã¼berfÃ¼hrt, in dem eine lineare Trennung leichter mÃ¶glich ist."],"explanationJa":["SVMã‚’ç·šå½¢åˆ†é¡å™¨ã¨ã—ã¦ä½¿ã†ã«ã¯ã€ã€Œãƒ‡ãƒ¼ã‚¿ãŒç·šå½¢åˆ†é›¢å¯èƒ½ï¼ˆlinear separierbarï¼‰ã€ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚","ç·šå½¢åˆ†é›¢å¯èƒ½ã¨ã¯ã€ã‚¯ãƒ©ã‚¹é–“ãŒç›´ç·šï¼ˆã¾ãŸã¯è¶…å¹³é¢ï¼‰ã§æ˜ç¢ºã«åˆ†å‰²ã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚","ã—ã‹ã—ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã¯è¤‡é›‘ã§ã€ç›´ç·šã‚„å¹³é¢ã§ç°¡å˜ã«åˆ†ã‘ã‚‰ã‚Œã‚‹ã“ã¨ã¯ã‚ã¾ã‚Šã‚ã‚Šã¾ã›ã‚“ã€‚","ãã“ã§ã€ã—ã°ã—ã°ã€Œã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ï¼ˆKernel-Trickï¼‰ã€ã¨ã„ã†æ‰‹æ³•ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚","ã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚Šé«˜æ¬¡å…ƒã®ç©ºé–“ã«å¤‰æ›ã—ã¦ã€ç·šå½¢åˆ†é›¢å¯èƒ½ãªçŠ¶æ…‹ã«ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Eine SVM ist also im simplen Fall ein linearer Klassifikator\\nâ€“ BenÃ¶tigt linear separierbare Klassen\\n   â€“ Meistens aber nicht vorhanden!\\n   â€“ Nutzung des sog. â€Kernel Tricksâ€œ","explanationImage":"","questionImage":""},{"id":3,"questionDe":"(s4) Warum optimiert die SVM zum breitesten Rand?","questionJa":"SVMãŒæœ€å¤§ãƒãƒ¼ã‚¸ãƒ³ï¼ˆæœ€ã‚‚åºƒã„å¹…ï¼‰ã‚’æŒã¤å¢ƒç•Œã‚’é¸ã¶ã®ã¯ãªãœã‹ï¼Ÿ","answerDe":["Objekte, die leicht von typischen Mustern abweichen, werden trotzdem korrekt klassifiziert"],"answerJa":["å¤šå°‘å…¸å‹çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å¤–ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ã‚‚æ­£ã—ãåˆ†é¡ã§ãã‚‹ãŸã‚"],"explanationDe":["Die SVM optimiert ihre Hyperebene, sodass der Abstand (die Margin) zu den nÃ¤chstliegenden Datenpunkten maximal wird.","Dies bedeutet, dass kleine Fehler oder Abweichungen weniger Einfluss auf die Klassifikation haben.","WÃ¤re die Grenze zu eng, kÃ¶nnten kleine Schwankungen schnell zu Fehlklassifikationen fÃ¼hren.","Ein breiter Abstand sorgt dagegen dafÃ¼r, dass das Modell robuster gegenÃ¼ber Datenabweichungen ist.","Ein konkretes Beispiel wÃ¤re eine automatische Erkennung von Spam-E-Mails: Ein groÃŸer Rand sorgt dafÃ¼r, dass ungewÃ¶hnliche, aber nicht eindeutig spamartige E-Mails trotzdem richtig klassifiziert werden."],"explanationJa":["SVMã¯å¢ƒç•Œç·šï¼ˆè¶…å¹³é¢ï¼‰ã¨æœ€ã‚‚è¿‘ã„ãƒ‡ãƒ¼ã‚¿ç‚¹ã¨ã®è·é›¢ï¼ˆãƒãƒ¼ã‚¸ãƒ³ï¼‰ãŒæœ€å¤§ã«ãªã‚‹ã‚ˆã†ã«å¢ƒç•Œç·šã‚’æ±ºå®šã—ã¾ã™ã€‚","ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ã«å¤šå°‘ã®èª¤å·®ã‚„å¤‰å‹•ãŒã‚ã£ã¦ã‚‚åˆ†é¡çµæœãŒå®‰å®šã—ã¾ã™ã€‚","ã‚‚ã—å¢ƒç•ŒãŒç‹­ã™ãã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ã«å°‘ã—ã§ã‚‚æºã‚ŒãŒã‚ã‚‹ã¨èª¤åˆ†é¡ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒé«˜ã¾ã‚Šã¾ã™ã€‚","å¢ƒç•ŒãŒåºƒã‘ã‚Œã°ã€å¤šå°‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å¤–ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã£ã¦ã‚‚æ­£ã—ãåˆ†é¡ã§ãã¾ã™ã€‚","ä¾‹ãˆã°ã€ã‚¹ãƒ‘ãƒ ãƒ¡ãƒ¼ãƒ«ã®è‡ªå‹•åˆ†é¡ã«ãŠã„ã¦ã¯ã€å¢ƒç•Œã‚’åºƒãè¨­å®šã™ã‚‹ã“ã¨ã§ã€ã‚„ã‚„é€šå¸¸ã¨ç•°ãªã‚‹ãƒ¡ãƒ¼ãƒ«ã§ã‚‚æ­£ã—ãåˆ†é¡ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Warum Optimierung zum breitesten Rand?\\n   â€“ Objekte, die leicht abweichen, werden trotzdem richtig klassifiziert","explanationImage":"","questionImage":""},{"id":4,"questionDe":"(s5, s6) Welchen Einfluss hat die Breite der Margin auf die Klassifikationsergebnisse bei SVM?","questionJa":"SVMã«ãŠã„ã¦ãƒãƒ¼ã‚¸ãƒ³ï¼ˆå¢ƒç•Œã®å¹…ï¼‰ã®åºƒã•ã¯åˆ†é¡çµæœã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ï¼Ÿ","answerDe":["Eine breitere Margin fÃ¼hrt zu robusteren Klassifikationen","Eine schmale Margin reagiert empfindlich auf kleine VerÃ¤nderungen oder AusreiÃŸer"],"answerJa":["ãƒãƒ¼ã‚¸ãƒ³ãŒåºƒã„ã¨åˆ†é¡çµæœãŒå®‰å®šã—ã‚„ã™ããªã‚‹","ãƒãƒ¼ã‚¸ãƒ³ãŒç‹­ã„ã¨å°ã•ãªå¤‰åŒ–ã‚„å¤–ã‚Œå€¤ã«æ•æ„Ÿã§ã€åˆ†é¡çµæœãŒå¤‰ã‚ã‚Šã‚„ã™ããªã‚‹"],"explanationDe":["Die Margin (Abstand zwischen Hyperebene und Datenpunkten) beeinflusst stark die StabilitÃ¤t der Klassifikation bei einer SVM.","Ist die Margin breit (wie im rechten Bild auf Folie 5), sind die Klassen eindeutig und stabil getrennt, und kleine VerÃ¤nderungen einzelner Punkte beeinflussen die Klassifikationsgrenze kaum.","Bei einer schmalen Margin (wie im linken Bild auf Folie 5) ist die Klassifikationsgrenze sehr nah an einzelnen Datenpunkten, sodass schon geringe Abweichungen oder einzelne AusreiÃŸer dazu fÃ¼hren kÃ¶nnen, dass neue Daten falsch klassifiziert werden.","Folie 6 zeigt genau diesen Effekt: Einzelne abweichende Datenpunkte (rot bzw. blau hervorgehoben) beeinflussen bei der schmalen Margin (linkes Bild) die Klassifikationsgrenze stÃ¤rker als bei der breiten Margin (rechtes Bild)."],"explanationJa":["SVMã«ãŠã‘ã‚‹ã€Œãƒãƒ¼ã‚¸ãƒ³ï¼ˆå¢ƒç•Œã®å¹…ï¼‰ã€ã¯åˆ†é¡ã®å®‰å®šæ€§ã«å¤§ããå½±éŸ¿ã—ã¾ã™ã€‚","å³å´ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰5æšç›®ï¼‰ã®ã‚ˆã†ã«ãƒãƒ¼ã‚¸ãƒ³ãŒåºƒã„å ´åˆã€ã‚¯ãƒ©ã‚¹é–“ã®è·é›¢ãŒååˆ†ã‚ã‚‹ãŸã‚ã€å°‘ã€…ãƒ‡ãƒ¼ã‚¿ã«ã°ã‚‰ã¤ããŒã‚ã£ã¦ã‚‚åˆ†é¡çµæœã¯å®‰å®šã—ã¾ã™ã€‚","ä¸€æ–¹ã§å·¦å´ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰5æšç›®ï¼‰ã®ã‚ˆã†ã«ãƒãƒ¼ã‚¸ãƒ³ãŒç‹­ã„ã¨ã€å¢ƒç•Œç·šãŒãƒ‡ãƒ¼ã‚¿ç‚¹ã®ã™ãè¿‘ãã«ã‚ã‚‹ãŸã‚ã€å°ã•ãªãƒ‡ãƒ¼ã‚¿ã®å¤‰åŒ–ã‚„å¤–ã‚Œå€¤ãŒåˆ†é¡çµæœã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚","ã‚¹ãƒ©ã‚¤ãƒ‰6æšç›®ã¯ãã®å…¸å‹çš„ãªä¾‹ã‚’ç¤ºã—ã¦ãŠã‚Šã€èµ¤è‰²ã‚„é’è‰²ã§ç¤ºã•ã‚ŒãŸä¸€éƒ¨ã®å¤–ã‚Œå€¤ãŒã€ç‹­ã„ãƒãƒ¼ã‚¸ãƒ³ï¼ˆå·¦å´ï¼‰ã®å¢ƒç•Œç·šã«ã¯å¤§ããå½±éŸ¿ã—ã¦ã„ã¾ã™ãŒã€åºƒã„ãƒãƒ¼ã‚¸ãƒ³ï¼ˆå³å´ï¼‰ã®å¢ƒç•Œç·šã«ã¯å½±éŸ¿ãŒå°‘ãªã„ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Aber:\\nâ€“ Objekte, die leicht abweichen, beeinflussen bei schmaler Margin die Klassifikationsgrenze stÃ¤rker als bei breiter Margin","explanationImage":"lecture01/lecture10_ex02.png","questionImage":""},{"id":5,"questionDe":"(s7) Wie funktioniert die Identifikation der StÃ¼tzvektoren bei einer SVM?","questionJa":"SVMã«ãŠã„ã¦ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ï¼ˆæ”¯æŒãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã¯ã©ã®ã‚ˆã†ã«ç‰¹å®šã•ã‚Œã‚‹ã®ã‹ï¼Ÿ","answerDe":["Identifikation der Datenpunkte nahe der Hyperebene","Datenpunkte am Rand ihrer Klasse","Nutzung der konvexen HÃ¼lle"],"answerJa":["è¶…å¹³é¢ä»˜è¿‘ã«ä½ç½®ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’ç‰¹å®šã™ã‚‹","å„ã‚¯ãƒ©ã‚¹ã®å¢ƒç•Œã«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’ç‰¹å®šã™ã‚‹","å‡¸åŒ…ï¼ˆã‚³ãƒ³ãƒ™ãƒƒã‚¯ã‚¹ãƒãƒ«ï¼‰ã‚’åˆ©ç”¨ã™ã‚‹"],"explanationDe":["Bei einer Support Vector Machine (SVM) sind StÃ¼tzvektoren jene Datenpunkte, die entscheidend fÃ¼r die Lage der Hyperebene sind.","Es handelt sich dabei um Punkte, die sehr nah an der Trennungslinie (Hyperebene) liegen oder genau am Rand der jeweiligen Klasse platziert sind.","Um diese wichtigen Datenpunkte zu finden, nutzt man das Konzept der konvexen HÃ¼lle (eine Art minimaler \'HÃ¼lle\' um eine Klasse von Datenpunkten).","Dabei sucht man gezielt nach denjenigen Punkten auf den konvexen HÃ¼llen der Klassen, die am dichtesten beieinander liegen.","Diese Punkte bestimmen letztendlich die optimale Lage der Hyperebene."],"explanationJa":["Support Vector Machine (SVM)ã«ãŠã„ã¦ã€ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ï¼ˆæ”¯æŒãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã¨ã¯åˆ†é¡ã®å¢ƒç•Œã‚’æ±ºã‚ã‚‹éš›ã«ç‰¹ã«é‡è¦ãªãƒ‡ãƒ¼ã‚¿ç‚¹ã§ã™ã€‚","å…·ä½“çš„ã«ã¯ã€åˆ†é¡ã®å¢ƒç•Œç·šï¼ˆè¶…å¹³é¢ï¼‰ã«éå¸¸ã«è¿‘ãã«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚„ã€å„ã‚¯ãƒ©ã‚¹ã®ç«¯ã«ä½ç½®ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã«ãªã‚Šã¾ã™ã€‚","ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã‚’ç‰¹å®šã™ã‚‹éš›ã«ã¯ã€ã€Œå‡¸åŒ…ï¼ˆã¨ã¤ã»ã†ã€ã‚³ãƒ³ãƒ™ãƒƒã‚¯ã‚¹ãƒãƒ«ï¼‰ã€ã¨ã„ã†è€ƒãˆæ–¹ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚","å‡¸åŒ…ã¨ã¯ã€ãƒ‡ãƒ¼ã‚¿ç¾¤ã‚’å›²ã‚€æœ€å°ã®å‡¸çŠ¶ã®é ˜åŸŸã®ã“ã¨ã§ã™ã€‚","ã“ã®å‡¸åŒ…ã®ä¸­ã§ç‰¹ã«ã‚¯ãƒ©ã‚¹é–“ã®è·é›¢ãŒè¿‘ã„ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’é¸ã³å‡ºã™ã“ã¨ã§ã€æœ€é©ãªå¢ƒç•Œç·šï¼ˆè¶…å¹³é¢ï¼‰ã®ä½ç½®ãŒæ±ºå®šã•ã‚Œã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nGenereller Ablauf:\\nâ€“ Identifikation der sog. StÃ¼tzvektoren\\nâ€“ Datenpunkte, die nah an der zu findenden Hyperebene liegen\\nâ€“ Datenpunkte, die am Rand ihrer Klasse liegen\\nâ€“ Nutzung der konvexen HÃ¼lle\\nâ€“ Was passiert, wenn die konvexen HÃ¼llen der Daten Ã¼berlappen?\\nâ€“ Berechnung der Datenpunkte auf den konvexen HÃ¼llen, die mÃ¶glichst nah bei einander liegen\\nâ€“ Es gibt immer mindestens einen StÃ¼tzvektor pro Klassen im Datensatz, meistens mehrere","explanationImage":"","questionImage":""},{"id":6,"questionDe":"(s7) Was passiert, wenn die konvexen HÃ¼llen der Klassen Ã¼berlappen?","questionJa":"å„ã‚¯ãƒ©ã‚¹ã®å‡¸åŒ…ãŒé‡ãªã‚‹å ´åˆã¯ã©ã†ãªã‚‹ã‹ï¼Ÿ","answerDe":["Berechnung derjenigen Punkte auf den Ã¼berlappenden konvexen HÃ¼llen, die mÃ¶glichst nah beieinander liegen","Diese Punkte werden zu StÃ¼tzvektoren"],"answerJa":["é‡ãªã£ã¦ã„ã‚‹å‡¸åŒ…ä¸Šã§ã€ã‚¯ãƒ©ã‚¹é–“ã§æœ€ã‚‚è¿‘æ¥ã—ãŸãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’è¨ˆç®—ã™ã‚‹","ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã¨ãªã‚‹"],"explanationDe":["Wenn sich die konvexen HÃ¼llen (die Bereiche, welche die Klassen jeweils minimal einschlieÃŸen) zweier Klassen Ã¼berlappen, bedeutet dies, dass die Klassen nicht eindeutig trennbar sind.","In diesem Fall werden gezielt diejenigen Datenpunkte bestimmt, welche innerhalb der Ã¼berlappenden Bereiche am dichtesten beieinander liegen.","Diese Punkte werden dann zu entscheidenden StÃ¼tzvektoren, da sie die Grenze zwischen den Klassen maÃŸgeblich definieren.","Dadurch wird eine mÃ¶glichst optimale und stabile Trennung der Klassen erreicht, obwohl diese nicht eindeutig separierbar sind."],"explanationJa":["å„ã‚¯ãƒ©ã‚¹ã®å‡¸åŒ…ï¼ˆã‚¯ãƒ©ã‚¹ã‚’åŒ…ã‚€æœ€å°ã®å‡¸é ˜åŸŸï¼‰ãŒé‡ãªã£ã¦ã„ã‚‹å ´åˆã€ã“ã‚Œã¯ã‚¯ãƒ©ã‚¹åŒå£«ãŒã¯ã£ãã‚Šã¨ã¯åˆ†ã‘ã‚‰ã‚Œãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚","ã“ã®ã‚ˆã†ãªçŠ¶æ³ã§ã¯ã€é‡ãªã£ãŸå‡¸åŒ…ã®ä¸­ã§ã€ç•°ãªã‚‹ã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãŒæœ€ã‚‚è¿‘ãã«ä½ç½®ã™ã‚‹ç‚¹ã‚’ç‰¹å®šã—ã¾ã™ã€‚","ã“ã®ã‚¯ãƒ©ã‚¹é–“ã§æœ€ã‚‚è¿‘æ¥ã—ãŸãƒ‡ãƒ¼ã‚¿ç‚¹ãŒã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã¨ãªã‚Šã€å¢ƒç•Œç·šã‚’æ±ºå®šã™ã‚‹é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚","ã“ã†ã™ã‚‹ã“ã¨ã§ã€ã‚¯ãƒ©ã‚¹ãŒæ˜ç¢ºã«åˆ†é›¢ã§ããªã„å ´åˆã§ã‚‚å¯èƒ½ãªé™ã‚Šæœ€é©ã§å®‰å®šã—ãŸåˆ†é¡å¢ƒç•Œã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"],"originalSlideText":"Was passiert, wenn die konvexen HÃ¼llen der Daten Ã¼berlappen?\\nâ€“ Berechnung der Datenpunkte auf den konvexen HÃ¼llen, die mÃ¶glichst nah bei einander liegen","explanationImage":"","questionImage":""},{"id":7,"questionDe":"(s8) Wie hÃ¤ngen die StÃ¼tzvektoren und die optimale Hyperebene bei einer SVM zusammen?","questionJa":"SVMã«ãŠã„ã¦ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã¨æœ€é©ãªè¶…å¹³é¢ã¯ã©ã®ã‚ˆã†ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ï¼Ÿ","answerDe":["Die StÃ¼tzvektoren definieren direkt die Lage der optimalen Hyperebene","Die optimale Hyperebene hat maximalen Abstand (Margin) zu diesen StÃ¼tzvektoren"],"answerJa":["ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã¯æœ€é©ãªè¶…å¹³é¢ã®ä½ç½®ã‚’ç›´æ¥æ±ºå®šã™ã‚‹","æœ€é©ãªè¶…å¹³é¢ã¨ã¯ã€ã“ã‚Œã‚‰ã®ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã‹ã‚‰æœ€å¤§ã®è·é›¢ï¼ˆãƒãƒ¼ã‚¸ãƒ³ï¼‰ãŒå–ã‚Œã‚‹ã‚ˆã†ã«é…ç½®ã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚‹"],"explanationDe":["StÃ¼tzvektoren sind bei der Support Vector Machine jene Datenpunkte, welche sich am nÃ¤chsten zur optimalen Hyperebene befinden.","Sie bestimmen direkt, wo genau die optimale Hyperebene liegt, da die optimale Grenze stets so gewÃ¤hlt wird, dass der Abstand zu diesen StÃ¼tzvektoren maximal wird.","Im Beispiel auf der Folie (s8) erkennt man, dass die optimale Hyperebene exakt zwischen den nÃ¤chstliegenden Punkten (den StÃ¼tzvektoren) beider Klassen verlÃ¤uft und dabei den grÃ¶ÃŸtmÃ¶glichen Abstand zu diesen Punkten gewÃ¤hrleistet.","Andere Datenpunkte innerhalb einer Klasse, die weiter entfernt liegen, beeinflussen die Lage der optimalen Hyperebene nicht direkt."],"explanationJa":["SVMã«ãŠã‘ã‚‹ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã¨ã¯ã€åˆ†é¡ã®å¢ƒç•Œï¼ˆè¶…å¹³é¢ï¼‰ã«æœ€ã‚‚è¿‘ã„ä½ç½®ã«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ç‚¹ã§ã™ã€‚","æœ€é©ãªè¶…å¹³é¢ã¯ã€ã“ã‚Œã‚‰ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã¨ã®è·é›¢ãŒæœ€å¤§ã«ãªã‚‹ã‚ˆã†ã«æ±ºå®šã•ã‚Œã¾ã™ã€‚","ã‚¹ãƒ©ã‚¤ãƒ‰ï¼ˆs8ï¼‰ã®ä¾‹ã‚’è¦‹ã‚‹ã¨ã€æœ€é©ãªè¶…å¹³é¢ãŒ2ã¤ã®ã‚¯ãƒ©ã‚¹ãã‚Œãã‚Œã®ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã®ã¡ã‚‡ã†ã©çœŸã‚“ä¸­ã‚’é€šã‚Šã€ãã‚Œã‚‰ã¨ã®è·é›¢ï¼ˆãƒãƒ¼ã‚¸ãƒ³ï¼‰ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«å¼•ã‹ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚","ã“ã‚Œã‚‰ã®ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ä»¥å¤–ã®ã€å¢ƒç•Œç·šã‹ã‚‰é›¢ã‚ŒãŸå ´æ‰€ã«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ç‚¹ã¯ã€æœ€é©ãªè¶…å¹³é¢ã®ä½ç½®æ±ºå®šã«ã¯ç›´æ¥å½±éŸ¿ã‚’ä¸ãˆã¾ã›ã‚“ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nmaximum margin hyperplane\\nsupport vectors","explanationImage":"lecture01/lecture10_ex03.png","questionImage":""},{"id":8,"questionDe":"(s9, s10) Wie wird die optimale Hyperebene bei einer SVM mathematisch definiert?","questionJa":"SVMã«ãŠã‘ã‚‹æœ€é©ãªè¶…å¹³é¢ã¯æ•°å­¦çš„ã«ã©ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ã‹ï¼Ÿ","answerDe":["Maximierung der Margin (2/||w||)","Hyperebene definiert durch die Gleichung: wãƒ»x + b = 0","Datenpunkte jeder Klasse erfÃ¼llen Bedingungen wãƒ»x + b â‰¥ 1 bzw. wãƒ»x + b â‰¤ -1"],"answerJa":["ãƒãƒ¼ã‚¸ãƒ³ï¼ˆ2/||w||ï¼‰ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§å®šç¾©ã•ã‚Œã‚‹","è¶…å¹³é¢ã¯ wãƒ»x + b = 0 ã®æ•°å¼ã§å®šç¾©ã•ã‚Œã‚‹","å„ã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ç‚¹ã¯ wãƒ»x + b â‰¥ 1ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ã€ã¾ãŸã¯ wãƒ»x + b â‰¤ -1ï¼ˆã‚¯ãƒ©ã‚¹2ï¼‰ã¨ã„ã†æ¡ä»¶ã‚’æº€ãŸã™"],"explanationDe":["Die optimale Hyperebene einer SVM ist mathematisch so definiert, dass die Margin (der Abstand zwischen den StÃ¼tzvektoren beider Klassen) maximal wird.","Diese Margin ergibt sich mathematisch zu 2 dividiert durch die Norm des Vektors w (2/||w||).","Die Hyperebene selbst wird durch die Gleichung wãƒ»x + b = 0 beschrieben, wobei w ein Gewichtsvektor und b ein Verschiebungsparameter ist.","FÃ¼r jeden Punkt x in der ersten Klasse gilt die Bedingung wãƒ»x + b â‰¥ 1, wÃ¤hrend fÃ¼r jeden Punkt x in der zweiten Klasse gilt wãƒ»x + b â‰¤ -1.","Diese Bedingungen stellen sicher, dass die Klassen klar getrennt sind und die Margin maximal bleibt."],"explanationJa":["SVMã«ãŠã‘ã‚‹æœ€é©ãªè¶…å¹³é¢ã¯ã€ä¸¡ã‚¯ãƒ©ã‚¹ã®ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼é–“ã®è·é›¢ï¼ˆãƒãƒ¼ã‚¸ãƒ³ï¼‰ãŒæœ€å¤§ã«ãªã‚‹ã‚ˆã†ã«æ•°å­¦çš„ã«å®šç¾©ã•ã‚Œã¾ã™ã€‚","ã“ã®ãƒãƒ¼ã‚¸ãƒ³ã¯æ•°å­¦çš„ã«ã¯ã€Œ2ã‚’ãƒ™ã‚¯ãƒˆãƒ«wã®ãƒãƒ«ãƒ (||w||)ã§å‰²ã£ãŸã‚‚ã®ï¼ˆ2/||w||ï¼‰ã€ã¨ã—ã¦è¡¨ã•ã‚Œã¾ã™ã€‚","è¶…å¹³é¢è‡ªä½“ã¯æ•°å¼ã§ wãƒ»x + b = 0 ã¨è¡¨ã•ã‚Œã€ã“ã“ã§ w ã¯é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã€b ã¯ä½ç½®ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚","1ã¤ç›®ã®ã‚¯ãƒ©ã‚¹ã«å±ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ç‚¹xã¯ wãƒ»x + b â‰¥ 1 ã‚’æº€ãŸã—ã€2ã¤ç›®ã®ã‚¯ãƒ©ã‚¹ã«å±ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ç‚¹xã¯ wãƒ»x + b â‰¤ -1 ã‚’æº€ãŸã—ã¾ã™ã€‚","ã“ã‚Œã‚‰ã®æ¡ä»¶ã«ã‚ˆã£ã¦ã€2ã¤ã®ã‚¯ãƒ©ã‚¹ãŒæ˜ç¢ºã«åˆ†é›¢ã•ã‚Œã€ã‹ã¤ãƒãƒ¼ã‚¸ãƒ³ãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹ã“ã¨ãŒä¿è¨¼ã•ã‚Œã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nmax 2/||w||\\ns.t.\\n(wÂ·x + b) â‰¥ 1, âˆ€x of class 1\\n(wÂ·x + b) â‰¤ -1, âˆ€x of class 2\\nwÂ·x + b = 0","explanationImage":"","questionImage":""},{"id":9,"questionDe":"(s11) Wie erfolgt die Berechnung der Parameter b und w bei einer SVM?","questionJa":"SVMã«ãŠã‘ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿bã¨wã¯ã©ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã‚‹ã‹ï¼Ÿ","answerDe":["Mittels einer eingeschrÃ¤nkten quadratischen Optimierungsfunktion","Verwendung von Lagrange-Multiplikatoren","Die LÃ¶sung liefert ein globales Minimum"],"answerJa":["åˆ¶ç´„ä»˜ãã®äºŒæ¬¡æœ€é©åŒ–å•é¡Œã‚’è§£ãã“ã¨ã§æ±‚ã‚ã‚‰ã‚Œã‚‹","ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã‚’ä½¿ç”¨ã™ã‚‹","å¾—ã‚‰ã‚Œã‚‹è§£ã¯å”¯ä¸€ã®å¤§åŸŸçš„æœ€å°è§£ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒŸãƒ‹ãƒãƒ ï¼‰ã¨ãªã‚‹"],"explanationDe":["Die Parameter w (Gewichtsvektor) und b (Bias oder Verschiebung) der optimalen Hyperebene bei einer Support Vector Machine werden durch eine mathematische Optimierung bestimmt.","Diese Optimierung ist eingeschrÃ¤nkt, da bestimmte Bedingungen erfÃ¼llt sein mÃ¼ssen, nÃ¤mlich dass alle Datenpunkte korrekt klassifiziert werden (Margin-Bedingungen).","Dazu nutzt man hÃ¤ufig die Methode der Lagrange-Multiplikatoren, welche solche EinschrÃ¤nkungen berÃ¼cksichtigt und in die Berechnung einflieÃŸen lÃ¤sst.","Die Optimierung ist quadratisch, was bedeutet, dass die Zielfunktion (hier: Minimierung von Â½ ||w||Â²) eine eindeutige optimale LÃ¶sung (globales Minimum) besitzt.","Dadurch ist sichergestellte, dass das Ergebnis der Optimierung immer eindeutig und optimal ist."],"explanationJa":["SVMã«ãŠã„ã¦æœ€é©ãªè¶…å¹³é¢ã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿wï¼ˆé‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã¨bï¼ˆãƒã‚¤ã‚¢ã‚¹ã€ä½ç½®èª¿æ•´ç”¨ï¼‰ã¯ã€æ•°å­¦çš„ãªæœ€é©åŒ–è¨ˆç®—ã‚’é€šã˜ã¦æ±ºå®šã•ã‚Œã¾ã™ã€‚","ã“ã®è¨ˆç®—ã¯ã€Œåˆ¶ç´„ä»˜ãæœ€é©åŒ–å•é¡Œã€ã¨ãªã£ã¦ãŠã‚Šã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãåˆ†é¡ã•ã‚Œã‚‹ã‚ˆã†ã«ä¸€å®šã®æ¡ä»¶ï¼ˆãƒãƒ¼ã‚¸ãƒ³æ¡ä»¶ï¼‰ã‚’æº€ãŸã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚","å…·ä½“çš„ã«ã¯ã€Œãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã€ã¨ã„ã†æ‰‹æ³•ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã“ã‚Œã‚‰ã®åˆ¶ç´„æ¡ä»¶ã‚’è€ƒæ…®ã—ãªãŒã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã—ã¾ã™ã€‚","ã“ã®æœ€é©åŒ–å•é¡Œã¯äºŒæ¬¡é–¢æ•°ï¼ˆÂ½||w||Â² ã‚’æœ€å°åŒ–ï¼‰ã‚’ç”¨ã„ã¦ã„ã‚‹ãŸã‚ã€æ˜ç¢ºã«ãŸã ä¸€ã¤ã®æœ€é©è§£ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒŸãƒ‹ãƒãƒ ï¼‰ã‚’æŒã¡ã¾ã™ã€‚","ã“ã‚Œã«ã‚ˆã‚Šã€è¨ˆç®—çµæœã¯å¸¸ã«å”¯ä¸€ã®æœ€é©è§£ãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ãŒä¿è¨¼ã•ã‚Œã¦ã„ã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Berechnung von b und w nun mittels einer Optimierungsfunktion\\nâ€“ Sie ist eingeschrÃ¤nkt\\n  â†’ Berechung mittels Lagrange Multiplikator\\nâ€“ Sie ist quadratisch\\n  â†’ Es existiert genau ein (globales) Minimum\\nmin Â½||w||Â²\\ns.t. yáµ¢(wÂ·xáµ¢ + b) â‰¥ 1, âˆ€xáµ¢","explanationImage":"lecture01/lecture10_ex04.png","questionImage":""},{"id":10,"questionDe":"(s12) Warum ist es sinnvoll, bei einer SVM einen Fehlergrad C zuzulassen?","questionJa":"SVMã«ãŠã„ã¦èª¤å·®è¨±å®¹åº¦Cã‚’å°å…¥ã™ã‚‹ã®ã¯ãªãœæœ‰åŠ¹ãªã®ã‹ï¼Ÿ","answerDe":["Breiteste Margin bei 100% Korrektheit kann zu schlechterer Generalisierung fÃ¼hren","C erlaubt Tradeoff zwischen Genauigkeit auf Trainingsdaten und FlexibilitÃ¤t der Hyperebene","Fehler in Trainingsdaten wirken sich weniger stark aus"],"answerJa":["è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§100%ã®æ­£ç¢ºã•ã‚’æ±‚ã‚ã‚‹ã¨æ±åŒ–æ€§èƒ½ãŒä¸‹ãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹","Cã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºã•ã¨è¶…å¹³é¢ã®è‡ªç”±åº¦ã¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¿æ•´ã™ã‚‹","è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸­ã®èª¤ã‚Šã®å½±éŸ¿ã‚’å°ã•ãã§ãã‚‹"],"explanationDe":["In der Praxis ist es oft nicht sinnvoll, eine Support Vector Machine so zu trainieren, dass sie alle Trainingsdaten zu 100% korrekt klassifiziert.","Eine zu strikte Trennung kann zu einer Ãœberanpassung (Overfitting) fÃ¼hren, bei der das Modell neue, unbekannte Daten schlecht klassifiziert.","Deshalb wird ein sogenannter Fehlergrad C eingefÃ¼hrt.","Dieser erlaubt es, einzelne Fehler im Training zu tolerieren, wenn dadurch die allgemeine Trennbarkeit (und damit die GeneralisierungsfÃ¤higkeit) verbessert wird.","C steuert also den Kompromiss zwischen Korrektheit auf den Trainingsdaten und der FlexibilitÃ¤t der Entscheidungsebene.","AuÃŸerdem reduziert C den Einfluss von falsch annotierten oder verrauschten Trainingspunkten, die ansonsten das Modell stark beeinflussen wÃ¼rden."],"explanationJa":["ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€ã™ã¹ã¦ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’100%æ­£ç¢ºã«åˆ†é¡ã™ã‚‹ã“ã¨ãŒå¸¸ã«æœ›ã¾ã—ã„ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚","ã‚ã¾ã‚Šã«ã‚‚å³å¯†ã«åˆ†é›¢ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ€§èƒ½ãŒè½ã¡ã‚‹ï¼ˆéå­¦ç¿’ï¼‰ãŠãã‚ŒãŒã‚ã‚Šã¾ã™ã€‚","ã“ã®å•é¡Œã«å¯¾å¿œã™ã‚‹ãŸã‚ã«ã€èª¤å·®è¨±å®¹åº¦Cã¨ã„ã†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°å…¥ã•ã‚Œã¾ã™ã€‚","Cã‚’ä½¿ã†ã“ã¨ã§ã€ã„ãã¤ã‹ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®èª¤åˆ†é¡ã‚’è¨±å®¹ã—ã€ãã®ä»£ã‚ã‚Šã«å…¨ä½“ã¨ã—ã¦ã‚ˆã‚Šè‰¯ã„åˆ†é¡å¢ƒç•Œã‚’ä½œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚","ã¤ã¾ã‚Šã€Cã¯ã€Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ­£ç¢ºæ€§ã€ã¨ã€Œè¶…å¹³é¢ã®æŸ”è»Ÿæ€§ï¼ˆæ±åŒ–èƒ½åŠ›ï¼‰ã€ã¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹å½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚","ã¾ãŸã€CãŒã‚ã‚‹ã“ã¨ã§ã€ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒŸã‚¹ã‚„ãƒã‚¤ã‚ºã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã®å½±éŸ¿ãŒå°ã•ããªã‚Šã€ã‚ˆã‚Šé ‘å¥ãªåˆ†é¡å™¨ãŒæ§‹ç¯‰ã§ãã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Breiteste Margin bei 100% Korrektheit kann zu schlechteren Klassifikatoren fÃ¼hren\\nâ€“ Nutzung eines erlaubten Fehlergrads C\\nâ€“ Dieser definiert den Tradeoff zwischen Korrektheit auf den Trainingsdaten und dem Freiheitsgrad fÃ¼r die Hyperebene\\nâ€“ Daten mÃ¼ssen generell linear trennbar sein\\nâ€“ Nah beieinanderliegende Daten kÃ¶nnen so besser getrennt werden\\nâ€“ Fehler in den annotierten Trainingsdaten haben einen geringeren Einfluss","explanationImage":"","questionImage":""},{"id":11,"questionDe":"(s13) Wie beeinflusst der Parameter C die Entscheidungsebene einer SVM?","questionJa":"SVMã«ãŠã‘ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Cã¯åˆ†é¡å¢ƒç•Œã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ï¼Ÿ","answerDe":["Kleine C-Werte â†’ breitere Margin, mehr Toleranz gegenÃ¼ber Fehlern","GroÃŸe C-Werte â†’ schmalere Margin, strikte Trennung der Trainingsdaten"],"answerJa":["CãŒå°ã•ã„ã¨ â†’ ãƒãƒ¼ã‚¸ãƒ³ãŒåºƒããªã‚Šã€èª¤åˆ†é¡ã‚’ã‚ã‚‹ç¨‹åº¦è¨±å®¹ã™ã‚‹","CãŒå¤§ãã„ã¨ â†’ ãƒãƒ¼ã‚¸ãƒ³ãŒç‹­ããªã‚Šã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å³å¯†ã«åˆ†é›¢ã—ã‚ˆã†ã¨ã™ã‚‹"],"explanationDe":["Der Parameter C in einer Support Vector Machine steuert den Kompromiss zwischen einer mÃ¶glichst groÃŸen Margin und einer mÃ¶glichst fehlerfreien Klassifikation der Trainingsdaten.","Bei kleinen C-Werten (z.â€¯B. C = 0.01) erlaubt das Modell einige Fehlklassifikationen, bevorzugt aber eine breite Margin und damit bessere Generalisierung.","Das sieht man in der oberen linken Grafik, wo die Trennungslinie zwar nicht alle Punkte perfekt trennt, aber robust bleibt.","Bei groÃŸen C-Werten (z.â€¯B. C = 10.00) wird das Modell sehr strikt: Es versucht, mÃ¶glichst alle Trainingsdaten korrekt zu klassifizieren, auch wenn das zu einer sehr engen Margin fÃ¼hrt.","In der unteren rechten Grafik sieht man, dass die Trennlinie sehr nah an den Daten liegt und kaum Spielraum lÃ¤sst â€“ was die Gefahr von Ãœberanpassung (Overfitting) erhÃ¶ht."],"explanationJa":["SVMã«ãŠã‘ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Cã¯ã€ã€Œãƒãƒ¼ã‚¸ãƒ³ã®åºƒã•ã€ã¨ã€Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºãªåˆ†é¡ã€ã®ã©ã¡ã‚‰ã‚’å„ªå…ˆã™ã‚‹ã‹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¿æ•´ã™ã‚‹å½¹å‰²ã‚’æŒã¡ã¾ã™ã€‚","CãŒå°ã•ã„ï¼ˆä¾‹ï¼šC = 0.01ï¼‰ã®å ´åˆã€ã„ãã¤ã‹ã®èª¤åˆ†é¡ã‚’è¨±å®¹ã™ã‚‹ä»£ã‚ã‚Šã«ãƒãƒ¼ã‚¸ãƒ³ã‚’åºƒãä¿ã¨ã†ã¨ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šæ±åŒ–æ€§èƒ½ãŒé«˜ã¾ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚","ã“ã‚Œã¯å·¦ä¸Šã®å›³ã«è¦‹ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã€å®Œå…¨ã«åˆ†é¡ã•ã‚Œã¦ã„ãªã„ç‚¹ãŒã‚ã‚‹ã‚‚ã®ã®ã€å…¨ä½“ã¨ã—ã¦å®‰å®šã—ãŸå¢ƒç•Œã«ãªã£ã¦ã„ã¾ã™ã€‚","ä¸€æ–¹ã€CãŒå¤§ãã„ï¼ˆä¾‹ï¼šC = 10.00ï¼‰ã¨ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ã™ã¹ã¦æ­£ç¢ºã«åˆ†é›¢ã—ã‚ˆã†ã¨ã™ã‚‹ãŸã‚ã€ãƒãƒ¼ã‚¸ãƒ³ãŒéå¸¸ã«ç‹­ããªã‚Šã¾ã™ã€‚","å³ä¸‹ã®å›³ã§ã¯ã€å¢ƒç•ŒãŒãƒ‡ãƒ¼ã‚¿ç‚¹ã«éå¸¸ã«è¿‘ããªã£ã¦ãŠã‚Šã€å¤–ã‚Œå€¤ã‚„ãƒã‚¤ã‚ºã«æ•æ„Ÿã«ãªã‚Šã€éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ãŒé«˜ã¾ã‚Šã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nC = 0.01, C = 0.10, C = 1.00, C = 10.00","explanationImage":"lecture01/lecture10_ex05.png","questionImage":""},{"id":12,"questionDe":"(s15) Wie wirkt sich der Fehlergrad C auf die Optimierung in der SVM aus?","questionJa":"SVMã«ãŠã‘ã‚‹èª¤å·®è¨±å®¹åº¦Cã¯æœ€é©åŒ–ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ï¼Ÿ","answerDe":["Der Algorithmus maximiert die Margin und hÃ¤lt C mÃ¶glichst klein","Er minimiert nicht direkt die Anzahl der Fehlklassifikationen","Optimiert wird stattdessen die Summe der AbstÃ¤nde (Slack-Variablen) zur Hyperebene"],"answerJa":["ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ãƒãƒ¼ã‚¸ãƒ³ã‚’æœ€å¤§åŒ–ã—ã¤ã¤ã€Cã‚’ã§ãã‚‹ã ã‘å°ã•ãä¿ã¨ã†ã¨ã™ã‚‹","èª¤åˆ†é¡ã®æ•°ãã®ã‚‚ã®ã¯æœ€é©åŒ–ã®å¯¾è±¡ã§ã¯ãªã„","ä»£ã‚ã‚Šã«ã€è¶…å¹³é¢ã‹ã‚‰ã®è·é›¢ï¼ˆã‚¹ãƒ©ãƒƒã‚¯å¤‰æ•°ï¼‰ã®åˆè¨ˆã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«æœ€é©åŒ–ã™ã‚‹"],"explanationDe":["Der Fehlergrad C in der SVM erlaubt es, dass einige Datenpunkte auf der falschen Seite der Hyperebene liegen dÃ¼rfen.","Das Ziel ist es, die Margin maximal zu halten und dabei mÃ¶glichst wenige und mÃ¶glichst kleine Verletzungen dieser Margin zuzulassen.","Anstatt die Anzahl der Fehlklassifikationen zu minimieren (was rechnerisch sehr aufwendig â€“ NP-hart â€“ wÃ¤re), wird die Summe der sogenannten Slack-Variablen Î¾áµ¢ minimiert.","Diese Variablen geben an, wie stark einzelne Punkte gegen die Trennbedingung verstoÃŸen.","Die Optimierungsfunktion wird daher angepasst: Sie kombiniert die Margin-Minimierung (Â½||w||Â²) mit der Bestrafung fÃ¼r Abweichungen (C * Î£Î¾áµ¢)."],"explanationJa":["SVMã«ãŠã‘ã‚‹èª¤å·®è¨±å®¹åº¦Cã¯ã€ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ãŒè¶…å¹³é¢ã®èª¤ã£ãŸå´ã«ä½ç½®ã—ã¦ã‚‚è¨±å®¹ã™ã‚‹ãŸã‚ã®ä»•çµ„ã¿ã§ã™ã€‚","ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç›®æ¨™ã¯ã€ãƒãƒ¼ã‚¸ãƒ³ã‚’æœ€å¤§é™ã«åºƒãä¿ã¡ã¤ã¤ã€åˆ¶ç´„é•åï¼ˆèª¤åˆ†é¡ï¼‰ã‚’ã§ãã‚‹é™ã‚Šå°‘ãªãã€å°ã•ãæŠ‘ãˆã‚‹ã“ã¨ã§ã™ã€‚","ç›´æ¥çš„ã«èª¤åˆ†é¡æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¯è¨ˆç®—ä¸Šéå¸¸ã«é›£ã—ãï¼ˆNPå›°é›£ï¼‰ã€å®Ÿç”¨çš„ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚","ãã®ãŸã‚ã€ä»£ã‚ã‚Šã«ã€Œã‚¹ãƒ©ãƒƒã‚¯å¤‰æ•°ï¼ˆÎ¾áµ¢ï¼‰ã€ã¨ã„ã†èª¤å·®ã®åº¦åˆã„ã‚’è¡¨ã™é‡ã®åˆè¨ˆï¼ˆÎ£Î¾áµ¢ï¼‰ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«æœ€é©åŒ–ãŒè¡Œã‚ã‚Œã¾ã™ã€‚","ã“ã‚Œã«ã‚ˆã‚Šæœ€é©åŒ–é–¢æ•°ã‚‚å¤‰ã‚ã‚Šã€ã€ŒÂ½||w||Â² + C * Î£Î¾áµ¢ã€ã®ã‚ˆã†ã«ã€ãƒãƒ¼ã‚¸ãƒ³ã®åºƒã•ã¨èª¤å·®ã®å¤§ãã•ã®ä¸¡æ–¹ã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãè€ƒæ…®ã™ã‚‹å½¢ã«ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Fehlergrad C\\nâ€“ Der Algorithmus versucht C bei 0 zu halten, wÃ¤hrend es die Margin maximiert.\\nâ€“ Optimiert dabei nicht die Zahl der Fehlklassifikation\\n   â€“ Dies wÃ¤re NP-hart\\nâ€“ Minimiert die Summe der AbstÃ¤nde zur Hyperebene\\nâ€“ Anpassung der EinschrÃ¤nkung\\n   yáµ¢(wÂ·xáµ¢ + b) â‰¥ 1 â€“ Î¾áµ¢, âˆ€xáµ¢\\n   Î¾áµ¢ â‰¥ 0\\nâ€“ Anpassung der Optimierungsfunktion\\n   min Â½||w||Â² + CÎ£Î¾áµ¢","explanationImage":"","questionImage":""},{"id":13,"questionDe":"(s16) Was bedeutet die duale Form der SVM und wozu dient sie?","questionJa":"SVMã®åŒå¯¾å½¢å¼ï¼ˆãƒ‡ãƒ¥ã‚¢ãƒ«å½¢å¼ï¼‰ã¨ã¯ä½•ã‚’æ„å‘³ã—ã€ä½•ã®ãŸã‚ã«ä½¿ã‚ã‚Œã‚‹ã®ã‹ï¼Ÿ","answerDe":["Der Normalenvektor w kann als Linearkombination der Trainingsbeispiele dargestellt werden","ErmÃ¶glicht genauere Beschreibung der Klassifikationsregel"],"answerJa":["æ³•ç·šãƒ™ã‚¯ãƒˆãƒ«wã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç·šå½¢çµåˆã¨ã—ã¦è¡¨ç¾ã§ãã‚‹","åˆ†é¡è¦å‰‡ï¼ˆåˆ¤åˆ¥é–¢æ•°ï¼‰ã‚’ã‚ˆã‚Šæ˜ç¢ºã«è¡¨ç¾ã§ãã‚‹"],"explanationDe":["In der dualen Formulierung der Support Vector Machine wird der zuvor berechnete Normalenvektor w nicht direkt verwendet.","Stattdessen stellt man ihn als Linearkombination der Trainingsdatenpunkte dar, die sogenannten StÃ¼tzvektoren.","Das bedeutet: w = Î£ Î±áµ¢ yáµ¢ xáµ¢, wobei Î±áµ¢ die gelernten Koeffizienten sind.","Diese Darstellung erlaubt es, die Klassifikationsregel f(x) besser zu formulieren: f(x) = sgn(Î£ Î±áµ¢ yáµ¢ (xáµ¢ Â· x) + b).","Hierbei beschreibt (xáµ¢ Â· x) das Skalarprodukt zwischen einem Trainingspunkt xáµ¢ und einem Testpunkt x.","Durch diese Formulierung lassen sich nicht nur lineare Probleme beschreiben, sondern sie bildet auch die Grundlage fÃ¼r den Kernel-Trick in nichtlinearen SVMs."],"explanationJa":["SVMã®åŒå¯¾å½¢å¼ï¼ˆãƒ‡ãƒ¥ã‚¢ãƒ«å½¢å¼ï¼‰ã§ã¯ã€åˆ†é¡ã«ä½¿ã‚ã‚Œã‚‹æ³•ç·šãƒ™ã‚¯ãƒˆãƒ«wã‚’ç›´æ¥ä½¿ã†ã®ã§ã¯ãªãã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç‚¹ï¼ˆã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ï¼‰ã®ç·šå½¢çµåˆã¨ã—ã¦è¡¨ç¾ã—ã¾ã™ã€‚","å…·ä½“çš„ã«ã¯ã€w = Î£ Î±áµ¢ yáµ¢ xáµ¢ ã®å½¢ã§è¡¨ã•ã‚Œã€Î±áµ¢ã¯å­¦ç¿’ã«ã‚ˆã£ã¦å¾—ã‚‰ã‚ŒãŸé‡ã¿ã§ã™ã€‚","ã“ã®å½¢å¼ã«ã‚ˆã‚Šã€åˆ†é¡é–¢æ•°ï¼ˆåˆ¤åˆ¥é–¢æ•°ï¼‰f(x) = sgn(Î£ Î±áµ¢ yáµ¢ (xáµ¢ãƒ»x) + b) ãŒå®šç¾©ã•ã‚Œã¾ã™ã€‚","ã“ã“ã§(xáµ¢ãƒ»x)ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿xáµ¢ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿xã¨ã®é–“ã®å†…ç©ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ç©ï¼‰ã‚’è¡¨ã—ã¾ã™ã€‚","ã“ã®è¡¨ç¾ã‚’ä½¿ã†ã“ã¨ã§ã€å˜ãªã‚‹ç·šå½¢åˆ†é¡ã ã‘ã§ãªãã€éç·šå½¢ãªSVMã«ã‚‚æ‹¡å¼µã§ãã‚‹ã€Œã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã€ã®å¿œç”¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Duale Form\\nâ€“ Der vorher berechnete Normalenvektor kann auch als Linearkombination der Trainingsbeispiele dual dargestellt werden\\nw = Î£ Î±áµ¢ yáµ¢ xáµ¢\\nâ€“ Damit kann man die Klassifikationsregel besser beschreiben:\\nf(x) = sgn(Î£ Î±áµ¢ yáµ¢ (xáµ¢, x) + b)\\nâ€“ b und alphaáµ¢ sind die erlernten Parameter\\nâ€“ yáµ¢ ist die Klasse des StÃ¼tzvektors xáµ¢\\nâ€“ x ist der Vektor einer Testinstanz\\nâ€“ Berechnung des Skalarprodukts zwischen xáµ¢ und x","explanationImage":"","questionImage":""},{"id":14,"questionDe":"(s17) Warum reicht eine lineare SVM fÃ¼r viele reale DatensÃ¤tze nicht aus?","questionJa":"ãªãœç¾å®Ÿã®å¤šãã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ç·šå½¢SVMã§ã¯ä¸ååˆ†ãªã®ã‹ï¼Ÿ","answerDe":["SVMs klassifizieren ursprÃ¼nglich nur linear trennbare Daten","Reale Daten sind meist nicht linear trennbar","Deshalb wird der Kernel-Trick eingesetzt"],"answerJa":["SVMã¯åŸºæœ¬çš„ã«ç·šå½¢åˆ†é›¢å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã—ã‹åˆ†é¡ã§ããªã„","ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã¯ã»ã¨ã‚“ã©ã®å ´åˆã€ç·šå½¢åˆ†é›¢ãŒé›£ã—ã„","ãã®ãŸã‚ã€ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ãŒç”¨ã„ã‚‰ã‚Œã‚‹"],"explanationDe":["Eine einfache lineare SVM kann nur dann gut arbeiten, wenn sich die Daten durch eine gerade Linie oder Hyperebene trennen lassen â€“ das nennt man lineare Trennbarkeit.","In der Praxis ist diese Voraussetzung selten erfÃ¼llt, weil reale Daten meist komplex verteilt sind und sich nicht durch eine einfache Linie trennen lassen.","Wie die Grafik auf der Folie zeigt, sind die roten und blauen Punkte zwar in Gruppen organisiert, aber eine einzige gerade Linie kann sie nicht perfekt trennen.","Daher verwendet man den sogenannten Kernel-Trick, um die Daten in einen hÃ¶herdimensionalen Raum abzubilden, wo eine lineare Trennung mÃ¶glich wird."],"explanationJa":["å˜ç´”ãªç·šå½¢SVMã¯ã€ãƒ‡ãƒ¼ã‚¿ãŒç›´ç·šã‚„è¶…å¹³é¢ã§åˆ†ã‘ã‚‰ã‚Œã‚‹å ´åˆã«ã—ã‹ã†ã¾ãæ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚ã“ã‚Œã‚’ã€Œç·šå½¢åˆ†é›¢å¯èƒ½ã€ã¨å‘¼ã³ã¾ã™ã€‚","ã—ã‹ã—ã€ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã¯è¤‡é›‘ãªåˆ†å¸ƒã‚’ã—ã¦ã„ã‚‹ã“ã¨ãŒå¤šãã€ç›´ç·šã§ã¯åˆ†ã‘ã‚‰ã‚Œãªã„ã‚±ãƒ¼ã‚¹ãŒå¤šãã‚ã‚Šã¾ã™ã€‚","ã‚¹ãƒ©ã‚¤ãƒ‰ã®å›³ã§ã¯ã€èµ¤ã¨é’ã®ç‚¹ãŒã¾ã¨ã¾ã£ã¦åˆ†å¸ƒã—ã¦ã„ã‚‹ã‚‚ã®ã®ã€1æœ¬ã®ç›´ç·šã§å®Œå…¨ã«åˆ†ã‘ã‚‹ã“ã¨ã¯å›°é›£ã§ã™ã€‚","ã“ã®ã‚ˆã†ãªå ´åˆã«ã¯ã€ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚Šé«˜æ¬¡å…ƒã®ç©ºé–“ã«å†™åƒã—ã€ãã®ç©ºé–“ã§ç·šå½¢ã«åˆ†é›¢ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Nicht linear trennbare Daten\\nâ€“ SVMs kÃ¶nnen bisher wie das einfache Perceptron nur lineare trennbare Daten klassifizieren!\\nâ€“ In der RealitÃ¤t sind die DatensÃ¤tze aber meistens nicht linear trennbar\\nâ†’ Nutzung des â€Kernel Tricksâ€œ","explanationImage":"","questionImage":""},{"id":15,"questionDe":"(s19, s20) Wie kann man nicht linear trennbare Daten mit SVM klassifizieren?","questionJa":"ç·šå½¢åˆ†é›¢ã§ããªã„ãƒ‡ãƒ¼ã‚¿ã‚’SVMã§ã©ã®ã‚ˆã†ã«åˆ†é¡ã™ã‚‹ã‹ï¼Ÿ","answerDe":["Transformation des Datensatzes mittels einer nicht linearen Kernelfunktion","Erweiterung des Merkmalsraums (z.â€¯B. durch xâ‚Â², xâ‚‚Â², âˆš2xâ‚xâ‚‚)","Hyperebene wird im hÃ¶herdimensionalen Raum berechnet"],"answerJa":["éç·šå½¢ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã™ã‚‹","ç‰¹å¾´ç©ºé–“ã‚’æ‹¡å¼µã™ã‚‹ï¼ˆä¾‹ï¼šxâ‚Â², xâ‚‚Â², âˆš2xâ‚xâ‚‚ ãªã©ï¼‰","é«˜æ¬¡å…ƒç©ºé–“ã§è¶…å¹³é¢ã‚’æ±‚ã‚ã¦åˆ†é¡ã™ã‚‹"],"explanationDe":["Wenn Daten nicht linear trennbar sind, kann man sie durch eine nicht lineare Transformation in einen hÃ¶herdimensionalen Raum Ã¼berfÃ¼hren.","In diesem Raum sind die Daten oft linear trennbar.","Die Transformation erfolgt durch eine sogenannte Kernelfunktion, zum Beispiel durch Berechnung neuer Merkmale wie xâ‚Â², xâ‚‚Â² und âˆš2xâ‚xâ‚‚.","Dadurch entsteht ein neuer Merkmalsraum (z.â€¯B. mit Xâ‚, Xâ‚‚, Xâ‚ƒ), in dem die Datenpunkte linear trennbar sind, wie in der 3D-Grafik auf Folie 19 gezeigt.","Nach dieser Transformation kann man wie bei einer linearen SVM eine optimale Hyperebene im neuen Raum berechnen.","Auch Testinstanzen werden in denselben Raum transformiert und dann klassifiziert."],"explanationJa":["ãƒ‡ãƒ¼ã‚¿ãŒç·šå½¢åˆ†é›¢ã§ããªã„å ´åˆã€ãã‚Œã‚’éç·šå½¢ãªå†™åƒã«ã‚ˆã£ã¦é«˜æ¬¡å…ƒç©ºé–“ã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã€ç·šå½¢ã«åˆ†ã‘ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚","ã“ã®å¤‰æ›ã¯ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã¨å‘¼ã°ã‚Œã‚‹æ‰‹æ³•ã«ã‚ˆã£ã¦è¡Œã‚ã‚Œã¾ã™ã€‚ãŸã¨ãˆã°ã€xâ‚Â²ã‚„xâ‚‚Â²ã€âˆš2xâ‚xâ‚‚ã®ã‚ˆã†ãªæ–°ã—ã„ç‰¹å¾´é‡ã‚’ä½œã‚‹ã“ã¨ã§ã€å…ƒã®ç‰¹å¾´ç©ºé–“ã‚’æ‹¡å¼µã—ã¾ã™ã€‚","ã“ã†ã—ã¦å¾—ã‚‰ã‚ŒãŸæ–°ã—ã„ç‰¹å¾´ç©ºé–“ï¼ˆXâ‚, Xâ‚‚, Xâ‚ƒãªã©ï¼‰ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ãŒç›´ç·šï¼ˆã¾ãŸã¯è¶…å¹³é¢ï¼‰ã§åˆ†å‰²å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚","ã‚¹ãƒ©ã‚¤ãƒ‰19ã®3Dã‚°ãƒ©ãƒ•ã®ã‚ˆã†ã«ã€ã‚‚ã¨ã‚‚ã¨åˆ†é›¢ã§ããªã‹ã£ãŸèµ¤ã¨é’ã®ç‚¹ãŒã€æ–°ã—ã„ç©ºé–“ã§ã¯å¹³é¢ã§åˆ†ã‘ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚","ã“ã®å¤‰æ›å¾Œã€å¾“æ¥ã®SVMã¨åŒã˜ã‚ˆã†ã«æœ€é©ãªè¶…å¹³é¢ã‚’æ±‚ã‚ã¦åˆ†é¡ã‚’è¡Œã„ã¾ã™ã€‚","æ–°ã—ã„å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚¹ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼‰ã‚‚åŒã˜æ–¹æ³•ã§å¤‰æ›ã•ã‚Œã€åˆ†é¡ã•ã‚Œã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Nicht linear trennbare Daten\\nâ€“ Transformiere den Datensatz mittels einer nicht linearen Kernel Funktion\\nâ€“ Beispiel:\\n   Xâ‚ = xâ‚Â²\\n   Xâ‚‚ = xâ‚‚Â²\\n   Xâ‚ƒ = âˆš2xâ‚xâ‚‚\\nâ€“ Nach der Transformation berechnet man die Hyperebene im hochdimensionalen aber linear trennbaren Raum\\nâ€“ Testinstanzen werden auch transformiert und dann klassifiziert","explanationImage":"","questionImage":""},{"id":16,"questionDe":"(s21) Welche Kernel-Funktionen kann man bei SVMs fÃ¼r nicht linear trennbare Daten verwenden?","questionJa":"ç·šå½¢åˆ†é›¢ã§ããªã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦SVMã§ä½¿ç”¨ã§ãã‚‹ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã«ã¯ã©ã®ã‚ˆã†ãªç¨®é¡ãŒã‚ã‚‹ã‹ï¼Ÿ","answerDe":["Homogenes Polynom: k(xáµ¢, xâ±¼) = (xáµ¢Â·xâ±¼)^d","Inhomogenes Polynom: k(xáµ¢, xâ±¼) = (xáµ¢Â·xâ±¼ + 1)^d","GauÃŸsche RBF: k(xáµ¢, xâ±¼) = exp(â€“Î³||xáµ¢ â€“ xâ±¼||Â²)"],"answerJa":["åŒæ¬¡å¤šé …å¼ã‚«ãƒ¼ãƒãƒ«: k(xáµ¢, xâ±¼) = (xáµ¢ãƒ»xâ±¼)^d","éåŒæ¬¡å¤šé …å¼ã‚«ãƒ¼ãƒãƒ«: k(xáµ¢, xâ±¼) = (xáµ¢ãƒ»xâ±¼ + 1)^d","ã‚¬ã‚¦ã‚¹RBFï¼ˆRadial Basis Functionï¼‰ã‚«ãƒ¼ãƒãƒ«: k(xáµ¢, xâ±¼) = exp(â€“Î³||xáµ¢ â€“ xâ±¼||Â²)"],"explanationDe":["Um nicht linear trennbare Daten mit einer SVM zu klassifizieren, nutzt man sogenannte Kernel-Funktionen.","Diese Funktionen ermÃ¶glichen es, die Daten in einen hÃ¶herdimensionalen Raum zu projizieren, ohne die Koordinaten explizit berechnen zu mÃ¼ssen.","Ein homogener Polynomial-Kernel hat die Form (xáµ¢Â·xâ±¼)^d. Er berÃ¼cksichtigt nur das Produkt der Eingabedaten.","Ein inhomogener Polynomial-Kernel erweitert dies mit einem konstanten Term: (xáµ¢Â·xâ±¼ + 1)^d. Dadurch kann er flexibler auf Verschiebungen reagieren.","Eine besonders beliebte Funktion ist der GauÃŸsche RBF-Kernel: exp(â€“Î³||xáµ¢ â€“ xâ±¼||Â²).","Er misst die Ã„hnlichkeit zwischen zwei Punkten und ermÃ¶glicht es, sehr komplexe Entscheidungsgrenzen zu modellieren.","Man beginnt oft mit einem kleinen Î³-Wert und erhÃ¶ht ihn schrittweise, wodurch sich die Anzahl der sogenannten Pseudodimensionen erhÃ¶ht und die Klassifikationsgenauigkeit steigen kann."],"explanationJa":["ç·šå½¢åˆ†é›¢ã§ããªã„ãƒ‡ãƒ¼ã‚¿ã‚’SVMã§åˆ†é¡ã™ã‚‹ãŸã‚ã«ã¯ã€ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã¨å‘¼ã°ã‚Œã‚‹æ‰‹æ³•ãŒç”¨ã„ã‚‰ã‚Œã¾ã™ã€‚","ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’é«˜æ¬¡å…ƒç©ºé–“ã«å†™åƒã™ã‚‹ã“ã¨ã§ã€ç·šå½¢åˆ†é›¢ã‚’å¯èƒ½ã«ã—ã¾ã™ãŒã€ãã®å†™åƒã‚’æ˜ç¤ºçš„ã«è¨ˆç®—ã›ãšã«æ¸ˆã‚€ã®ãŒç‰¹å¾´ã§ã™ã€‚","åŒæ¬¡å¤šé …å¼ã‚«ãƒ¼ãƒãƒ«ã¯ (xáµ¢ãƒ»xâ±¼)^d ã¨ã„ã†å½¢ã§ã€å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«é–“ã®å†…ç©ã‚’dä¹—ã—ãŸã‚‚ã®ã§ã™ã€‚","éåŒæ¬¡å¤šé …å¼ã‚«ãƒ¼ãƒãƒ«ã¯ (xáµ¢ãƒ»xâ±¼ + 1)^d ã®ã‚ˆã†ã«ã€å®šæ•°é …1ã‚’åŠ ãˆã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠæŸ”è»Ÿã«ãƒ‡ãƒ¼ã‚¿ã®ä½ç½®ãšã‚Œã«å¯¾å¿œã§ãã¾ã™ã€‚","ç‰¹ã«äººæ°—ãªã®ãŒã‚¬ã‚¦ã‚¹RBFï¼ˆRadial Basis Functionï¼‰ã‚«ãƒ¼ãƒãƒ«ã§ã€å¼ã¯ exp(â€“Î³||xáµ¢ â€“ xâ±¼||Â²) ã§ã™ã€‚","ã“ã‚Œã¯2ç‚¹é–“ã®è·é›¢ã«åŸºã¥ã„ã¦é¡ä¼¼åº¦ã‚’æ¸¬ã‚‹ã‚‚ã®ã§ã€éå¸¸ã«è¤‡é›‘ãªå¢ƒç•Œã‚‚è¡¨ç¾å¯èƒ½ã§ã™ã€‚","é€šå¸¸ã€Î³ï¼ˆã‚¬ãƒ³ãƒï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å°ã•ãªå€¤ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã€å¾ã€…ã«å¢—ã‚„ã™ã“ã¨ã§åˆ†é¡å™¨ã®è¡¨ç¾åŠ›ï¼ˆæ“¬ä¼¼çš„ãªæ¬¡å…ƒæ•°ï¼‰ã¨ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Nicht linear trennbare Daten\\nâ€“ Es gibt verschiedene Kernel zum Transformieren\\n   â€“ Homogen Polynomial  k(xáµ¢, xâ±¼) = (xáµ¢Â·xâ±¼)^d\\n   â€“ Inhomogen Polynomial  k(xáµ¢, xâ±¼) = (xáµ¢Â·xâ±¼ + 1)^d\\nâ€“ GauÃŸsche radiale Basisfunktion  k(xáµ¢, xâ±¼) = exp(â€“Î³||xáµ¢ â€“ xâ±¼||Â²)\\nâ€“ Starte mit einem niedrigen Parameter und erhÃ¶he die Anzahl an Pseudodimensionen, bis die Genauigkeit des Klassifikators steigt","explanationImage":"","questionImage":""},{"id":17,"questionDe":"(s24) Wie hilft der Kernel-Trick, die RechenkomplexitÃ¤t bei nicht linear trennbaren Daten zu reduzieren?","questionJa":"ç·šå½¢åˆ†é›¢ã§ããªã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã¯ã©ã®ã‚ˆã†ã«è¨ˆç®—é‡ã‚’å‰Šæ¸›ã™ã‚‹ã®ã‹ï¼Ÿ","answerDe":["Berechnung des Skalarprodukts im Ursprungsraum","AnschlieÃŸendes Potenzieren ersetzt aufwendige Rechenoperationen im hÃ¶herdimensionalen Raum","Nur 4 Rechenschritte nÃ¶tig: 2 Multiplikationen, 1 Addition, 1 Potenzierung"],"answerJa":["ã‚‚ã¨ã®ç©ºé–“ã§å†…ç©ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ç©ï¼‰ã‚’è¨ˆç®—ã™ã‚‹","ãã®å¾Œã«ã¹ãä¹—ã™ã‚‹ã“ã¨ã§ã€é«˜æ¬¡å…ƒç©ºé–“ã§ã®è¤‡é›‘ãªæ¼”ç®—ã‚’çœç•¥ã§ãã‚‹","å¿…è¦ãªæ¼”ç®—ã¯ã‚ãšã‹4å›ï¼šæ›ã‘ç®—2å›ã€è¶³ã—ç®—1å›ã€2ä¹—1å›"],"explanationDe":["Der Kernel-Trick ermÃ¶glicht es, nicht linear trennbare Daten so zu behandeln, als ob sie in einem hÃ¶herdimensionalen Raum linear trennbar wÃ¤ren â€“ und das, ohne die Daten tatsÃ¤chlich in diesen Raum zu transformieren.","Ein Beispiel dafÃ¼r ist der in der Folie gezeigte Polynomial-Kernel: K(xáµ¢, xâ±¼) = (xáµ¢Â·xâ±¼)Â².","Man berechnet zunÃ¤chst das Skalarprodukt der beiden Vektoren im Ursprungsraum, also xáµ¢Â·xâ±¼.","AnschlieÃŸend quadriert man das Ergebnis â€“ diese einfache Rechenweise ersetzt das explizite Rechnen in einem 3-dimensionalen Raum, wie es die Umformungen (1)â€“(4) zeigen.","Dadurch spart man viele Rechenschritte: Statt mehrere Komponenten zu berechnen und aufsummieren, benÃ¶tigt man nur 4 einfache Operationen.","Dies macht SVMs mit Kernel effizient und praktisch anwendbar, auch bei komplexen nichtlinearen Problemen."],"explanationJa":["ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã¯ã€ç·šå½¢åˆ†é›¢ã§ããªã„ãƒ‡ãƒ¼ã‚¿ã‚’ã€å®Ÿéš›ã«é«˜æ¬¡å…ƒç©ºé–“ã«å¤‰æ›ã›ãšã«ã€ã¾ã‚‹ã§é«˜æ¬¡å…ƒã§ç·šå½¢åˆ†é›¢ã§ãã‚‹ã‹ã®ã‚ˆã†ã«æ‰±ã†æ–¹æ³•ã§ã™ã€‚","ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã¯ãã®ä¸€ä¾‹ã¨ã—ã¦ã€2æ¬¡ã®å¤šé …å¼ã‚«ãƒ¼ãƒãƒ«ï¼šK(xáµ¢, xâ±¼) = (xáµ¢ãƒ»xâ±¼)Â² ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚","ã¾ãšå…ƒã®ç©ºé–“ã§2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã®å†…ç©ï¼ˆxáµ¢ãƒ»xâ±¼ï¼‰ã‚’è¨ˆç®—ã—ã€ãã‚Œã‚’2ä¹—ã™ã‚‹ã ã‘ã§ã€é«˜æ¬¡å…ƒç©ºé–“ã§ã®è¤‡é›‘ãªè¨ˆç®—ã‚’ä»£æ›¿ã§ãã¾ã™ã€‚","ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã‚ã‚‹å¼å¤‰å½¢ï¼ˆ1ï¼‰ï½ï¼ˆ4ï¼‰ã§ã¯ã€å®Ÿéš›ã«å¤‰æ›ã•ã‚ŒãŸç©ºé–“ã§ã®ç‰¹å¾´é‡ã®è¨ˆç®—ãŒçœç•¥ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚","ã“ã®æ–¹æ³•ã§ã¯ã€æ›ã‘ç®—2å›ã€è¶³ã—ç®—1å›ã€2ä¹—1å›ã®è¨ˆ4å›ã®æ¼”ç®—ã®ã¿ã§æ¸ˆã¿ã€éå¸¸ã«åŠ¹ç‡çš„ã§ã™ã€‚","ã“ã®ã‚ˆã†ã«ã€ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã¯SVMã‚’éç·šå½¢å•é¡Œã«ã‚‚é©ç”¨å¯èƒ½ã«ã—ã€ã‹ã¤è¨ˆç®—åŠ¹ç‡ã‚‚å¤§ããå‘ä¸Šã•ã›ã‚‹é‡è¦ãªæŠ€è¡“ã§ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Nicht linear trennbare Daten\\nâ€“ Es geht aber besser:\\n  K(xáµ¢, xâ±¼) = (xáµ¢ Â· xâ±¼)Â²\\nâ€“ Man berechnet das Skalarprodukt im Ursprungsraum und potenziert dieses mit der Anzahl der Dimensionen des transformierten Raums\\nâ€“ Damit benÃ¶tigt man fÃ¼r das Beispiel nur noch 2 Multiplikationen fÃ¼r das Skalarprodukt und eine weitere fÃ¼r die Quadrierung\\nâ€“ Eine Addition im Skalarprodukt\\nâ†’ 4 Operationen","explanationImage":"","questionImage":""},{"id":18,"questionDe":"(s24) Was zeigt das Rechenbeispiel zum Kernel K(xáµ¢, xâ±¼) = (xáµ¢ Â· xâ±¼)Â²?","questionJa":"K(xáµ¢, xâ±¼) = (xáµ¢ãƒ»xâ±¼)Â² ã¨ã„ã†ã‚«ãƒ¼ãƒãƒ«ã®è¨ˆç®—ä¾‹ã¯ä½•ã‚’ç¤ºã—ã¦ã„ã‚‹ã‹ï¼Ÿ","answerDe":["Das Skalarprodukt im Ursprungsraum genÃ¼gt fÃ¼r die Berechnung","Es sind nur 4 einfache Operationen nÃ¶tig","Keine explizite Transformation in den hÃ¶heren Raum notwendig"],"answerJa":["å…ƒã®ç©ºé–“ã§ã®ã‚¹ã‚«ãƒ©ãƒ¼ç©ã®ã¿ã§è¨ˆç®—ã§ãã‚‹","å¿…è¦ãªæ¼”ç®—ã¯ã‚ãšã‹4ã¤","é«˜æ¬¡å…ƒç©ºé–“ã¸ã®å¤‰æ›ã‚’æ˜ç¤ºçš„ã«è¡Œã†å¿…è¦ãŒãªã„"],"explanationDe":["Das Rechenbeispiel auf der Folie zeigt, wie man durch die Verwendung des Kernel-Tricks komplexe Berechnungen in hÃ¶heren Dimensionen vermeiden kann.","Statt die Daten explizit in einen Raum mit drei Dimensionen zu transformieren, reicht es aus, das Skalarprodukt xáµ¢Â·xâ±¼ im Ursprungsraum zu berechnen und anschlieÃŸend zu quadrieren.","Die Schritte (1) bis (4) zeigen, dass dies rechnerisch gleichwertig zu einer vollstÃ¤ndigen Transformation ist.","So spart man Berechnungen: Man benÃ¶tigt nur 2 Multiplikationen, 1 Addition und 1 Potenzierung â€“ also 4 Operationen insgesamt.","Diese Technik macht die Anwendung von nichtlinearen SVMs deutlich effizienter und schneller."],"explanationJa":["ã‚¹ãƒ©ã‚¤ãƒ‰ã®è¨ˆç®—ä¾‹ã¯ã€ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã«ã‚ˆã£ã¦é«˜æ¬¡å…ƒç©ºé–“ã§ã®è¤‡é›‘ãªè¨ˆç®—ã‚’å›é¿ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚","æœ¬æ¥ãªã‚‰3æ¬¡å…ƒã«å¤‰æ›ã—ã¦ã‹ã‚‰ã‚¹ã‚«ãƒ©ãƒ¼ç©ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€å…ƒã®ç©ºé–“ã§ã®ã‚¹ã‚«ãƒ©ãƒ¼ç©ï¼ˆxáµ¢ãƒ»xâ±¼ï¼‰ã‚’è¨ˆç®—ã—ã¦2ä¹—ã™ã‚‹ã ã‘ã§åŒã˜çµæœãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚","æ‰‹é †ï¼ˆ1ï¼‰ã€œï¼ˆ4ï¼‰ã¯ã€ã“ã‚ŒãŒæ•°å¼çš„ã«æ­£å½“ã§ã‚ã‚‹ã“ã¨ã‚’æ®µéšçš„ã«ç¤ºã—ã¦ã„ã¾ã™ã€‚","ã“ã®æ–¹æ³•ã«ã‚ˆã‚Šã€å¿…è¦ãªæ¼”ç®—ã¯æ›ã‘ç®—2å›ã€è¶³ã—ç®—1å›ã€2ä¹—1å›ã®è¨ˆ4å›ã ã‘ã§æ¸ˆã¿ã¾ã™ã€‚","ã“ã®ã‚ˆã†ã«ã€ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã¯éç·šå½¢SVMã‚’é«˜é€Ÿã‹ã¤åŠ¹ç‡çš„ã«ã™ã‚‹ãŸã‚ã®éå¸¸ã«é‡è¦ãªæ‰‹æ³•ã§ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Nicht linear trennbare Daten\\nâ€“ Es geht aber besser:\\n  K(ğ‘¥áµ¢, ğ‘¥â±¼) = (ğ‘¥áµ¢ Â· ğ‘¥â±¼)Â²\\nâ€“ Man berechnet das Skalarprodukt im Ursprungsraum und potenziert dieses mit der Anzahl der Dimensionen des transformierten Raums\\nâ€“ Damit benÃ¶tigt man fÃ¼r das Beispiel nur noch 2 Multiplikationen fÃ¼r das Skalarprodukt und eine weitere fÃ¼r die Quadrierung\\nâ€“ Eine Addition im Skalarprodukt\\nâ†’ 4 Operationen\\n\\nK(ğ‘¥áµ¢, ğ‘¥â±¼) = (ğ‘¥áµ¢ Â· ğ‘¥â±¼)Â²\\nâ€ƒâ€ƒâ€ƒâ€ƒ= (ğ‘¥áµ¢â‚ğ‘¥â±¼â‚ + ğ‘¥áµ¢â‚‚ğ‘¥â±¼â‚‚)Â²â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ(1)\\nâ€ƒâ€ƒâ€ƒâ€ƒ= ğ‘¥áµ¢â‚Â²ğ‘¥â±¼â‚Â² + ğ‘¥áµ¢â‚‚Â²ğ‘¥â±¼â‚‚Â² + 2ğ‘¥áµ¢â‚ğ‘¥áµ¢â‚‚ğ‘¥â±¼â‚ğ‘¥â±¼â‚‚â€ƒâ€ƒ(2,3)\\nâ€ƒâ€ƒâ€ƒâ€ƒ= (ğ‘¥áµ¢â‚Â², ğ‘¥áµ¢â‚‚Â², âˆš2ğ‘¥áµ¢â‚ğ‘¥áµ¢â‚‚) Â· (ğ‘¥â±¼â‚Â², ğ‘¥â±¼â‚‚Â², âˆš2ğ‘¥â±¼â‚ğ‘¥â±¼â‚‚)â€ƒ(4)","explanationImage":"","questionImage":""},{"id":19,"questionDe":"(s25) Was sind die zentralen Eigenschaften und Vorteile einer Support Vector Machine (SVM)?","questionJa":"SVMï¼ˆã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼‰ã®ä¸»ãªç‰¹å¾´ã¨åˆ©ç‚¹ã¯ä½•ã‹ï¼Ÿ","answerDe":["Optimiert die trennende Hyperebene zwischen den Klassen","Liefert stabiles Ergebnis (globales Minimum)","Kernel-Trick erlaubt Klassifikation nichtlinearer Daten","Skalarprodukt kann im Ursprungsraum berechnet werden","Optimierung beschrÃ¤nkt sich auf StÃ¼tzvektoren","Robust gegenÃ¼ber Ã„nderungen der Trainingsdaten (nur bei neuen StÃ¼tzvektoren)","Weniger anfÃ¤llig fÃ¼r AusreiÃŸer"],"answerJa":["ã‚¯ãƒ©ã‚¹é–“ã‚’åˆ†ã‘ã‚‹æœ€é©ãªè¶…å¹³é¢ã‚’è¨ˆç®—ã™ã‚‹","æœ€é©åŒ–å•é¡Œã«ã‚ˆã‚Šå®‰å®šã—ãŸçµæœï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒŸãƒ‹ãƒãƒ ï¼‰ã‚’å¾—ã‚‰ã‚Œã‚‹","ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã«ã‚ˆã‚Šéç·šå½¢ãƒ‡ãƒ¼ã‚¿ã‚‚åˆ†é¡å¯èƒ½","ã‚¹ã‚«ãƒ©ãƒ¼ç©ã¯å…ƒã®ç©ºé–“ã§è¨ˆç®—ã§ãã‚‹","æœ€é©åŒ–ã¯ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã®ã¿ã«é™å®šã•ã‚Œã‚‹ãŸã‚è¨ˆç®—ãŒç°¡å˜","è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¤‰åŒ–ã«å¯¾ã—ã¦é ‘å¥ï¼ˆã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãŒå¤‰ã‚ã‚‰ãªã‘ã‚Œã°åˆ†é¡å™¨ã¯å¤‰ã‚ã‚‰ãªã„ï¼‰","å¤–ã‚Œå€¤ã«å¯¾ã—ã¦å½±éŸ¿ã‚’å—ã‘ã«ãã„"],"explanationDe":["Die Support Vector Machine (SVM) ist ein leistungsfÃ¤higer Klassifikationsalgorithmus, der eine optimale Trennung zwischen zwei Klassen durch eine sogenannte Hyperebene findet.","Das Optimierungsproblem besitzt genau ein globales Minimum, wodurch die LÃ¶sung stabil und wiederholbar ist.","Durch den Kernel-Trick kann die SVM auch nichtlinear trennbare Daten effektiv klassifizieren, indem sie die Daten in einen hÃ¶herdimensionalen Raum abbildet.","Dabei muss das Skalarprodukt der Vektoren nicht im hÃ¶herdimensionalen Raum berechnet werden, sondern kann effizient im Ursprungsraum durchgefÃ¼hrt werden.","Die Berechnung beschrÃ¤nkt sich auf die sogenannten StÃ¼tzvektoren â€“ das sind die Datenpunkte, die der Trennlinie am nÃ¤chsten liegen.","Nur wenn sich diese StÃ¼tzvektoren Ã¤ndern, Ã¤ndert sich der Klassifikator. Ã„nderungen an anderen Daten haben keinen Einfluss.","Daher ist die SVM weniger empfindlich gegenÃ¼ber AusreiÃŸern oder verrauschten Trainingsdaten."],"explanationJa":["ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼ˆSVMï¼‰ã¯ã€2ã¤ã®ã‚¯ãƒ©ã‚¹ã‚’æœ€é©ã«åˆ†ã‘ã‚‹ãŸã‚ã®è¶…å¹³é¢ï¼ˆå¢ƒç•Œï¼‰ã‚’è¦‹ã¤ã‘ã‚‹å¼·åŠ›ãªåˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚","ã“ã®æœ€é©åŒ–å•é¡Œã¯ãŸã 1ã¤ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒŸãƒ‹ãƒãƒ ã‚’æŒã¤ãŸã‚ã€çµæœãŒå®‰å®šã—ã¦ã„ã¦å†ç¾æ€§ã‚‚é«˜ã„ã§ã™ã€‚","ã¾ãŸã€ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ç·šå½¢ã§ã¯åˆ†é›¢ã§ããªã„ã‚ˆã†ãªè¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿ã‚‚åˆ†é¡å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚","ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§ã¯é«˜æ¬¡å…ƒç©ºé–“ã§ã®ã‚¹ã‚«ãƒ©ãƒ¼ç©ã‚’ã€å…ƒã®ç©ºé–“ã§è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã€è¨ˆç®—åŠ¹ç‡ã‚‚è‰¯å¥½ã§ã™ã€‚","æœ€é©åŒ–ã®å¯¾è±¡ã¯åˆ†é¡ã«ãŠã„ã¦é‡è¦ãªç‚¹ï¼ˆã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ï¼‰ã«é™å®šã•ã‚Œã‚‹ãŸã‚ã€ç„¡é§„ãŒã‚ã‚Šã¾ã›ã‚“ã€‚","è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¤šå°‘å¤‰åŒ–ã—ã¦ã‚‚ã€ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãŒå¤‰ã‚ã‚‰ãªã‘ã‚Œã°åˆ†é¡å™¨ã¯å¤‰ã‚ã‚‰ãšã€ãƒ¢ãƒ‡ãƒ«ã¯éå¸¸ã«é ‘å¥ã§ã™ã€‚","ãã®ãŸã‚ã€å¤–ã‚Œå€¤ï¼ˆã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ¤ãƒ¼ï¼‰ã‚„ãƒã‚¤ã‚ºã«ã‚‚å¼·ã„ã¨ã„ã†ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Zusammenfassung\\nâ€“ SVM optimiert die trennende Hyperebene zwischen den Klassen\\nâ€“ Ergebnis ist stabil, das da Optimierungsproblem nur ein globales Minimum hat\\nâ€“ Kernel Trick erlaubt die Klassifikation nicht linearer Daten\\nâ€“ Skalarproduktberechnung kann im Ursprungsraum durchgefÃ¼hrt werden\\nâ€“ BeschrÃ¤nkung auf StÃ¼tzvektoren vereinfacht die Optimierung\\nâ€“ VerÃ¤nderungen der Trainingsdaten fÃ¼hren nur bei neuen StÃ¼tzvektoren zu anderen Klassifikatoren\\nâ†’ weniger anfÃ¤llig auf Outlier","explanationImage":"","questionImage":""},{"id":20,"questionDe":"(s26) Welche Eigenschaften hat das Kernel-Perceptron in der dualen Form?","questionJa":"åŒå¯¾å½¢å¼ã®ã‚«ãƒ¼ãƒãƒ«ä»˜ãPerceptronã«ã¯ã©ã®ã‚ˆã†ãªç‰¹å¾´ãŒã‚ã‚‹ã‹ï¼Ÿ","answerDe":["Verwendet Kernel-Funktion K zur Transformation der Daten","Klassifikation erfolgt durch sgn(Î£ Î±áµ¢ yáµ¢ K(xáµ¢, x))","Rechenaufwand steigt mit Anzahl der Trainingsdaten, da jedes Î±áµ¢ gespeichert werden muss"],"answerJa":["ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ã«ã‚«ãƒ¼ãƒãƒ«é–¢æ•°Kã‚’ä½¿ç”¨ã™ã‚‹","åˆ†é¡ã¯ sgn(Î£ Î±áµ¢ yáµ¢ K(xáµ¢, x)) ã«ã‚ˆã£ã¦è¡Œã‚ã‚Œã‚‹","å…¨ã¦ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦Î±áµ¢ãŒå¿…è¦ãªãŸã‚ã€ãƒ‡ãƒ¼ã‚¿æ•°ãŒå¢—ãˆã‚‹ã¨è¨ˆç®—é‡ãŒå¢—åŠ ã™ã‚‹"],"explanationDe":["Das Kernel-Perceptron erweitert das klassische Perceptron, indem es eine Kernel-Funktion K verwendet, um nicht linear trennbare Daten zu klassifizieren.","Anstelle einer direkten linearen Gewichtung der Merkmale erfolgt die Klassifikation in der dualen Form Ã¼ber das Skalarprodukt im transformierten Raum.","Dies geschieht mit der Formel: sgn(Î£ Î±áµ¢ yáµ¢ K(xáµ¢, x)), wobei xáµ¢ Trainingsdaten sind, x der Eingabepunkt, yáµ¢ die Klassen und Î±áµ¢ die Lernparameter.","Die Kernel-Funktion K erlaubt eine implizite Abbildung in hÃ¶here Dimensionen, ohne diese explizit zu berechnen.","Ein Nachteil ist jedoch, dass fÃ¼r jeden Trainingspunkt ein Î±áµ¢ gespeichert werden muss.","Somit steigt der Rechenaufwand linear mit der Anzahl der Trainingsbeispiele."],"explanationJa":["ã‚«ãƒ¼ãƒãƒ«Perceptronã¯ã€é€šå¸¸ã®Perceptronã‚’æ‹¡å¼µã—ã¦ã€ã‚«ãƒ¼ãƒãƒ«é–¢æ•°Kã‚’ä½¿ã†ã“ã¨ã§éç·šå½¢ãƒ‡ãƒ¼ã‚¿ã‚‚åˆ†é¡ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚","ç‰¹å¾´é‡ã‚’ç›´æ¥é‡ã¿ä»˜ã‘ã™ã‚‹ã®ã§ã¯ãªãã€å¤‰æ›ã•ã‚ŒãŸç©ºé–“ã§ã®ã‚¹ã‚«ãƒ©ãƒ¼ç©ã«åŸºã¥ã„ã¦åˆ†é¡ã‚’è¡Œã„ã¾ã™ã€‚","å…·ä½“çš„ã«ã¯ã€åˆ†é¡é–¢æ•°ã¯ sgn(Î£ Î±áµ¢ yáµ¢ K(xáµ¢, x)) ã¨ã„ã†å½¢ã§è¡¨ã•ã‚Œã€xáµ¢ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€xã¯å…¥åŠ›ã€yáµ¢ã¯ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã€Î±áµ¢ã¯å­¦ç¿’ã§å¾—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚","ã‚«ãƒ¼ãƒãƒ«é–¢æ•°Kã«ã‚ˆã‚Šã€é«˜æ¬¡å…ƒç©ºé–“ã¸ã®å†™åƒã‚’æ˜ç¤ºçš„ã«è¨ˆç®—ã™ã‚‹ã“ã¨ãªãè¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚","ãŸã ã—ã€ã“ã®æ‰‹æ³•ã«ã¯æ¬ ç‚¹ã‚‚ã‚ã‚Šã€å…¨ã¦ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦Î±áµ¢ã‚’ä¿æŒã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿æ•°ãŒå¢—ãˆã‚‹ã¨è¨ˆç®—è² è·ã‚‚å¢—å¤§ã—ã¾ã™ã€‚","ãã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ãŒå¤šã„å ´åˆã¯è¨ˆç®—ã‚³ã‚¹ãƒˆã«æ³¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Perceptron\\nâ€“ Der Kerneltrick der SVM ist auch beim Perceptron anwendbar:\\nâ€ƒâ€ƒâ€ƒâ€ƒâˆ‘ wáµ¢xáµ¢\\nâ€“ Ist in der dualen Form:\\nâ€ƒâ€ƒâ€ƒâ€ƒsgn âˆ‘ Î±áµ¢yáµ¢K(xáµ¢, x)\\nâ€“ K ist hier die Kernelfunktion zum transformieren der Daten\\nâ€“ Rechenbedarf steigt mit der Anzahl der Trainingsdaten, da jeder Datensatz ein alpha hat","explanationImage":"","questionImage":""},{"id":22,"questionDe":"(s2-26) ErklÃ¤re den grundlegenden Ablauf der SVM und die Rolle der Support-Vektoren.","questionJa":"(è©¦é¨“é¡é¡Œ)SVMã®åŸºæœ¬çš„ãªæµã‚Œã‚’èª¬æ˜ã—ã€ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãŒã©ã®ã‚ˆã†ãªå½¹å‰²ã‚’æœãŸã™ã‹ã‚’è¿°ã¹ã‚ˆã€‚","answerDe":["SVM sucht eine Hyperebene, die Klassen optimal trennt","Ziel ist die maximale Margin zwischen den Klassen","Nur Support-Vektoren bestimmen die Trennlinie","Kernelfunktion erlaubt nichtlineare Trennung"],"answerJa":["SVMã¯ã‚¯ãƒ©ã‚¹ã‚’æœ€ã‚‚ã‚ˆãåˆ†ã‘ã‚‹è¶…å¹³é¢ï¼ˆæ±ºå®šå¢ƒç•Œï¼‰ã‚’æ±‚ã‚ã‚‹","ç›®çš„ã¯ã‚¯ãƒ©ã‚¹é–“ã®ãƒãƒ¼ã‚¸ãƒ³ï¼ˆé–“éš”ï¼‰ã‚’æœ€å¤§ã«ã™ã‚‹ã“ã¨","åˆ†é¡å¢ƒç•Œã¯ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã®ã¿ã§æ±ºå®šã•ã‚Œã‚‹","ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã«ã‚ˆã‚Šéç·šå½¢ãªåˆ†é›¢ã‚‚å¯èƒ½"],"explanationDe":["Der grundlegende Ablauf der SVM beginnt damit, dass Trainingsdaten in einem Merkmalsraum dargestellt werden.","Dann wird eine Hyperebene gesucht, die die Klassen mit mÃ¶glichst groÃŸem Abstand (Margin) voneinander trennt.","Dabei gibt es oft mehrere mÃ¶gliche Trennlinien â€“ die SVM wÃ¤hlt diejenige mit der grÃ¶ÃŸten Margin, um eine robuste Klassifikation zu gewÃ¤hrleisten.","Support-Vektoren sind die Datenpunkte, die am nÃ¤chsten an der Trennlinie liegen â€“ sie bestimmen somit direkt die Lage der optimalen Hyperebene.","Alle anderen Trainingspunkte spielen bei der endgÃ¼ltigen Klassifikation keine Rolle.","Wenn die Daten nicht linear trennbar sind, kommt der sogenannte Kernel-Trick zum Einsatz.","Dieser ermÃ¶glicht es, die Daten in einen hÃ¶herdimensionalen Raum zu transformieren, wo eine lineare Trennung mÃ¶glich ist.","Die Berechnung basiert auf Skalarprodukten der transformierten Merkmale, ohne dass diese explizit berechnet werden mÃ¼ssen."],"explanationJa":["SVMï¼ˆã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼‰ã¯ã€ã¾ãšè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å¾´ç©ºé–“ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚","æ¬¡ã«ã€ã‚¯ãƒ©ã‚¹ã‚’åˆ†é›¢ã™ã‚‹ãŸã‚ã®è¶…å¹³é¢ï¼ˆå¢ƒç•Œç·šï¼‰ã‚’æ¢ã—ã¾ã™ã€‚","ã“ã®ã¨ãã€ã‚¯ãƒ©ã‚¹é–“ã®é–“éš”ï¼ˆãƒãƒ¼ã‚¸ãƒ³ï¼‰ã‚’æœ€å¤§ã«ã™ã‚‹ã‚ˆã†ãªå¢ƒç•Œã‚’é¸ã¶ã®ãŒSVMã®ç‰¹å¾´ã§ã™ã€‚","è¤‡æ•°ã®å¢ƒç•ŒãŒåŒã˜ãã‚‰ã„è‰¯ã•ãã†ãªå ´åˆã§ã‚‚ã€æœ€ã‚‚ãƒãƒ¼ã‚¸ãƒ³ãŒåºƒã„ã‚‚ã®ã‚’é¸ã¶ã“ã¨ã§æ±åŒ–æ€§èƒ½ã‚’é«˜ã‚ã¾ã™ã€‚","ã“ã“ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã™ã®ãŒã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚","ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã¨ã¯ã€å¢ƒç•Œã«æœ€ã‚‚è¿‘ã„ãƒ‡ãƒ¼ã‚¿ç‚¹ã§ã‚ã‚Šã€ã“ã‚Œã‚‰ãŒåˆ†é¡å¢ƒç•Œã‚’æ±ºå®šã—ã¾ã™ã€‚","ä»–ã®ãƒ‡ãƒ¼ã‚¿ç‚¹ã¯å¢ƒç•Œã®ä½ç½®ã«å½±éŸ¿ã‚’ä¸ãˆã¾ã›ã‚“ã€‚","ãƒ‡ãƒ¼ã‚¿ãŒç·šå½¢åˆ†é›¢ã§ããªã„å ´åˆã¯ã€ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã‚’ä½¿ã£ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚’é«˜æ¬¡å…ƒç©ºé–“ã«å†™åƒã—ã¾ã™ã€‚","ã“ã‚Œã«ã‚ˆã‚Šã€è¤‡é›‘ãªå¢ƒç•Œã§ã‚‚ç·šå½¢ãªå½¢ã§åˆ†é›¢å¯èƒ½ã¨ãªã‚Šã€SVMã¯éç·šå½¢ãªåˆ†é¡ã‚‚å®Ÿç¾ã§ãã¾ã™ã€‚"],"originalSlideText":"SUPPORT VECTOR MACHINE\\nâ€“ Klassifikationsmethode, welche auf StÃ¼tzvektoren basiert\\nâ€“ Teilt den Raum mit einer Hyperebene\\nâ€“ Suche nach der Hyperebene mit der grÃ¶ÃŸten Breite\\nâ€“ Maximum margin method\\nâ€“ Nur Support-Vektoren bestimmen die optimale Hyperebene\\nâ€“ Kernel Trick erlaubt nichtlineare Trennung","explanationImage":"","questionImage":""}]');const o={class:"container py-4"},m={class:"text-center mb-5"},g={class:"display-5 fw-bold text-primary"},h={class:"fs-5 text-muted"},c={class:"text-dark"};var b={__name:"Lecture09Page",setup(e){const n=(0,s.lq)(),i=(0,t.KR)(""),b=(0,t.KR)(""),k=(0,t.KR)(""),p=(0,t.KR)([]);return(0,r.sV)(()=>{const e="lecture01",r=parseInt(n.name.split("_")[1]),a=l[e];i.value=a.title,k.value=r.toString().padStart(2,"0");const t=a.lectures.find(e=>e.number===r);b.value=t?t.title:"",p.value=u}),(e,n)=>((0,r.uX)(),(0,r.CE)("div",o,[(0,r.Lk)("div",m,[(0,r.Lk)("h1",g,(0,a.v_)(i.value),1),(0,r.Lk)("p",h,[(0,r.eW)(" Lecture "+(0,a.v_)(k.value)+": ",1),(0,r.Lk)("span",c,(0,a.v_)(b.value),1)]),n[0]||(n[0]=(0,r.Lk)("hr",{class:"w-25 mx-auto border-primary"},null,-1))]),((0,r.uX)(!0),(0,r.CE)(r.FK,null,(0,r.pI)(p.value,e=>((0,r.uX)(),(0,r.Wv)(d.A,{key:e.id,question:e},null,8,["question"]))),128))]))}};const k=b;var p=k}}]);
//# sourceMappingURL=744.0bf1a356.js.map