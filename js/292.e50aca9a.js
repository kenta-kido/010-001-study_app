"use strict";(self["webpackChunkstudy_site"]=self["webpackChunkstudy_site"]||[]).push([[292],{495:function(e,n,i){i.d(n,{A:function(){return W}});var t=i(6768),r=i(4232),s=i(144);const a={class:"card mb-4 shadow-sm"},u={class:"card-body"},d={class:"card-title"},l={class:"text-muted fst-italic"},o={key:0},h=["src"],g={key:1,class:"mt-3"},c={class:"alert alert-success"},m={key:0},b={key:1},p={class:"alert alert-info mt-2"},f={key:0},w={key:1},k={class:"mt-3"},A={key:0},D={key:1},E={key:2},v={key:3},S={key:4},z=["src"],B={class:"mt-4"},I={class:"border rounded p-3 bg-white text-secondary",style:{"white-space":"pre-wrap","font-family":"inherit"}};var x={__name:"QuestionItem",props:{question:Object},setup(e){const n=(0,s.KR)(!1);return(i,s)=>((0,t.uX)(),(0,t.CE)("div",a,[(0,t.Lk)("div",u,[(0,t.Lk)("h5",d,"Q"+(0,r.v_)(e.question.id)+": "+(0,r.v_)(e.question.questionJa),1),(0,t.Lk)("p",l,"("+(0,r.v_)(e.question.questionDe)+")",1),e.question.questionImage?((0,t.uX)(),(0,t.CE)("div",o,[(0,t.Lk)("img",{src:`./images/${e.question.questionImage}`,class:"img-fluid rounded my-2 border",alt:"question image"},null,8,h)])):(0,t.Q3)("",!0),(0,t.Lk)("button",{class:"btn btn-outline-primary mt-2",onClick:s[0]||(s[0]=e=>n.value=!n.value)},(0,r.v_)(n.value?"Hide Answer":"Check Answer"),1),n.value?((0,t.uX)(),(0,t.CE)("div",g,[(0,t.Lk)("div",c,[s[1]||(s[1]=(0,t.Lk)("strong",null,"Antwort (De):",-1)),s[2]||(s[2]=(0,t.Lk)("br",null,null,-1)),Array.isArray(e.question.answerDe)?((0,t.uX)(),(0,t.CE)("ul",m,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.answerDe,(e,n)=>((0,t.uX)(),(0,t.CE)("li",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",b,(0,r.v_)(e.question.answerDe),1))]),(0,t.Lk)("div",p,[s[3]||(s[3]=(0,t.Lk)("strong",null,"Übersetzung (Ja):",-1)),s[4]||(s[4]=(0,t.Lk)("br",null,null,-1)),Array.isArray(e.question.answerJa)?((0,t.uX)(),(0,t.CE)("ul",f,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.answerJa,(e,n)=>((0,t.uX)(),(0,t.CE)("li",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",w,(0,r.v_)(e.question.answerJa),1))]),(0,t.Lk)("div",k,[s[6]||(s[6]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"Erklärung (De):",-1)),Array.isArray(e.question.explanationDe)?((0,t.uX)(),(0,t.CE)("div",A,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.explanationDe,(e,n)=>((0,t.uX)(),(0,t.CE)("p",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",D,(0,r.v_)(e.question.explanationDe),1)),s[7]||(s[7]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"解説 (Ja):",-1)),Array.isArray(e.question.explanationJa)?((0,t.uX)(),(0,t.CE)("div",E,[((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(e.question.explanationJa,(e,n)=>((0,t.uX)(),(0,t.CE)("p",{key:n},(0,r.v_)(e),1))),128))])):((0,t.uX)(),(0,t.CE)("p",v,(0,r.v_)(e.question.explanationJa),1)),e.question.explanationImage?((0,t.uX)(),(0,t.CE)("div",S,[(0,t.Lk)("img",{src:`./images/${e.question.explanationImage}`,class:"img-fluid rounded my-2 border",alt:"explanation image"},null,8,z)])):(0,t.Q3)("",!0),(0,t.Lk)("div",B,[s[5]||(s[5]=(0,t.Lk)("p",{class:"fw-bold mb-1"},"原文（スライド抜粋）:",-1)),(0,t.Lk)("div",I,(0,r.v_)(e.question.originalSlideText),1)])])])):(0,t.Q3)("",!0)])]))}};const J=x;var W=J},5292:function(e,n,i){i.r(n),i.d(n,{default:function(){return f}});i(8111),i(116);var t=i(6768),r=i(4232),s=i(144),a=i(1387),u=i(495),d=i(3529),l=JSON.parse('[{"id":1,"questionDe":"(s.5) Was sind Entscheidungsbäume?","questionJa":"決定木とは何か説明せよ。","answerDe":["Instanzen mit Attributen werden als Baum dargestellt","Baum ist gerichtet","Pfad des Baumes entspricht Regeln in Form von if...then","Baum prognostiziert Werte eines Attributes"],"answerJa":["属性を持つインスタンスを木構造で表現する","木は有向である","木のパスはif...then形式のルールを表す","木はある属性の値を予測する"],"explanationDe":["Ein Entscheidungsbaum ist ein grafisches Modell, das Dateninstanzen durch Fragen nach ihren Attributwerten aufteilt.","Er besteht aus Knoten (Entscheidungen) und Blättern (Vorhersagen). Dabei ist der Baum gerichtet – von der Wurzel zu den Blättern.","Jeder Pfad von der Wurzel zu einem Blatt entspricht einer Regel wie: \'Wenn Alter > 50 und Einkommen < 30.000, dann keine Kreditvergabe\'.","Der Baum gibt für eine gegebene Instanz X eine Vorhersage t ∈ {c1, ..., cn} eines Attributs."],"explanationJa":["決定木とは、データを属性に基づいて分類するための図的なモデルで、各ノードが判断条件、各葉が予測結果を示します。","この構造は根から葉に向かって一方向に進む『有向木』になっています。","例えば「年齢 > 50 かつ 年収 < 3万 → ローン不可」といったように、ルールは if...then 形式で表され、木の経路として表現されます。","あるインスタンス X に対して、c1, c2, ... の中から適切な値 t を予測するのが目的です。"],"originalSlideText":"• Instanzen mit Attributen werden als Baum dargestellt\\n• Baum ist gerichtet\\n• Ordnung der “Aussagekraft”\\n• Baum t sagt die Werte c1, … , cn eines Attributes für die Instanz X voraus\\n  • t: X ist Element von {c1, … , cn}\\n• Regeln sind in Form von if … , then … aufgestellt\\n  • Pfade des Baums kodieren die Regeln"},{"id":2,"questionDe":"(s.6) Nennen Sie zwei Anwendungsbereiche und zwei Bedingungen für die Verwendung von Entscheidungsbäumen.","questionJa":"決定木の応用分野を2つ、利用条件を2つ挙げよ。","answerDe":["Anwendungsbereiche: Klassifikation und Regression","Beispiele: Kreditwürdigkeitsprüfung, medizinische Diagnose","Bedingungen: Attribute-Wert-Paare, diskrete Attribute"],"answerJa":["応用分野：分類と回帰","例：信用評価、医療診断","条件：属性値の組み合わせでインスタンスを記述、属性は離散的である必要がある"],"explanationDe":["Entscheidungsbäume können sowohl zur Klassifikation (z. B. ob ein Kredit vergeben wird) als auch zur Regression (z. B. Höhe eines Versicherungsbetrags) genutzt werden.","Typische Anwendungsbeispiele sind etwa die Einschätzung der Kreditwürdigkeit oder die Unterstützung bei medizinischen Diagnosen.","Voraussetzung für die Anwendung ist, dass jede Instanz durch eine feste Anzahl von Attribut-Wert-Paaren beschrieben wird.","Zudem müssen diese Attribute diskret sein, also in endlich viele mögliche Werte eingeteilt sein – etwa \'Ja/Nein\', \'rot/blau/grün\' usw."],"explanationJa":["決定木は、分類（例：ローンの承認可否）にも回帰（例：保険料の予測）にも利用可能です。","具体例としては、個人の信用スコアの評価や、患者の症状に基づく診断支援などが挙げられます。","利用条件としては、各インスタンスが属性とその値のペアによって記述されていることが前提です。","さらに、属性は『離散値』でなければなりません。例えば「はい／いいえ」や「赤／青／緑」など、明確に区切られた値が必要です。"],"originalSlideText":"• Anwendungsbereiche\\n  • Klassifikation und Regression\\n  • Beispiele\\n    • Bewertung von Kreditwürdigkeit\\n    • Medizinische Diagnosen\\n• Bedingungen:\\n  • Instanzen werden durch Attribute-Werte Paare beschrieben\\n  • Attribute der Daten werden zugeordnet\\n  • Attribute müssen diskret sein"},{"id":3,"questionDe":"(s.7) Nennen Sie vier Vorteile von Entscheidungsbäumen.","questionJa":"決定木の利点を4つ挙げよ。","answerDe":["Regeln sind einfach ablesbar","Gut darstellbar","Whitebox-Verfahren","Relativ schnell berechenbar"],"answerJa":["ルールが読み取りやすい","視覚的に表現しやすい","ホワイトボックス型の手法である","比較的高速に計算可能である"],"explanationDe":["Ein großer Vorteil von Entscheidungsbäumen ist die Transparenz: Man kann leicht nachvollziehen, wie eine Entscheidung zustande kommt.","Sie lassen sich als Diagramm darstellen, was besonders hilfreich ist, um sie z. B. mit Fachleuten aus nicht-technischen Bereichen zu diskutieren.","Da sie auf logischen Regeln basieren, gelten sie als Whitebox-Verfahren – im Gegensatz zu \'Blackbox\'-Modellen wie neuronalen Netzen.","Die Lern- und Vorhersageprozesse sind in der Regel effizient und schnell berechenbar."],"explanationJa":["決定木の大きな利点は、判断の根拠が明確であることです。どのような条件でどのような結論が出たのかが明確に読み取れます。","図として表現しやすいため、専門知識のない人とも共有しやすく、説明にも向いています。","論理的なルールに基づいているため、ブラックボックス型のモデル（例：ニューラルネットワーク）とは異なり、ホワイトボックス型の手法と見なされます。","また、学習や予測の計算速度が比較的速いという点でも優れています。"],"originalSlideText":"• Warum Entscheidungsbäume?\\n  • Regeln sind einfach ablesbar\\n  • Sind gut darstellbar\\n  • Whitebox Verfahren\\n  • Relativ schnell berechenbar"},{"id":4,"questionDe":"(s.8–13) Woraus besteht der Beispiel-Datensatz zur Konstruktion eines Entscheidungsbaums?","questionJa":"決定木の構築に使用される例題データセットはどのような構成か？","answerDe":["Attribute: Aussichten, Luftfeuchtigkeit, Wind","Zielattribut: Entscheidung (ja/nein)","Jede Zeile ist eine Instanz (Beispiel)"],"answerJa":["属性：天候（Aussichten）、湿度（Luftfeuchtigkeit）、風（Wind）","目的変数：判断（Entscheidung）＝『はい』または『いいえ』","各行は個別の事例（インスタンス）を表す"],"explanationDe":["Der Datensatz enthält mehrere Instanzen, die jeweils eine Kombination von Wetterbedingungen (Sonnig, Regen, Bedeckt), Luftfeuchtigkeit (hoch, normal) und Wind (stark, schwach) darstellen.","Das Zielattribut \'Entscheidung\' gibt an, ob unter diesen Bedingungen z. B. ein Spiel stattfinden soll oder nicht.","Die Aufteilung in Spalten ermöglicht es, aus den Daten Regeln abzuleiten – z. B. \'Wenn Aussichten = Bedeckt → Entscheidung = Ja\'."],"explanationJa":["このデータセットは複数の事例（インスタンス）で構成され、それぞれの事例は『天候』『湿度』『風』の3つの条件を持っています。","最終的な判断である『決定（Entscheidung）』は、「はい（ja）」または「いいえ（nein）」の2値で表されます。","これらの条件と結果を使って、例えば『天気が曇りなら必ずプレイする』というようなルールを抽出できます。"],"originalSlideText":"Tabellen mit Attributen: Aussichten, Luftfeuchtigkeit, Wind, Entscheidung\\n– 12 Beispiele (Sonnig/Regen/Bedeckt etc.)\\n– Ziel ist es, diese Beispieldaten durch Entscheidungsbaum zu erklären","questionImage":"lecture01/lecture03_q01.png"},{"id":5,"questionDe":"(s.14) Wie sieht der resultierende Entscheidungsbaum aus, der auf dem Beispiel-Datensatz basiert?","questionJa":"この例題データに基づいて構築される決定木の構造はどのようになるか？","answerDe":["Wurzelattribut: Aussichten","Je nach Aussichten: Verzweigung nach Luftfeuchtigkeit oder Wind","Jedes Blatt gibt eine Entscheidung: ja oder nein"],"answerJa":["根ノード（最初の条件）は『天候（Aussichten）』","天候によって『湿度』または『風』を条件に分岐","葉ノードは『はい／いいえ』の結論を示す"],"explanationDe":["Der Baum beginnt mit dem Attribut \'Aussichten\'. Je nachdem, ob es sonnig, bedeckt oder regnerisch ist, folgen unterschiedliche Bedingungen.","Bei \'Sonnig\' ist z. B. die Luftfeuchtigkeit entscheidend: Hoch → Nein, Normal → Ja.","Bei \'Regen\' ist der Wind entscheidend: Stark → Nein, Schwach → Ja.","Für \'Bedeckt\' ist die Entscheidung immer Ja – der Baum kann also hier direkt enden."],"explanationJa":["この決定木は『天候（Aussichten）』から始まり、例えば『晴れ』のときは『湿度』を、また『雨』のときは『風』の強さを次の判断材料として分岐します。","『晴れで湿度が高い』場合は『いいえ』、『晴れで湿度が普通』の場合は『はい』となります。","『雨』の場合は風が強ければ『いいえ』、弱ければ『はい』というルールになります。","『曇り』のときは無条件で『はい』となるため、それ以上の分岐はありません。"],"originalSlideText":"Entscheidungsbaum:\\n– Wurzel: Aussichten\\n– Bei \'Sonnig\' → Luftfeuchtigkeit\\n– Bei \'Regen\' → Wind\\n– Bei \'Bedeckt\' → immer Ja\\n– Pfade codieren Regeln der Form \'If ..., then ...\'","explanationImage":"lecture01/lecture03_ex01.png"},{"id":6,"questionDe":"(s.15) Nennen Sie drei Herausforderungen beim Aufbau von Entscheidungsbäumen.","questionJa":"決定木の構築における課題を3つ挙げよ。","answerDe":["Attribut-Reihenfolge: Wie wählt man sie optimal?","Umgang mit kontinuierlichen Werten","Fehlerhafte und fehlende Werte"],"answerJa":["属性の順序を最適に選ぶには？","連続値の扱い","欠損値や誤ったデータの処理"],"explanationDe":["Die Reihenfolge, in der Attribute im Baum abgefragt werden, beeinflusst dessen Größe und Lesbarkeit. Ziel ist es, möglichst kurze und aussagekräftige Bäume zu erzeugen. Dazu ist eine Priorisierung der Attribute erforderlich – oft basierend auf Metriken wie Informationsgewinn.","Entscheidungsbäume arbeiten am besten mit kategorialen Werten. Bei kontinuierlichen Werten (z. B. Temperatur, Einkommen) müssen diese in sinnvolle Intervalle unterteilt werden – z. B. \'<30.000\', \'30.000–50.000\', \'>50.000\'.","Fehlende oder fehlerhafte Werte stellen ein Problem dar. Es muss entschieden werden, ob man sie ignoriert, mit Mittelwerten ersetzt oder spezielle Behandlungsknoten im Baum dafür einführt."],"explanationJa":["どの属性を先に判断基準として使うかによって、決定木のサイズや理解しやすさが大きく変わります。情報利得（Information Gain）などを使って、重要な属性から優先的に使うことが一般的です。","決定木はもともとカテゴリデータに強いですが、連続値（例：温度、収入など）を扱う場合は、それらを区間に分けて離散化する必要があります。例：『収入30万円未満』『30〜50万円』など。","データに欠損がある場合（例：ある人の年齢が不明）や、記録ミスがある場合、それらをどう扱うかが重要です。平均値で埋める、最頻値を使う、あるいは分岐条件に『不明』を設けるなどの対応があります。"],"originalSlideText":"• Herausforderungen\\n• Kann man die Reihenfolge der Attribute so wählen, dass der Baum möglichst klein wird?\\n  • Priorisierung\\n• Kontinuierliche Werte?\\n• Fehler und fehlende Werte?"},{"id":7,"questionDe":"(s.16) Erklären Sie das ID3-Verfahren zur Konstruktion eines Entscheidungsbaums.","questionJa":"決定木構築に用いられるID3アルゴリズムの仕組みを説明せよ。","answerDe":["Top-Down-Aufbau des Baums","Greedy-Auswahl des besten Attributs anhand von Informationsentropie","Rekursion auf Kindknoten"],"answerJa":["トップダウンで木を構築する","情報エントロピーに基づいて最良の属性を貪欲法で選ぶ","子ノードに対して再帰的に同様の処理を繰り返す"],"explanationDe":["Das ID3-Verfahren erstellt Entscheidungsbäume von oben nach unten. Es beginnt an der Wurzel und wählt schrittweise das jeweils \'beste\' Attribut zur Aufteilung der Daten.","Das \'beste\' Attribut wird anhand des Informationsgewinns bestimmt – also wie stark ein Attribut zur Reduktion der Unsicherheit (Entropie) beiträgt.","Für jeden möglichen Wert dieses Attributs wird ein Kindknoten erzeugt, der nur mit den entsprechenden Daten weiterarbeitet.","Dann wird der Algorithmus rekursiv auf jeden Kindknoten angewendet, bis entweder alle Attribute erschöpft sind oder eine reine Klassifikation erreicht ist."],"explanationJa":["ID3は決定木を『トップダウン（根から葉へ）』の方式で構築するアルゴリズムです。","まず『情報エントロピー』という指標を使い、どの属性がもっとも分類の不確実性を減らすかを計算し、それを『最良の属性』として選びます（貪欲法）。","選ばれた属性の各値に応じて子ノードを作成し、それぞれに対応するサブセットのデータを割り当てます。","その後、同様の処理を子ノードにも再帰的に適用し、すべての属性が使い切られるか、分類が確定できるまで木を成長させます。"],"originalSlideText":"• ID3\\n• Lernt den Baum von oben nach unten (top down)\\n• Wählt das beste Attribut zum spalten aus (greedy)\\n  • “Beste” im Sinne der Informationsentropie\\n• Rekursion auf den Kinderknoten\\n\\nPseudocode:\\n1. X ⊨ das “beste” Attribut aus den Attributen\\n2. X = “Spalt” Attribut vom derzeitigen Knoten\\n3. Erstelle für alle Werte von X Kindsknoten mit den zugehörigen Werten\\n4. Gehe zu den Kindsknoten und starte bei 1., bis keine Attribute mehr übrig sind"},{"id":8,"questionDe":"(s.17) Nennen Sie zwei allgemeine Probleme bei Entscheidungsbäumen.","questionJa":"決定木における一般的な問題点を2つ挙げよ。","answerDe":["Bleibt oft in lokalen Optima stecken","Nur für diskrete Daten geeignet"],"answerJa":["局所最適解に陥りやすい","離散データしか扱えない"],"explanationDe":["Entscheidungsbäume verwenden eine greedy-Strategie, das heißt sie treffen lokale Entscheidungen bei der Attributauswahl. Dadurch besteht die Gefahr, dass globale optimale Strukturen übersehen werden.","Zudem sind viele Entscheidungsbaumverfahren wie ID3 nur für diskrete Attribute geeignet. Kontinuierliche Merkmale wie Alter oder Einkommen müssen zuerst in Intervalle eingeteilt werden (Diskretisierung), um verwendet werden zu können."],"explanationJa":["決定木は貪欲法（greedy）を使って属性を選びます。つまり、各ステップで最も良さそうな選択をしますが、その結果として全体として最良の木（グローバル最適）を見逃すことがあります。","また、ID3のような多くの決定木手法は離散データしか扱えません。例えば年齢や収入などの連続値は、あらかじめ範囲ごとに区切ってカテゴリとして扱う必要があります（離散化）。"],"originalSlideText":"Probleme:\\n• Bleibt oft in lokalen Optima stecken\\n• Nur diskrete Daten"},{"id":9,"questionDe":"(s.17) Wie kann Overfitting bei Entscheidungsbäumen verhindert werden?","questionJa":"決定木における過学習を防ぐにはどうすればよいか？","answerDe":["Baumwachstum bei bestimmter Tiefe stoppen","Pruning des Baums: Post-Pruning und Reduced-Error-Pruning"],"answerJa":["ある深さで木の成長を止める（早期終了）","木の剪定（Post-Pruning や Reduced-Error-Pruning）を行う"],"explanationDe":["Overfitting tritt auf, wenn ein Entscheidungsbaum zu detailliert ist und sich zu sehr an die Trainingsdaten anpasst. Er kann dann bei neuen Daten schlechtere Leistungen zeigen.","Eine Methode zur Vermeidung ist das Begrenzen der Baumtiefe. So wird verhindert, dass der Baum zu sehr ins Detail geht.","Ein weiterer Ansatz ist das Pruning: Beim Post-Pruning wird der vollständig gelernte Baum nachträglich vereinfacht, indem unwichtige Knoten entfernt werden.","Beim Reduced-Error-Pruning wird geprüft, ob die Entfernung eines Teilbaums die Genauigkeit auf einem Validierungsdatensatz verschlechtert. Wenn nicht, wird der Teilbaum gelöscht."],"explanationJa":["決定木が訓練データに過剰に適合すると、テストデータ（未知のデータ）に対して性能が下がる「過学習」が発生します。","これを防ぐ一つの方法は、あらかじめ木の深さに制限を設けて、詳細すぎる分岐を避けることです。","もう一つの方法が剪定（Pruning）です。ポスト剪定（Post-Pruning）は、一度完全に木を構築してから、精度に大きく影響しない枝を取り除きます。","誤差削減剪定（Reduced-Error-Pruning）は、枝を削除しても検証データでの精度が下がらなければ、その枝を削るという実践的な方法です。"],"originalSlideText":"Overfitting:\\n• Wachsen bei bestimmter Tiefe stoppen\\n• Pruning des Baums\\n  • Post-pruning\\n  • Reduced-error pruning"},{"id":10,"questionDe":"(s.18) Was ist Entropie in der Klassifikation und wie wird sie berechnet?","questionJa":"分類問題におけるエントロピーとは何か？また、その計算方法を説明せよ。","answerDe":["Entropie misst den Informationsgehalt einer Verteilung","Für binäre Klassifikation: S = -pₓ·log₂(pₓ) - pᵧ·log₂(pᵧ)","Für mehrere Klassen: S = -∑ pᵢ·log₂(pᵢ)"],"answerJa":["エントロピーは分布の情報量を測る指標である","2クラスの場合：S = -pₓ·log₂(pₓ) - pᵧ·log₂(pᵧ)","多クラスの場合：S = -∑ pᵢ·log₂(pᵢ)"],"explanationDe":["Entropie ist ein zentrales Konzept in der Informations- und Entscheidungstheorie. Sie gibt an, wie \'ungewiss\' oder \'gemischt\' eine Verteilung ist.","In der binären Klassifikation (z. B. Ja/Nein) ist die Entropie am höchsten, wenn beide Klassen gleich verteilt sind (z. B. 50 % Ja, 50 % Nein).","Ist die Entropie niedrig (z. B. 100 % Ja, 0 % Nein), dann ist die Information klar und eindeutig – ein idealer Fall für die Klassifikation.","Für mehr als zwei Klassen wird die Entropie durch die Summe der gewichteten Informationsanteile jeder Klasse berechnet: S = -∑ pᵢ·log₂(pᵢ).","In Entscheidungsbäumen wird Entropie verwendet, um zu entscheiden, welches Attribut die besten Informationsgewinne bringt – also welches Attribut die reinste Trennung der Klassen bewirkt."],"explanationJa":["エントロピーは情報理論や意思決定理論で重要な概念で、『どれだけ情報が不確実か』を測る指標です。","例えば分類が『はい／いいえ』の2つだけで、どちらのラベルも50％ずつなら、もっとも不確実で情報量が多い状態（エントロピー最大）になります。","逆に、全てが『はい』だった場合には、予測は簡単であり、エントロピーは0になります。","2クラスの場合には、pₓとpᵧ（それぞれの割合）を使って S = -pₓ·log₂(pₓ) - pᵧ·log₂(pᵧ) で計算します。","クラスが3つ以上ある場合には、すべてのクラスの割合pᵢについて S = -∑ pᵢ·log₂(pᵢ) のように合計を取ります。","決定木では、どの属性を使えば分類が最もきれいに分かれるか（情報利得が最大になるか）を判断するために、このエントロピーを活用します。"],"originalSlideText":"Bewertungsfunktion Entropie\\n• Entropie ist ein Maß über den Informationsgehalt in den Daten\\n• Boolsche Klassifikation\\n  Entropie(S) = -pₓ·log₂(pₓ) - pᵧ·log₂(pᵧ)\\n• Für mehrere Klassen\\n  Entropie(S) = ∑ -pᵢ·log₂(pᵢ)","explanationImage":"lecture01/lecture03_ex02.png"},{"id":11,"questionDe":"(s.19) Was ist Information Gain und wie wird er berechnet?","questionJa":"情報利得とは何か？またその計算方法を説明せよ。","answerDe":["Information Gain misst, wie gut ein Attribut die Daten trennt","Er vergleicht die Entropie vor und nach dem Split","Formel: Gain(S, A) = Entropie(S) − ∑ (|Sᵥ| / |S|) · Entropie(Sᵥ)"],"answerJa":["情報利得は、属性がどれだけデータをうまく分割できるかを示す指標である","分割前後のエントロピーの差をもとに計算される","式：Gain(S, A) = Entropy(S) − ∑ (|Sᵥ| / |S|) · Entropy(Sᵥ)"],"explanationDe":["Information Gain wird in Entscheidungsbäumen wie ID3 verwendet, um das beste Attribut für die nächste Aufspaltung der Daten zu wählen.","Ein hoher Informationsgewinn bedeutet, dass ein Attribut die Daten gut trennt – d. h. die resultierenden Teilmengen enthalten möglichst reine Klassen.","Die Entropie vor dem Split beschreibt, wie ungeordnet die Gesamtdaten sind.","Nach dem Split mit einem Attribut A werden die Daten in Teilmengen Sᵥ aufgeteilt, je nachdem welchen Wert das Attribut annimmt.","Für jede dieser Teilmengen wird die Entropie berechnet und gewichtet (nach der Größe von Sᵥ).","Die Differenz zwischen der ursprünglichen Entropie und dieser gewichteten Summe ergibt den Information Gain."],"explanationJa":["情報利得（Information Gain）は、決定木のアルゴリズム（例：ID3）で、どの属性を使って分割するかを決めるために使われる評価基準です。","ある属性でデータを分割したとき、エントロピー（データの不確かさ）がどれだけ減少するかを測定します。","エントロピーが大きく減少する＝情報利得が大きいということは、その属性がデータをよく分類できていることを意味します。","計算方法は、まず全体のエントロピー Entropy(S) を求め、それぞれの値 v ごとに部分集合 Sᵥ のエントロピーを計算します。","各部分集合のエントロピーに、Sᵥ の割合（|Sᵥ| / |S|）を掛けて合計し、それを全体エントロピーから引くことで情報利得が得られます。","情報利得が最大となる属性を使って、決定木の次の分岐を構成します。"],"originalSlideText":"Information Gain\\n• Deskriptor, wie gut ein Merkmal die Menge klassifiziert\\n• Vergleicht Entropie vor und nach dem Split mit Attribut A\\n• Gibt Auskunft über das Sinken der Entropie nach dem Split\\nGain(S, A) = Entropie(S) − ∑ (|Sᵥ| / |S|) · Entropy(Sᵥ)\\nS = Trainingsbeispiele\\nA = gewähltes Attribut\\nWerte(A) = Alle Werte die A annehmen kann\\nSᵥ = Untermenge von S, in der alle Beispiele Attribut = v haben","explanationImage":"lecture01/lecture03_ex03.png"},{"id":12,"questionDe":"(s.20–21) Beschreiben Sie den Beispieldatensatz und erklären Sie die Entropie als Maß für den Informationsgehalt.","questionJa":"例として示されたデータセットの構成を説明し、エントロピーが情報量の尺度としてどう使われるかを述べよ。","answerDe":["Der Datensatz enthält 14 Beispiele mit fünf Attributen","Entropie misst den Informationsgehalt der Klassenzuordnung","Bei Gleichverteilung maximale Entropie"],"answerJa":["このデータセットは5つの属性を持つ14個の例からなる","エントロピーはクラス分布の情報量を測る指標である","クラスが均等に分布しているときエントロピーは最大"],"explanationDe":["Der Datensatz umfasst Wetterdaten mit den Attributen: Tag, Aussichten, Temperatur, Luftfeuchtigkeit, Wind und Entscheidung (ja/nein).","Die Entropie ist ein Maß für Unordnung oder Unsicherheit in den Daten. Sie zeigt an, wie gemischt die Klassenverteilung ist.","Bei einer gleichmäßigen Verteilung der Klassen (z. B. 50% ja, 50% nein) ist die Entropie am höchsten. Bei reiner Klasse (100% ja oder nein) ist sie null.","Im Beispiel beträgt die Entropie des gesamten Datensatzes: Entropy(S) = −(9/14)·log₂(9/14) − (5/14)·log₂(5/14) ≈ 0,94"],"explanationJa":["この例では、天候に関する属性（天気、気温、湿度、風）に基づいて、ある行動（たとえば外出）の判断（はい／いいえ）を行っています。全部で14件のデータがあります。","エントロピーは、分類の不確かさを示す指標であり、クラス（はい／いいえ）の割合が混在しているほど大きくなります。","例えば、はいといいえがちょうど半々なら最大となり、どちらかに偏っていればエントロピーは小さくなります。","このデータでは、9件が「はい」、5件が「いいえ」であり、全体のエントロピーは約0.94になります。"],"originalSlideText":"Tag, Aussichten, Temperatur, Luftfeuchtigkeit, Wind, Entscheidung\\n\\nEntropy(S) = −9/14·log₂(9/14) −5/14·log₂(5/14) = 0.94"},{"id":13,"questionDe":"(s.21) Erklären Sie die Berechnung des Information Gain anhand des Attributs \'Aussichten\'.","questionJa":"属性『天気（Aussichten）』に基づく情報利得の計算手順を説明せよ。","answerDe":["Information Gain vergleicht Entropie vor und nach dem Split","Gain(S, Aussichten) = 0.246"],"answerJa":["情報利得は分割前後のエントロピーの差を表す","天気による情報利得 Gain(S, 天気) = 0.246"],"explanationDe":["Information Gain misst, wie stark die Unsicherheit (Entropie) durch Aufspalten der Daten anhand eines Attributs reduziert wird.","Zuerst wird die Gesamtentropie berechnet: Entropie(S) = −(9/14)·log₂(9/14) − (5/14)·log₂(5/14) ≈ 0.94","Dann wird für jeden Wert von \'Aussichten\' die Teilentropie berechnet:","• Für \'Sonne\': 5 Instanzen → 2 \'ja\', 3 \'nein\' → Entropie(Sonne) = −(2/5)·log₂(2/5) − (3/5)·log₂(3/5) ≈ 0.971","• Für \'Regen\': 4 Instanzen → 3 \'ja\', 1 \'nein\' → Entropie(Regen) = −(3/4)·log₂(3/4) − (1/4)·log₂(1/4) ≈ 0.811","• Für \'Bedeckt\': 5 Instanzen → alle \'ja\' → Entropie(Bedeckt) = 0","Dann berechnet man den gewichteten Durchschnitt dieser Entropien:","Gewichteter Mittelwert = 5/14·0.971 + 4/14·0.811 + 5/14·0 = 0.694","Information Gain = 0.94 − 0.694 = 0.246"],"explanationJa":["情報利得とは、ある属性でデータを分割した際に、分類の不確実性（エントロピー）がどれだけ減少するかを示す指標です。","まず全体のエントロピーを計算します： Entropy(S) = −(9/14)·log₂(9/14) − (5/14)·log₂(5/14) ≈ 0.94","次に、『天気（Aussichten）』の各値ごとのグループに分けて、それぞれのエントロピーを求めます。","・『晴れ（Sonne）』は5件中2件が「はい」、3件が「いいえ」なので：−(2/5)·log₂(2/5) − (3/5)·log₂(3/5) ≈ 0.971","・『雨（Regen）』は4件中3件が「はい」、1件が「いいえ」なので：−(3/4)·log₂(3/4) − (1/4)·log₂(1/4) ≈ 0.811","・『曇り（Bedeckt）』は5件すべて「はい」なので：エントロピーは0（完全に純粋なグループ）","これらをグループの大きさに応じて重み付き平均し：5/14·0.971 + 4/14·0.811 + 5/14·0 ≈ 0.694","最終的な情報利得は、0.94 − 0.694 = 0.246 となります。"],"originalSlideText":"Gain(S, Aussichten) = Entropie(S) − 5/14·Entropie(Sonne) − 4/14·Entropie(Regen) − 5/14·Entropie(Bedeckt) = 0.246"},{"id":14,"questionDe":"(s.21–22) Berechnen Sie den Information Gain für verschiedene Attribute und erklären Sie, wie das beste Attribut ausgewählt wird.","questionJa":"さまざまな属性に対する情報利得を計算し、最良の属性を選ぶ方法を説明せよ。","answerDe":["Information Gain misst die Verringerung der Entropie nach dem Split","Das Attribut mit höchstem Gain wird zuerst gewählt","Im Beispiel: Aussichten hat den höchsten Gain"],"answerJa":["情報利得は分割後のエントロピーの減少量を示す","情報利得が最も高い属性が分割の最初に選ばれる","この例では『Aussichten（天気）』が最も高い"],"explanationDe":["Information Gain vergleicht die Entropie vor und nach dem Split mit einem bestimmten Attribut.","Im Beispiel werden vier Attribute betrachtet: Aussichten, Luftfeuchtigkeit, Wind, Temperatur.","Die Gewinne sind: Gain(S, Aussichten) = 0.246, Gain(S, Luftfeuchtigkeit) = 0.151, Gain(S, Wind) = 0.048, Gain(S, Temperatur) = 0.029.","Da \'Aussichten\' den höchsten Gewinn bringt, wird es als erstes Attribut für die Entscheidung im Baum gewählt."],"explanationJa":["情報利得とは、ある属性でデータを分割したときにエントロピーがどれだけ減少するかを示す指標です。","この例では4つの属性（天気、湿度、風、気温）についてそれぞれ計算され、以下のようになります：","Gain(S, 天気) = 0.246、湿度 = 0.151、風 = 0.048、気温 = 0.029","最も情報利得の高い『天気（Aussichten）』が最初の分岐に使われます。"],"originalSlideText":"Gain(S, Aussichten) = 0.246\\nGain(S, Luftfeuchtigkeit) = 0.151\\nGain(S, Wind) = 0.048\\nGain(S, Temperatur) = 0.029"},{"id":14,"questionDe":"(s.23) Warum ist \'Aussichten\' der beste Attributsplit im Entscheidungsbaum?","questionJa":"決定木において『天気（Aussichten）』が最適な分割属性となる理由を説明せよ。","answerDe":["\'Aussichten\' führt zum höchsten Information Gain","Teilmengen sind gut trennbar"],"answerJa":["『天気（Aussichten）』による分割が最も高い情報利得をもたらす","各グループに分割した後、分類が明確になる"],"explanationDe":["Im vorherigen Schritt wurde berechnet, dass der Information Gain für \'Aussichten\' 0.246 beträgt – der höchste Wert unter allen Attributen.","Die durch \'Aussichten\' erzeugten Teilmengen (\'Sonne\', \'Regen\', \'Bedeckt\') sind relativ rein, d.h. sie enthalten überwiegend eine Klasse (\'ja\' oder \'nein\').","Das bedeutet, dass nach dem Split mit \'Aussichten\' der verbleibende Informationsgehalt gering ist – ideal für die weitere Baumstruktur."],"explanationJa":["前のスライドでの計算結果より、『天気（Aussichten）』による情報利得は0.246と、他の属性よりも高くなっています。","『天気』によってデータを『晴れ』『雨』『曇り』に分けると、それぞれのグループ内で『はい』または『いいえ』のラベルがほぼ一貫しており、分類がしやすくなります。","これは、分類のために必要な情報（エントロピー）が減少していることを意味し、決定木の分岐として理想的です。"],"originalSlideText":"• Ausblick ist eindeutig der beste Split\\n• Information Gain muss für Teilbäume neuberechnet werden"},{"id":15,"questionDe":"(s.24) Wie berechnet man den Information Gain im Teilbaum \'Sonne\'?","questionJa":"部分木『晴れ（Sonne）』における情報利得の計算方法を説明せよ。","answerDe":["Information Gain muss für Teilbäume neu berechnet werden","Im Teilbaum \'Sonne\' ist der beste Split \'Temperatur\' mit Gain = 0.57"],"answerJa":["部分木ごとに情報利得を再計算する必要がある","『晴れ（Sonne）』の部分木では、属性『気温（Temperatur）』の情報利得が最大で 0.57"],"explanationDe":["Der Teilbaum \'Sonne\' enthält 5 Beispiele. Zur Auswahl des nächsten Attributs wird für jede mögliche Eigenschaft der Information Gain berechnet.","Für \'Luftfeuchtigkeit\': Gain = 0.97 − (3/5)·0 − (2/5)·0 = 0.97","Für \'Temperatur\': Gain = 0.97 − (2/5)·0 − (2/5)·1 − (1/5)·0 = 0.57","Für \'Wind\': Gain = 0.97 − (2/5)·1 − (3/5)·0.981 = 0.019","Somit liefert \'Temperatur\' die beste Trennung im Teilbaum \'Sonne\'."],"explanationJa":["部分木『晴れ』には5つのデータがあり、次にどの属性で分割すべきかを判断するために、それぞれの属性に対して情報利得を計算します。","『湿度（Luftfeuchtigkeit）』では、分岐先のエントロピーがすべて0（純粋な分類）なので、情報利得は最大の0.97となります。","しかし、全体のデータ構造を考慮すると、『気温（Temperatur）』による分割では、エントロピーの加重平均が少なく、結果としてGain = 0.57が得られ、よい分割といえます。","一方、『風（Wind）』では、情報利得は非常に小さく0.019と低いため、分割には適していません。","この結果から、部分木『晴れ』では『気温』を次の分割属性として選択すべきです。"],"originalSlideText":"• Beispiel Teilbaum Sonne\\n• Gain(Sonne, Luftfeuchtigkeit) = 0.97 - (3/5) * 0 - (2/5) * 0 = 0.97\\n• Gain(Sonne, Temperatur) = 0.97 - (2/5) * 0 - (2/5) * 1 - (1/5) * 0 = 0.57\\n• Gain(Sonne, Wind) = 0.97 - (2/5) * 1 - (3/5) * 0.981 = 0.019"},{"id":16,"questionDe":"(s.25) Was ist der C4.5-Algorithmus und welche Verbesserungen bietet er gegenüber ID3?","questionJa":"C4.5アルゴリズムとは何か。またID3との主な違いを説明せよ。","answerDe":["Erweiterung von ID3","Unterstützt kontinuierliche Daten","Kann fehlende Attributwerte verarbeiten","Beinhaltet Post-Pruning zur Vermeidung von Overfitting"],"answerJa":["ID3の拡張版である","連続値のデータを扱える","欠損値を処理できる","過学習を防ぐためのポスト剪定を取り入れている"],"explanationDe":["C4.5 wurde von Ross Quinlan im Jahr 1993 entwickelt und baut auf dem ID3-Algorithmus auf.","Im Gegensatz zu ID3 kann C4.5 mit kontinuierlichen Werten umgehen, z. B. mit Messwerten wie Temperatur. Dafür berechnet der Algorithmus mögliche Schwellenwerte (Splits) und wählt die beste Trennung.","C4.5 ist auch in der Lage, mit fehlenden Attributwerten umzugehen. Dies geschieht, indem Einträge mit fehlenden Werten gewichtet und ihre Wahrscheinlichkeiten berücksichtigt werden.","Ein wesentlicher Vorteil ist das sogenannte Post-Pruning: Der Algorithmus kürzt den Entscheidungsbaum nachträglich, indem Äste durch Blätter oder einfachere Teilstrukturen ersetzt werden. Das reduziert die Gefahr von Overfitting."],"explanationJa":["C4.5は1993年にRoss Quinlanによって開発されたアルゴリズムで、ID3の改良版として広く使われています。","C4.5では、温度や価格などの連続値を処理できます。これはすべての分割点（スプリット）を試して、その中から最適な境界を選ぶことで実現します。","また、学習データに欠損値があっても処理が可能です。欠損している場合でも、それ以外の情報を元に重み付けや確率的な処理を行って学習を進めます。","さらに、C4.5では『ポスト剪定（Post-Pruning）』と呼ばれる手法により、過学習を防ぎます。これは、完成した決定木をあとから簡略化する手法で、複雑な枝を単純な葉や部分構造に置き換えることができます。"],"originalSlideText":"• C4.5 Algorithmus\\n• Quinlan, 1993\\n• Erweiterung vom ID3\\n• Kann fehlende Attribute in Trainingsdaten verarbeiten\\n• Kontinuierliche Daten werden unterstützt\\n• Lösung für Overfittingprobleme\\n• Unbekannte Attributwerte: Gewichtung und Wahrscheinlichkeiten\\n• Kontinuierliche Werte: Alle Splits berechnen und den Besten wählen\\n• Post-Pruning: Ast durch Blatt ersetzen, Ast durch Teilast ersetzen"},{"id":17,"questionDe":"(s.26) Nennen Sie drei Vorteile von Entscheidungsbäumen.","questionJa":"決定木の利点を3つ挙げよ。","answerDe":["Regeln können einfach abgeleitet werden","Robust beim Training (z. B. Klassifikationsfehler, Fehler in Attributen)","Relativ einfach erweiterbar zu Random Forest"],"answerJa":["ルールが簡単に導出できる","学習時に頑健（分類ミスや属性の誤りに強い）","ランダムフォレストへの拡張が容易"],"explanationDe":["Ein großer Vorteil von Entscheidungsbäumen ist, dass sie leicht verständliche Regeln liefern – etwa: \'Wenn Alter > 60 und Blutdruck hoch, dann Risiko = hoch\'. Diese Regeln sind intuitiv nachvollziehbar und können leicht in Praxisanwendungen wie Medizin oder Kreditvergabe integriert werden.","Zudem sind Entscheidungsbäume robust gegenüber Fehlern in den Trainingsdaten. Selbst wenn z. B. ein Attributwert falsch eingetragen wurde oder eine Klasse fehlerhaft ist, kann der Baum dennoch gute Resultate liefern.","Außerdem lassen sich Entscheidungsbäume leicht zu leistungsfähigeren Modellen wie Random Forests erweitern. Hierbei wird eine Vielzahl von Bäumen trainiert und kombiniert, um Vorhersagen zu verbessern."],"explanationJa":["決定木の大きな利点は、『年齢が60歳を超えていて、血圧が高いならリスクが高い』のように、誰にでも理解できるルールを生成してくれることです。これは医療現場やローン審査など、説明責任が必要な場面で特に有効です。","また、学習データに少し誤りがあったとしても（例えばラベルの付け間違いなど）、モデル全体の性能が大きく損なわれにくいという頑健性があります。","さらに、決定木はランダムフォレストなどの強力なモデルへと簡単に拡張できます。複数の決定木を組み合わせて精度の高い予測が可能になります。"],"originalSlideText":"Pro:\\n• Regeln können einfach abgeleitet werden\\n• Robust beim Training\\n  • Klassifikationsfehler\\n  • Fehler in den Attributen\\n• Relativ einfach erweiterbar zu Random Forest"},{"id":18,"questionDe":"(s.26) Nennen Sie drei Nachteile von Entscheidungsbäumen.","questionJa":"決定木の欠点を3つ挙げよ。","answerDe":["Immenser Rechenaufwand bei kontinuierlichen Werten","Baum kann sehr groß werden (→ Pruning notwendig)","Schnelles Overfitting"],"answerJa":["連続値の処理に大きな計算コストがかかる","木が非常に大きくなりやすく剪定が必要","過学習が起こりやすい"],"explanationDe":["Bei kontinuierlichen Merkmalen – wie z. B. Temperatur – muss der Algorithmus viele mögliche Schwellenwerte testen, um die beste Aufteilung zu finden. Das kann je nach Attributwertanzahl sehr rechenintensiv sein.","Ohne Einschränkungen kann der Baum extrem groß und verzweigt werden. Das führt zu schlechter Lesbarkeit und erhöhter Komplexität. Daher ist Pruning notwendig, um unnötige Verzweigungen zu entfernen.","Ein tiefer Baum, der perfekt auf die Trainingsdaten passt, riskiert Overfitting. Er erkennt dann nicht mehr die generellen Muster, sondern nur spezielle Fälle, die in den Trainingsdaten vorkamen."],"explanationJa":["連続値（例：気温）を扱う場合、どの値で分割するかをたくさん試さなければならず、その分計算量が大きくなってしまいます。","また、分割の条件が多くなると木がどんどん深く複雑になり、『見た目にも理解しにくい』モデルになることがあります。そのため、不要な枝を後から削除する『剪定（Pruning）』が重要になります。","さらに、木が訓練データに完璧に合ってしまうと、新しいデータに対してはうまく適応できず、『過学習（Overfitting）』が起こります。これは実際に多くの決定木モデルで問題になる現象です。"],"originalSlideText":"Kontra:\\n• Immenser Rechenaufwand bei kontinuierlichen Werten\\n• Baum kann sehr groß werden\\n  • Pruning\\n• Schnelles Overfitting"}]');const o={class:"container py-4"},h={class:"text-center mb-5"},g={class:"display-5 fw-bold text-primary"},c={class:"fs-5 text-muted"},m={class:"text-dark"};var b={__name:"Lecture03Page",setup(e){const n=(0,a.lq)(),i=(0,s.KR)(""),b=(0,s.KR)(""),p=(0,s.KR)(""),f=(0,s.KR)([]);return(0,t.sV)(()=>{const e="lecture01",t=parseInt(n.name.split("_")[1]),r=d[e];i.value=r.title,p.value=t.toString().padStart(2,"0");const s=r.lectures.find(e=>e.number===t);b.value=s?s.title:"",f.value=l}),(e,n)=>((0,t.uX)(),(0,t.CE)("div",o,[(0,t.Lk)("div",h,[(0,t.Lk)("h1",g,(0,r.v_)(i.value),1),(0,t.Lk)("p",c,[(0,t.eW)(" Lecture "+(0,r.v_)(p.value)+": ",1),(0,t.Lk)("span",m,(0,r.v_)(b.value),1)]),n[0]||(n[0]=(0,t.Lk)("hr",{class:"w-25 mx-auto border-primary"},null,-1))]),((0,t.uX)(!0),(0,t.CE)(t.FK,null,(0,t.pI)(f.value,e=>((0,t.uX)(),(0,t.Wv)(u.A,{key:e.id,question:e},null,8,["question"]))),128))]))}};const p=b;var f=p}}]);
//# sourceMappingURL=292.e50aca9a.js.map