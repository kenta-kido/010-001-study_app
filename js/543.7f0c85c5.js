"use strict";(self["webpackChunkstudy_site"]=self["webpackChunkstudy_site"]||[]).push([[543],{495:function(e,n,i){i.d(n,{A:function(){return J}});var a=i(6768),s=i(4232),r=i(144);const t={class:"card mb-4 shadow-sm"},l={class:"card-body"},d={class:"card-title"},o={class:"text-muted fst-italic"},u={key:0},h=["src"],g={key:1,class:"mt-3"},c={class:"alert alert-success"},k={key:0},m={key:1},b={class:"alert alert-info mt-2"},w={key:0},f={key:1},z={class:"mt-3"},p={key:0},D={key:1},v={key:2},x={key:3},A={key:4},K=["src"],M={class:"mt-4"},E={class:"border rounded p-3 bg-white text-secondary",style:{"white-space":"pre-wrap","font-family":"inherit"}};var S={__name:"QuestionItem",props:{question:Object},setup(e){const n=(0,r.KR)(!1);return(i,r)=>((0,a.uX)(),(0,a.CE)("div",t,[(0,a.Lk)("div",l,[(0,a.Lk)("h5",d,"Q"+(0,s.v_)(e.question.id)+": "+(0,s.v_)(e.question.questionJa),1),(0,a.Lk)("p",o,"("+(0,s.v_)(e.question.questionDe)+")",1),e.question.questionImage?((0,a.uX)(),(0,a.CE)("div",u,[(0,a.Lk)("img",{src:`./images/${e.question.questionImage}`,class:"img-fluid rounded my-2 border",alt:"question image"},null,8,h)])):(0,a.Q3)("",!0),(0,a.Lk)("button",{class:"btn btn-outline-primary mt-2",onClick:r[0]||(r[0]=e=>n.value=!n.value)},(0,s.v_)(n.value?"Hide Answer":"Check Answer"),1),n.value?((0,a.uX)(),(0,a.CE)("div",g,[(0,a.Lk)("div",c,[r[1]||(r[1]=(0,a.Lk)("strong",null,"Antwort (De):",-1)),r[2]||(r[2]=(0,a.Lk)("br",null,null,-1)),Array.isArray(e.question.answerDe)?((0,a.uX)(),(0,a.CE)("ul",k,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(e.question.answerDe,(e,n)=>((0,a.uX)(),(0,a.CE)("li",{key:n},(0,s.v_)(e),1))),128))])):((0,a.uX)(),(0,a.CE)("p",m,(0,s.v_)(e.question.answerDe),1))]),(0,a.Lk)("div",b,[r[3]||(r[3]=(0,a.Lk)("strong",null,"Übersetzung (Ja):",-1)),r[4]||(r[4]=(0,a.Lk)("br",null,null,-1)),Array.isArray(e.question.answerJa)?((0,a.uX)(),(0,a.CE)("ul",w,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(e.question.answerJa,(e,n)=>((0,a.uX)(),(0,a.CE)("li",{key:n},(0,s.v_)(e),1))),128))])):((0,a.uX)(),(0,a.CE)("p",f,(0,s.v_)(e.question.answerJa),1))]),(0,a.Lk)("div",z,[r[6]||(r[6]=(0,a.Lk)("p",{class:"fw-bold mb-1"},"Erklärung (De):",-1)),Array.isArray(e.question.explanationDe)?((0,a.uX)(),(0,a.CE)("div",p,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(e.question.explanationDe,(e,n)=>((0,a.uX)(),(0,a.CE)("p",{key:n},(0,s.v_)(e),1))),128))])):((0,a.uX)(),(0,a.CE)("p",D,(0,s.v_)(e.question.explanationDe),1)),r[7]||(r[7]=(0,a.Lk)("p",{class:"fw-bold mb-1"},"解説 (Ja):",-1)),Array.isArray(e.question.explanationJa)?((0,a.uX)(),(0,a.CE)("div",v,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(e.question.explanationJa,(e,n)=>((0,a.uX)(),(0,a.CE)("p",{key:n},(0,s.v_)(e),1))),128))])):((0,a.uX)(),(0,a.CE)("p",x,(0,s.v_)(e.question.explanationJa),1)),e.question.explanationImage?((0,a.uX)(),(0,a.CE)("div",A,[(0,a.Lk)("img",{src:`./images/${e.question.explanationImage}`,class:"img-fluid rounded my-2 border",alt:"explanation image"},null,8,K)])):(0,a.Q3)("",!0),(0,a.Lk)("div",M,[r[5]||(r[5]=(0,a.Lk)("p",{class:"fw-bold mb-1"},"原文（スライド抜粋）:",-1)),(0,a.Lk)("div",E,(0,s.v_)(e.question.originalSlideText),1)])])])):(0,a.Q3)("",!0)])]))}};const q=S;var J=q},4543:function(e,n,i){i.r(n),i.d(n,{default:function(){return w}});i(8111),i(116);var a=i(6768),s=i(4232),r=i(144),t=i(1387),l=i(495),d=i(3529),o=JSON.parse('[{"id":1,"questionDe":"(s.3) Beschreiben Sie die Voraussetzungen und das Prinzip der linearen Regression.","questionJa":"線形回帰の前提条件と基本原理を説明せよ。","answerDe":["Funktioniert mit numerischen Werten","Ergebnis und alle Attribute müssen numerisch sein","Ergebnis wird als lineare Kombination der Attribute modelliert"],"answerJa":["数値データに対して有効に機能する","目的変数とすべての属性が数値でなければならない","属性の線形結合として結果を表現する"],"explanationDe":["Lineare Regression ist ein einfaches, aber leistungsfähiges Verfahren zur Vorhersage numerischer Werte. Es basiert auf der Annahme, dass der Zielwert (Ergebnis) eine lineare Funktion der Eingabemerkmale ist.","Voraussetzung ist, dass sowohl das vorherzusagende Ergebnis als auch alle Eingabeattribute numerisch sind – z. B. Alter, Größe oder Einkommen.","Die Vorhersage erfolgt mit der Formel x = Σ(wᵢ·aᵢ), wobei aᵢ die Werte der Attribute und wᵢ die berechneten Gewichtungsfaktoren sind. Der Term a₀ = 1 steht für den Bias oder Achsenabschnitt."],"explanationJa":["線形回帰は、数値の予測に用いられる基本的かつ強力な手法です。予測対象（目的変数）が入力属性の線形関数で表現できるという前提に基づいています。","前提条件として、予測対象の変数も、入力する属性もすべて数値データである必要があります。たとえば、年齢・身長・収入などの数値が該当します。","予測は x = Σ(wᵢ·aᵢ) のように計算されます。ここで aᵢ は各属性の値、wᵢ はそれに対応する重み（係数）です。a₀ = 1 は切片項（バイアス）を意味します。"],"originalSlideText":"• Lineare Modelle\\n– Funktionieren gut mit numerischen Werten\\n– Numerische Vorhersage: Linear regression\\n  • Vorbedingung: Ergebnis und alle Attribute sind numerisch\\n• Das Ergebnis kann als Kombination der Attribute ausgedrückt werden:\\n  x = Σ wᵢaᵢ\\n  x: Ergebnis\\n  a₀: 1\\n  aᵢ: Wert des Attributs i\\n  wᵢ: Vorberechnete Gewichte"},{"id":2,"questionDe":"(s.4) Was ist das Ziel der linearen Regression und wie sieht das Ergebnis aus?","questionJa":"線形回帰の目的とその結果は何か？","answerDe":["Ziel: Minimierung der Vorhersagefehler (Fehlerquadratsumme)","Ergebnis: Gewichte wᵢ, basierend auf Trainingsdaten"],"answerJa":["目的：予測誤差（二乗誤差和）を最小化すること","結果：学習データに基づいて求められた重み wᵢ"],"explanationDe":["Bei der linearen Regression ist das Ziel, die Summe der quadratischen Abweichungen zwischen den tatsächlichen Ausgabewerten x⁽ⁱ⁾ und den vorhergesagten Werten Σwⱼaⱼ⁽ⁱ⁾ zu minimieren.","Die mathematische Zielfunktion lautet: Σⁿⁱ(x⁽ⁱ⁾ − Σʲwⱼaⱼ⁽ⁱ⁾)², wobei a₀⁽ⁱ⁾ = 1 für den Bias-Term steht.","Als Ergebnis erhält man eine Reihe von Gewichten wⱼ, die angeben, wie stark jedes Attribut zur Vorhersage beiträgt.","Diese Gewichte werden anschließend verwendet, um den Wert neuer, unbekannter Instanzen zu schätzen.","Wichtig ist, dass die Anzahl der Trainingsbeispiele n deutlich größer ist als die Anzahl der Attribute k, um Überanpassung zu vermeiden und ein stabiles Modell zu erhalten."],"explanationJa":["線形回帰の目的は、実際の出力（x⁽ⁱ⁾）と予測値（Σwⱼ·aⱼ⁽ⁱ⁾）との二乗誤差和を最小にすることです。","数学的には、Σⁿⁱ(x⁽ⁱ⁾ − Σʲwⱼaⱼ⁽ⁱ⁾)² の式を最小化します。ここで a₀⁽ⁱ⁾ = 1 はバイアス項（切片）を含む定数項を表します。","学習の結果として得られるのは、各属性に対する重み（wⱼ）です。この重みによって、各属性がどれだけ予測に影響を与えるかが示されます。","この重みを使って、新しいデータの出力値を予測することができます。","なお、学習に使用するデータ数 n は、属性の数 k より十分に多いことが望まれます。そうすることで、モデルが安定し、過学習を避けることができます。"],"originalSlideText":"Ziel: Minimiere die folgende Gleichung:\\nΣ (x⁽ⁱ⁾ − Σ wⱼaⱼ⁽ⁱ⁾)²\\nx⁽ⁱ⁾: Ergebnis, Klasse der Instanz i\\na₀⁽ⁱ⁾ = 1, aⱼ⁽ⁱ⁾: Wert des Attributs j der Instanz i\\nwⱼ: gesuchte Gewichte\\nDie Anzahl der Trainingsinstanzen n sollte deutlich größer als die Anzahl der Attribute k sein\\n\\nErgebnis:\\n• Gewichte, die auf den Trainingsdaten basieren\\n• Diese Gewichte werden dann für die Vorhersage der Instanz unbekannter Daten genutzt","explanationImage":"lecture01/lecture04_ex01.png"},{"id":3,"questionDe":"(s.5) Nennen Sie vier Eigenschaften der linearen Regression.","questionJa":"線形回帰の特徴を4つ挙げよ。","answerDe":["Einfache Methode","Wird oft in statistischen Analysen genutzt","Geeignet nur für lineare Zusammenhänge","Funktioniert nicht bei nicht-linearen Abhängigkeiten"],"answerJa":["シンプルな手法である","統計分析でよく使われる","線形関係にあるデータにのみ適している","データが線形関係でない場合には機能しない"],"explanationDe":["Die lineare Regression ist leicht zu verstehen und umzusetzen, weshalb sie eine beliebte Methode ist.","Sie wird in vielen Bereichen der Statistik angewendet – z. B. um Trends, Korrelationen oder Prognosen zu modellieren.","Sie setzt voraus, dass zwischen den Eingabevariablen (Attributen) und der Zielvariable ein linearer Zusammenhang besteht, d. h. eine Veränderung in einem Attribut führt zu einer proportionalen Veränderung im Ergebnis.","Wenn die Beziehung jedoch nicht-linear ist – z. B. bei exponentiellen oder logistischen Zusammenhängen – liefert die lineare Regression schlechte oder irreführende Ergebnisse."],"explanationJa":["線形回帰は非常にシンプルで扱いやすいため、多くの分析で使われています。","例えば、統計学では傾向分析や予測、相関関係の把握などに広く利用されます。","ただし、入力属性と出力（結果）との間に線形の関係（比例関係）があることが前提です。つまり、ある属性の値が増加すると結果も比例的に増えるような関係です。","一方、指数関数的な関係やS字カーブのような非線形のデータに対しては、この手法はうまく機能せず、不正確な結果をもたらす可能性があります。"],"originalSlideText":"Eigenschaften:\\n- Einfache Methode\\n- Wird oft in statistischen Analysen genutzt\\n- Nur für Daten mit linearen Abhängigkeiten geeignet\\n- Wenn die Daten nicht linear abhängig sind, funktioniert das Verfahren nicht!"},{"id":4,"questionDe":"(s.6) Wie funktioniert die multiresponse lineare Regression zur Klassifikation?","questionJa":"分類問題におけるマルチレスポンス線形回帰はどのように機能するか説明せよ。","answerDe":["Für jede Klasse wird eine eigene lineare Regression erstellt","Trainingsbeispiele der Zielklasse erhalten Zielwert 1, andere 0","Für unbekannte Instanz werden alle Regressionswerte berechnet und der höchste gewählt"],"answerJa":["各クラスごとに個別の線形回帰モデルを構築する","目的のクラスに属する訓練データには出力1を、その他には0を与える","未知のインスタンスに対してはすべての線形表現を計算し、最も高い値のクラスを選択する"],"explanationDe":["Bei der multiresponse linearen Regression wird für jede mögliche Klasse ein eigenes Modell trainiert.","Für das Training erhält ein Beispiel den Zielwert 1, wenn es zu der Klasse gehört, und 0, wenn nicht.","Für eine neue Eingabe berechnet man alle Ausdrücke – also wie gut die Eingabe zu jeder Klasse passt – und wählt die Klasse mit dem höchsten Wert.","Beispiel: Man möchte Tiere in \'Hund\', \'Katze\' und \'Maus\' einteilen. Man erstellt drei Modelle:","Das Hund-Modell lernt: \'Wenn Ohren groß und Gewicht 20 kg, dann wahrscheinlich Hund\'.","Gibt man nun ein Tier mit bestimmten Attributen ein (z. B. Ohren = mittel, Gewicht = 4 kg), berechnet jedes Modell eine Zahl. Wenn das \'Katze\'-Modell den höchsten Wert ergibt, wird das Tier als \'Katze\' klassifiziert."],"explanationJa":["マルチレスポンス線形回帰では、分類したい各クラスごとに独立した線形回帰モデルを作ります。","訓練データのうち、あるクラスに属するインスタンスには目標値として「1」を、それ以外には「0」を与えます。","新しいデータが与えられた場合、各モデル（各クラス）ごとにスコア（線形表現の結果）を計算し、最も高いスコアを出したクラスをそのデータの分類先とします。","例：動物を『犬』『猫』『ネズミ』の3種類に分類したいとします。","それぞれのクラスについて、たとえば『犬モデル』は「耳が大きく体重20kgなら犬の可能性が高い」といった傾向を学びます。","もし『耳の大きさ＝中』『体重＝4kg』という新しい動物のデータを入力すると、3つのモデルすべてでスコアを計算し、もっともスコアが高かったもの（例えば猫）が分類結果になります。"],"originalSlideText":"- Multiresponse linear regression\\n- Nutzung der linearen Regression auch hier möglich\\n- Nutzung anderer Regressionstechniken aber auch möglich\\n- Erstelle eine Regression für jede Instanz:\\n  - Setze das Ergebnis auf 1 für Trainingsinstanzen, die zu der Klasse gehören\\n  - Setze das Ergebnis auf 0 für alle anderen\\n- Das Ergebnis ist ein linearer Ausdruck für die Klasse\\n- Für eine unbekannte Instanz:\\n  - Berechne die Werte für alle lineare Ausdrücke\\n  - Wähle den größten"},{"id":5,"questionDe":"(s.7) Nennen Sie zwei Vorteile der multiresponse linearen Regression bei Klassifikationsproblemen.","questionJa":"分類問題におけるマルチレスポンス線形回帰の利点を2つ挙げよ。","answerDe":["Kann als numerische Funktion für Mitgliedschaften zu jeder Klasse aufgefasst werden","Bringt gute Ergebnisse in der Praxis"],"answerJa":["各クラスに対する所属度を数値関数として扱える","実際の応用で良好な結果を示す"],"explanationDe":["Die multiresponse lineare Regression berechnet für jede Klasse einen numerischen Wert. Dieser kann als Maß dafür gesehen werden, wie stark eine Eingabe zu einer bestimmten Klasse gehört.","Zum Beispiel: Man möchte Tiere in Hund, Katze und Maus klassifizieren. Für ein neues Tier berechnet man drei Werte (je einen für jede Klasse). Der höchste Wert zeigt an, zu welcher Klasse das Tier wahrscheinlich gehört.","Obwohl diese Methode eigentlich für Regressionsaufgaben gedacht ist, funktioniert sie überraschend gut für Klassifikation, besonders wenn die Klassen gut voneinander trennbar sind.","Ein Vorteil in der Praxis ist, dass diese Methode einfach zu implementieren ist und oft stabile, brauchbare Ergebnisse liefert – auch bei kleineren Datensätzen."],"explanationJa":["マルチレスポンス線形回帰では、各クラスごとに数値（スコア）を出力し、その値を『どれだけそのクラスに属しているか』の指標と見なすことができます。","例えば、動物を『犬』『猫』『ネズミ』に分類したいとき、新しい動物のデータ（例：体重や耳の形など）に対して3つのスコアを出します。最も高いスコアを出したクラスが、その動物の予測クラスになります。","本来は回帰分析用の手法ですが、クラス間の区別が明確な場合には分類問題でも十分な性能を発揮します。","また、実装が簡単で安定した結果を出しやすいため、少ないデータや簡易な分析にもよく利用されます。"],"originalSlideText":"- Multiresponse linear regression\\n- Kann als numerische Funktion für Mitgliedschaften zu jeder Klasse aufgefasst werden\\n- Bringt gute Ergebnisse in der Praxis"},{"id":6,"questionDe":"(s.7) Nennen Sie einen Nachteil der multiresponse linearen Regression bei Klassifikationsproblemen.","questionJa":"分類問題におけるマルチレスポンス線形回帰の欠点を1つ挙げよ。","answerDe":["Mitgliedschaftswerte können außerhalb des [0, 1] Intervalls liegen"],"answerJa":["所属度（確率のような値）が [0, 1] の範囲外になる可能性がある"],"explanationDe":["Ein großer Nachteil besteht darin, dass die berechneten Werte nicht automatisch zwischen 0 und 1 liegen – was aber notwendig wäre, wenn man sie als Wahrscheinlichkeiten interpretieren möchte.","Zum Beispiel: Wenn eine Regression für die Klasse \'Hund\' den Wert 1.3 ergibt, ist das höher als 1 und kann nicht als Wahrscheinlichkeit verstanden werden.","Ebenso kann ein negativer Wert wie -0.2 entstehen, was ebenfalls keine sinnvolle Wahrscheinlichkeit darstellt. Das kann zu falscher Interpretation und fehlerhafter Klassifikation führen."],"explanationJa":["大きな欠点は、出力される数値が確率のように見えても、0〜1の範囲外になることがある点です。","例えば、『犬』クラスに対するスコアが1.3と出た場合、確率であれば最大は1のはずなので、1.3という値は正しく解釈できません。","また、-0.2のようにマイナスの値になることもあり、これも確率としては不適切です。こうした数値は、結果を誤って解釈してしまう原因になります。"],"originalSlideText":"- Hat zwei Nachteile:\\n  – Mitgliedschaftswerte können außerhalb des [0, 1] Intervalls liegen und sind somit keine “ordentlichen” Wahrscheinlichkeiten"},{"id":7,"questionDe":"(s.7) Erklären Sie, warum die Annahmen der least-squares Regression bei Klassifikationsaufgaben verletzt werden.","questionJa":"分類問題ではなぜ最小二乗回帰の前提が成立しないのかを説明せよ。","answerDe":["Fehler sollen statistisch unabhängig sein","Fehler sollen normalverteilt sein mit gleicher Standardabweichung","Beobachtungen bei Klassifikation liegen nur zwischen 0 und 1","Diese Annahmen gelten bei Klassifikation nicht"],"answerJa":["誤差は統計的に独立であることが前提とされている","誤差は正規分布で、同じ標準偏差を持つ必要がある","分類問題の出力は0または1のみである","そのため、正規分布や独立性の仮定が成り立たない"],"explanationDe":["Die Methode der kleinsten Quadrate nimmt an, dass die Fehler (also die Abweichungen vom tatsächlichen Wert) statistisch unabhängig, normalverteilt und mit konstanter Varianz sind.","In Klassifikationsaufgaben hat man jedoch Zielwerte wie 0 oder 1 (z. B. ‚kauft‘ oder ‚kauft nicht‘), also keine kontinuierlichen Zahlen. Dadurch ist eine Normalverteilung der Fehler kaum möglich.","Beispiel: Wenn man das Kaufverhalten von Kunden vorhersagen möchte (0 = nicht gekauft, 1 = gekauft), sind alle Zielwerte binär. Die Fehler sind dann zwangsläufig asymmetrisch verteilt und verletzen die statistischen Annahmen.","Das führt dazu, dass die Prognoseungenauigkeit der Methode wächst, weil das Modell falsche Annahmen über die Natur der Daten trifft."],"explanationJa":["最小二乗法では、誤差（予測値と実測値の差）は『独立している』『正規分布に従う』『ばらつきが一定である』ことが前提です。","しかし、分類では目的変数が 0（例：買わない）か 1（例：買う）のように離散的な値しか取りません。そのため、誤差の分布は正規分布にはなりません。","例えば、ある顧客が商品を購入するかどうか（0 または 1）を予測するモデルを考えると、出力も0か1に限られます。そのため誤差も限られた値に偏ってしまい、正規分布の仮定が破綻します。","このように前提が崩れていると、モデルの性能が落ちたり、予測結果が歪んだりする原因になります。"],"originalSlideText":"- Least-squares Regression erwartet, dass die Fehler\\n  – Statistisch unabhängig sind\\n  – Normalverteilt sind mit der gleichen Standardabweichung\\n- Diese Annahme wird aber verletzt, wenn man sie für Klassifikationsprobleme verwendet\\n  – Beobachtungswerte können nur zwischen 0 und 1 liegen"},{"id":8,"questionDe":"(s.8) Erklären Sie die Transformation in der logistischen Regression.","questionJa":"ロジスティック回帰における変換処理を説明せよ。","answerDe":["Baut ein lineares Modell auf einer transformierten Zielvariable auf","Ersetze Pr[1|a₁, ..., aₖ] mit log(Pr[1|a₁, ..., aₖ] / (1 − Pr[1|a₁, ..., aₖ]))","Die Ergebniswerte liegen zwischen [−∞, ∞]","Transformation wird logit transformation genannt"],"answerJa":["変換された目的変数に対して線形モデルを構築する","Pr[1|a₁, ..., aₖ] を log(Pr[1|a₁, ..., aₖ] / (1 − Pr[1|a₁, ..., aₖ])) に置き換える","この変換後の値は [−∞, ∞] の範囲を取る","この変換は logit 変換（ロジット変換）と呼ばれる"],"explanationDe":["Bei der logistischen Regression wird nicht direkt eine Wahrscheinlichkeit modelliert, sondern eine transformierte Version davon – der sogenannte Logit.","Der Logit ist definiert als: log(Pr[1|a₁, ..., aₖ] / (1 − Pr[1|a₁, ..., aₖ])). Diese Transformation macht es möglich, eine lineare Funktion auf eine eigentlich begrenzte Wahrscheinlichkeit anzuwenden.","Beispiel: Wenn Pr = 0.8, dann ist log(0.8 / 0.2) = log(4) ≈ 1.39. Dieser Wert kann dann mit einem linearen Modell wie ∑ wᵢ·aᵢ approximiert werden.","Dadurch entstehen Werte im Bereich [−∞, ∞], was mit einer linearen Regression gut funktioniert."],"explanationJa":["ロジスティック回帰では、確率そのものを予測するのではなく、まずそれを log(Pr / (1 − Pr)) という形に変換します。この変換は logit（ロジット）変換と呼ばれます。","このように変換することで、出力値は −∞ から ∞ の実数範囲に広がり、線形モデルと相性がよくなります。","例：ある入力に対して購入確率が0.8だと仮定すると、log(0.8 / 0.2) = log(4) ≈ 1.39 になります。この値を ∑ wᵢ·aᵢ のような線形式で近似します。","変換によって、線形回帰の枠組みの中で確率を扱えるようになります。"],"originalSlideText":"- Logistische Regression\\n- Baut ein lineares Modell auf einer transformierten Zielvariable\\n- Ersetze Pr[1|a₁, ..., aₖ] mit log(Pr[1|a₁, ..., aₖ] / (1 − Pr[1|a₁, ..., aₖ]))\\n- Die Ergebniswerte liegen zwischen [−∞, ∞]\\n- Transformation wird auch logit transformation genannt (siehe Bild auf der nächsten Folie)"},{"id":9,"questionDe":"(s.8–10) Wie sieht das resultierende Modell der logistischen Regression aus und wie ist seine Form?","questionJa":"ロジスティック回帰の最終的なモデルとその関数形状を説明せよ。","answerDe":["Die transformierte Variable wird wie zuvor durch eine lineare Funktion approximiert","Resultierendes Modell: Pr[1|a₁, ..., aₖ] = 1 / (1 + e^(−∑ wᵢ·aᵢ))"],"answerJa":["変換後の変数は従来と同様に線形関数で近似される","最終モデル：Pr[1|a₁, ..., aₖ] = 1 / (1 + e^(−∑ wᵢ·aᵢ))"],"explanationDe":["Nach der Logit-Transformation wird der Logit-Wert (log(P / (1 − P))) durch eine lineare Funktion ∑ wᵢ·aᵢ angenähert.","Durch Rücktransformation erhält man das finale Modell: Pr[1|a₁, ..., aₖ] = 1 / (1 + e^(−∑ wᵢ·aᵢ))","Dieses Modell ergibt eine S-förmige (sigmoidale) Kurve, wie auf Folie 10 zu sehen: Für kleine ∑ wᵢ·aᵢ nähert sich die Wahrscheinlichkeit 0, für große Werte nähert sie sich 1.","Beispiel: Wenn ∑ wᵢ·aᵢ = 0, dann ist Pr = 1 / (1 + e⁰) = 0.5 – genau in der Mitte."],"explanationJa":["ロジット変換された値（log(P / (1 − P)））を ∑ wᵢ·aᵢ のような線形関数で近似したあと、元の確率に戻すための逆変換を行います。","これにより、ロジスティック回帰のモデルは次のように表されます：Pr[1|a₁, ..., aₖ] = 1 / (1 + e^(−∑ wᵢ·aᵢ))","この関数はS字型（シグモイド）になっており、∑ wᵢ·aᵢ が小さいときは0に、大きいときは1に近づきます。これはスライド10のグラフに対応しています。","例：∑ wᵢ·aᵢ = 0 のとき、e⁰ = 1 なので Pr = 1 / (1 + 1) = 0.5、つまり確率は50%になります。"],"originalSlideText":"- Die transformierte Variable wird wie zuvor durch eine lineare Funktion approximiert\\n- Resultierendes Modell:\\n  Pr[1|a₁, ..., aₖ] = 1 / (1 + e^(−∑ wᵢ·aᵢ))\\n\\nFigure 4.9 Logistic regression: (a) the logit transform and (b) an example logistic regression function.","explanationImage":"lecture01/lecture04_ex02.png"},{"id":10,"questionDe":"(s.9–10) Was zeigen die beiden Abbildungen zur logistischen Regression?","questionJa":"スライド9と10の図はロジスティック回帰において何を表しているか説明せよ。","answerDe":["Die linke Abbildung (s.9) zeigt die logit-Transformation: log(P / (1 − P))","Die rechte Abbildung (s.10) zeigt die logistische Funktion: 1 / (1 + e^(−x))","Die logit-Funktion transformiert Wahrscheinlichkeiten in den Bereich (−∞, ∞)","Die logistische Funktion wandelt lineare Werte wieder in Wahrscheinlichkeiten zwischen 0 und 1 um"],"answerJa":["左側の図（s.9）は logit 変換 log(P / (1 − P)) を示している","右側の図（s.10）はロジスティック関数 1 / (1 + e^(−x)) を示している","logit 関数は確率を −∞〜∞ に広げる変換である","ロジスティック関数は線形値を 0〜1 の確率に戻す逆変換である"],"explanationDe":["Auf Folie 9 ist die logit-Funktion dargestellt. Sie nimmt eine Wahrscheinlichkeit (zwischen 0 und 1) und berechnet log(P / (1 − P)).","Diese Transformation dehnt die Wahrscheinlichkeit auf einen unbegrenzten Wertebereich aus – kleine Wahrscheinlichkeiten werden stark negativ, große stark positiv.","Auf Folie 10 sieht man die logistische Funktion, die genau diese Transformation wieder rückgängig macht.","Die x-Achse stellt hier den linearen Ausdruck ∑ wᵢ·aᵢ dar. Die y-Achse ist die daraus berechnete Wahrscheinlichkeit.","Beispiel: Wenn ∑ wᵢ·aᵢ = 0, ergibt sich Pr = 0.5. Bei −5 ist Pr ≈ 0.007, bei +5 ist Pr ≈ 0.993.","Die S-Form (sigmoid) der Funktion stellt sicher, dass alle Ausgaben im Bereich [0, 1] bleiben – ideal für Klassifikation."],"explanationJa":["スライド9は、確率Pを log(P / (1 − P)) に変換する logit 関数の形を示しています。確率が0.5ならlogitは0、1に近づくと急激に大きな正の値に、0に近づくと大きな負の値になります。","この変換により、確率を無限の範囲（−∞〜∞）に広げて、線形回帰で扱いやすくしています。","一方、スライド10はロジスティック関数（1 / (1 + e^(−x))）を示しており、logit変換された値を0〜1の確率に戻す役割を果たします。","横軸は線形な入力（例えば ∑ wᵢ·aᵢ）で、縦軸がその入力に対応する確率です。","例：∑ wᵢ·aᵢ = 0 のとき、出力は 0.5。−5 のときは約 0.007、+5 のときは約 0.993 になります。","このS字型の関数（シグモイド関数）によって、どんな入力でも確率として有効な範囲（0〜1）に収めることができます。"],"originalSlideText":"Figure 4.9 Logistic regression: (a) the logit transform and (b) an example logistic regression function.","questionImage":"lecture01/lecture04_q01.png"},{"id":11,"questionDe":"(s.11) Wie wird das logistische Regressionsmodell an Trainingsdaten angepasst?","questionJa":"ロジスティック回帰モデルはどのように訓練データに適合させるか説明せよ。","answerDe":["Finde Gewichte, die zu den Trainingsdaten passen","Nutze die log-likelihood des Modells","Die Gewichte wᵢ werden so gewählt, dass die log-likelihood maximiert wird","Simple Methode: löse iterativ gewichtete least-square Regressionsprobleme","Konvergiert normalerweise nach ein paar Iterationen"],"answerJa":["訓練データに合うような重みを求める","モデルの対数尤度（log-likelihood）を用いる","重み wᵢ は対数尤度を最大にするように選ばれる","簡単な方法としては、加重最小二乗法を繰り返し解く手法がある","通常、数回の繰り返しで収束する"],"explanationDe":["In der logistischen Regression passt man das Modell an die Trainingsdaten an, indem man Gewichte wᵢ bestimmt, die die Vorhersagewahrscheinlichkeiten verbessern.","Dazu nutzt man die sogenannte log-likelihood-Funktion. Diese bewertet, wie gut das Modell die beobachteten Werte erklärt.","Die log-likelihood lautet:\\n∑ᵢ=1ⁿ (1 − x⁽ⁱ⁾) log(1 − Pr[1|a₁⁽ⁱ⁾, ..., aₖ⁽ⁱ⁾]) + x⁽ⁱ⁾ log(Pr[1|a₁⁽ⁱ⁾, ..., aₖ⁽ⁱ⁾])\\nDabei ist x⁽ⁱ⁾ ∈ {0, 1}","Das Ziel ist es, diese Funktion zu maximieren – d. h. die Parameter wᵢ so zu wählen, dass die beobachteten Ausgaben möglichst wahrscheinlich erscheinen.","Ein praktisches Verfahren ist, das Problem schrittweise als eine Reihe von gewichteten linearen Regressionen zu lösen (z. B. IRLS – Iteratively Reweighted Least Squares).","Beispiel: Wenn das Modell immer genau die Trainingsdaten korrekt klassifiziert, wäre die log-likelihood maximal."],"explanationJa":["ロジスティック回帰では、入力に対して正しい出力（0または1）を得られるように、各特徴量に対する重み wᵢ を調整します。","このとき使われるのが対数尤度（log-likelihood）という関数です。これは『現在のモデルがどれだけデータをうまく説明しているか』を数値で評価するものです。","対数尤度の式は次の通りです：\\n∑ᵢ=1ⁿ (1 − x⁽ⁱ⁾) log(1 − Pr[1|a₁⁽ⁱ⁾, ..., aₖ⁽ⁱ⁾]) + x⁽ⁱ⁾ log(Pr[1|a₁⁽ⁱ⁾, ..., aₖ⁽ⁱ⁾])\\nここで x⁽ⁱ⁾ は正解ラベル（0 または 1）を表します。","この関数を最大にするように wᵢ を選ぶと、モデルはより正確に予測できるようになります。","実装上は、重み付き最小二乗法（weighted least squares）を繰り返し解く方法（例：IRLS法）などが用いられます。","例：すべての訓練データに対して正しいクラスを100%予測できる場合、この対数尤度は最大値になります。"],"originalSlideText":"- Logistische Regression\\n- Finde Gewichte , die zu den Trainingsdaten passen\\n- Nutzt die log-likelihood des Modells\\n- ∑ⁿᵢ=1 (1 − x⁽ⁱ⁾) log(1 − Pr[1|a₁⁽ⁱ⁾, ..., aₖ⁽ⁱ⁾]) + x⁽ⁱ⁾ log(Pr[1|a₁⁽ⁱ⁾, ..., aₖ⁽ⁱ⁾])\\n- x⁽ⁱ⁾ ∈ {0, 1}\\n\\n- Die Gewichte wᵢ werden so gewählt, dass die log-likelihood maximiert wird\\n- Simple Methode:\\n  – Löse iterativ eine Folge von gewichteten least-square Regressionsproblemen, bis die log-likelihood in einem Maximum konvergiert\\n  – Konvergiert normalerweise nach ein paar Iterationen"},{"id":13,"questionDe":"(s.12) Nennen Sie drei Merkmale der paarweisen Klassifikation.","questionJa":"ペアワイズ分類の特徴を3つ挙げよ。","answerDe":["Ein Klassifikator wird für jedes Paar von Klassen gebaut","Nur Instanzen dieser Klassen werden benutzt","Ergebnis basiert auf der Klasse, die am besten passt"],"answerJa":["すべてのクラスのペアごとに分類器を構築する","分類器は該当する2クラスのデータだけを使用する","未知のデータには最も適合するクラスを選択する"],"explanationDe":["Bei der paarweisen Klassifikation (auch one-vs-one) wird für jede Kombination von zwei Klassen ein separater Klassifikator trainiert.","Beispiel: Bei drei Klassen (A, B, C) gibt es drei Paare: (A vs B), (A vs C), (B vs C). Für jedes dieser Paare wird ein eigenes Modell erstellt.","Jeder Klassifikator verwendet nur die Daten dieser zwei Klassen. Für eine neue Eingabe stimmen alle Klassifikatoren ab, und die Klasse mit den meisten \'Stimmen\' wird als Ergebnis gewählt."],"explanationJa":["ペアワイズ分類（または one-vs-one 手法）では、すべてのクラスの組み合わせに対して個別の分類器を作ります。","例：クラスが A・B・C の3つある場合、(AとB)、(AとC)、(BとC) の3組があり、それぞれに分類器を作ります。","各分類器はその2クラスのデータだけを学習し、新しいデータについてどちらのクラスに近いかを判断します。最終的には、投票のように多数決で最も適合したクラスを選びます。"],"originalSlideText":"- Paarweise Klassifikation:\\n  – Ein Klassifikator wird für jedes Paar von Klassen gebaut\\n  – Nur Instanzen dieser Klassen werden benutzt\\n  – Ergebnis für einen unbekannten Datensatz basiert auf der Klasse, die am besten passt"},{"id":14,"questionDe":"(s.12) Nennen Sie drei Vorteile oder Anwendungen der paarweisen Klassifikation.","questionJa":"ペアワイズ分類の利点または応用を3つ挙げよ。","answerDe":["Bringt gute Ergebnisse unter Beachtung des Klassifikationsfehlers","Kann zur Berechnung von Wahrscheinlichkeiten genutzt werden","Kalibriert die individuellen Wahrscheinlichkeiten der verschiedenen Klassifikatoren"],"answerJa":["分類誤差を考慮することで良い結果が得られる","確率の計算にも利用できる","各分類器の出力確率を校正できる"],"explanationDe":["Da jeder Klassifikator nur zwei Klassen unterscheidet, sind die Aufgaben einfacher und oft genauer als bei Multiklassenmodellen.","Man kann die paarweisen Entscheidungen auch zur Schätzung von Wahrscheinlichkeiten verwenden – z. B. durch Kombination der Stimmen oder durch Wahrscheinlichkeitskalibrierung.","Beispiel: Wenn der Klassifikator (A vs B) A mit 90 % Wahrscheinlichkeit wählt und (A vs C) auch A, kann man A als wahrscheinlichste Klasse betrachten."],"explanationJa":["それぞれの分類器が2クラスだけを扱うため、複雑な多クラス分類よりも精度が高くなりやすいです。","ペアワイズ分類の結果をうまく組み合わせることで、最終的なクラスごとの確率を推定することもできます。","例：『A vs B』分類器で A の確率が90%、『A vs C』でも A が優勢なら、最終的に A を選ぶのが自然です。このように、分類器の出力を統合・調整（校正）してより信頼性の高い予測を得ることができます。"],"originalSlideText":"- Bringt gute Ergebnisse unter Beachtung des Klassifikationsfehlers\\n- Kann man auch zur Berechnung von Wahrscheinlichkeiten nutzen:\\n  – Paarweise koppeln\\n  – Kalibriert die individuellen Wahrscheinlichkeiten der verschiedenen Klassifikatoren"},{"id":15,"questionDe":"(s.13) Nennen Sie drei Eigenschaften der Anzahl und Nutzung der Klassifikatoren bei der paarweisen Klassifikation.","questionJa":"ペアワイズ分類における分類器の数とその使い方に関する特徴を3つ挙げよ。","answerDe":["Für k Klassen werden (k·(k−1))/2 Klassifikatoren erstellt","Nur die Instanzen, die zu den zwei Klassen gehören, werden benutzt","Ist dennoch genauso schnell wie die anderen Multiklassenmethoden"],"answerJa":["k 個のクラスに対して (k·(k−1))/2 個の分類器が作られる","各分類器には対象の2クラスのインスタンスだけを使う","他の多クラス分類法と同じくらい高速である"],"explanationDe":["Bei k Klassen gibt es insgesamt k·(k−1)/2 mögliche Paare, für die jeweils ein eigener Klassifikator trainiert wird.","Da jeder Klassifikator nur zwei Klassen voneinander trennt, ist das Training effizient – insbesondere, da nur ein Teil der Daten verwendet wird.","Obwohl mehr Klassifikatoren gebaut werden, ist die Methode in der Praxis genauso schnell wie One-vs-Rest oder andere Multiklass-Verfahren."],"explanationJa":["k個のクラスがある場合、すべての組み合わせは (k·(k−1))/2 通りあり、それぞれに分類器を1つ作成します。","ただし、各分類器は対応する2つのクラスに属するデータのみを使うため、1つの分類器あたりのデータ量は少なく、学習は効率的です。","分類器の数は多くても、全体としては one-vs-rest など他の多クラス分類法と同程度の学習時間で済みます。"],"originalSlideText":"- Paarweise Klassifikation\\n  – Für k Klassen werden (k(k−1))/2 Klassifikatoren erstellt\\n  – Ist dennoch genauso schnell wie die anderen Multiklassenmethoden\\n  – Nur die Instanzen, die zu den zwei Klassen gehören, werden benutzt"},{"id":16,"questionDe":"(s.13) Nennen Sie drei Aussagen über den Rechenaufwand der paarweisen Klassifikation.","questionJa":"ペアワイズ分類の計算量に関する記述を3つ挙げよ。","answerDe":["Gleichmäßige Verteilung von n Instanzen über k Klassen: (2n/k) Instanzen pro Problem","Laufzeit ist proportional zu (k−1)·n","Methode skaliert linear mit der Anzahl der Klassen"],"answerJa":["n 個のインスタンスが k クラスに均等に分布すると、1つの分類器あたりのインスタンス数は (2n/k)","全体の実行時間は (k−1)·n に比例する","この手法はクラス数に対して線形にスケールする"],"explanationDe":["Wenn die n Trainingsinstanzen gleichmäßig auf k Klassen verteilt sind, enthält jede Klasse etwa n/k Instanzen. Für ein Paar sind das 2n/k Instanzen.","Da es (k(k−1))/2 Klassifikatoren gibt, multipliziert mit 2n/k Instanzen ergibt sich ein Rechenaufwand von (k−1)·n.","Das bedeutet: Selbst wenn die Klassenanzahl wächst, steigt der Aufwand nur linear mit der Anzahl k – die Methode bleibt skalierbar."],"explanationJa":["n 個の訓練データが k クラスに均等に分布していると仮定すると、各クラスには n/k 件が割り当てられます。2クラスの分類器ならその合計で 2n/k 件のデータを使うことになります。","分類器の総数は (k(k−1))/2 個なので、全体の処理量は (k−1)·n に比例します。","つまり、クラス数が増えても、学習や分類に必要なコストは直線的に増えるだけなので、大規模な分類にも適しています。"],"originalSlideText":"- Gleichmäßige Verteilung von n Instanzen über k Klassen: (2n/k) Instanzen pro Problem\\n- Laufzeit ist proportional zu (k(k−1))/2 · (2n/k) = (k−1)·n\\n- Methode skaliert linear mit der Anzahl der Klassen"},{"id":17,"questionDe":"(s.14) Nennen Sie fünf Eigenschaften des Nearest-Neighbor-Algorithmus.","questionJa":"最近傍法（Nearest Neighbor法）の特徴を5つ挙げよ。","answerDe":["Klassifiziere Datenpunkte aufgrund ihrer Nachbarschaft","Attribute werden als Vektoren aufgefasst und in einen Raum gelegt","Mehrheitsentscheidung der Trainingsdaten in der Nachbarschaft","Algorithmus kann k Nachbarn in die Klassifikation einbeziehen","Zu wenig/zu viele Nachbarn verändern die Ergebnisse"],"answerJa":["データ点を近傍に基づいて分類する","属性はベクトルとして扱われ、空間に配置される","近傍の訓練データの多数決で分類される","k個の近傍を使って分類を行うことができる","近傍が少なすぎても多すぎても結果に影響を与える"],"explanationDe":["Der Nearest-Neighbor-Algorithmus klassifiziert einen neuen Punkt basierend auf den Datenpunkten in seiner Umgebung (Nachbarschaft).","Die Eingabedaten werden als Vektoren interpretiert – z. B. (Größe, Gewicht) – und in einem mehrdimensionalen Raum dargestellt.","Um die Klasse zu bestimmen, wird geschaut, welche Klassen unter den k nächsten Trainingsbeispielen am häufigsten vorkommen (Mehrheitsentscheidung).","Man kann die Anzahl k der berücksichtigten Nachbarn anpassen. k=1 bedeutet nur der nächste Punkt zählt, k=5 berücksichtigt die fünf nächsten.","Ist k zu klein, ist die Entscheidung empfindlich gegen Ausreißer. Ist k zu groß, kann es die tatsächliche lokale Struktur verwischen."],"explanationJa":["Nearest Neighbor（最近傍法）は、ある新しいデータがどのクラスに属するかを、その近くにある既知のデータ（訓練データ）から判断します。","入力データは、例えば『身長』や『体重』などの特徴量をもつベクトルとして空間に配置されます。","新しいデータに対して、その周囲のk個の訓練データを見て、多数決でどのクラスが多いかを決めて分類します。","kは自由に設定でき、k=1なら最も近い1点だけを見る、k=3やk=5にするとより安定した判断になります。","ただし、kが小さすぎると外れ値の影響を受けやすく、大きすぎると逆に局所的なパターンを見逃してしまう可能性があります。"],"originalSlideText":"- Klassifiziere Datenpunkte aufgrund ihrer Nachbarschaft\\n- Attribute werden als Vektoren aufgefasst und in einen Raum gelegt\\n- Mehrheitsentscheidung der Trainingsdaten in der Nachbarschaft\\n- Algorithmus kann k Nachbarn in die Klassifikation einbeziehen\\n- Zu wenig/ zu viele Nachbarn verändern die Ergebnisse"},{"id":18,"questionDe":"(s.14) Nennen Sie drei typische Probleme beim Einsatz des Nearest-Neighbor-Verfahrens.","questionJa":"Nearest Neighbor法を用いる際の典型的な問題点を3つ挙げよ。","answerDe":["Rechenzeit","Speicherbedarf","Verteilung der Daten im Raum"],"answerJa":["計算時間がかかる","メモリ使用量が多い","データの空間分布に依存する"],"explanationDe":["Da alle Trainingsdaten beim Klassifizieren durchsucht werden müssen, ist der Algorithmus rechnerisch aufwendig – vor allem bei großen Datensätzen.","Weil man alle Trainingsdaten speichern und vergleichen muss, ist auch der Speicherbedarf hoch.","Wenn die Trainingsdaten ungleichmäßig im Raum verteilt sind (z. B. Cluster oder Lücken), kann das die Klassifikation stark verzerren."],"explanationJa":["Nearest Neighbor法では、新しいデータが来るたびにすべての訓練データとの距離を計算する必要があるため、処理時間が非常にかかります（特にデータが多い場合）。","また、全データを記憶しておく必要があるため、メモリ使用量も多くなります。","さらに、データが空間上に偏って分布していたり、密集と空白があるような場合には、分類結果が不安定になることがあります。"],"originalSlideText":"- Vektoren werden durch Distanzmaße auf „Nähe“/„Ähnlichkeit“ geprüft\\n- Probleme:\\n  – Rechenzeit\\n  – Speicherbedarf\\n  – Verteilung der Daten im Raum"},{"id":19,"questionDe":"(s.15) Nennen Sie drei Eigenschaften von Ähnlichkeitsmaßen und Ähnlichkeitsmatrizen.","questionJa":"類似度尺度と類似度行列の性質を3つ挙げよ。","answerDe":["Ähnlichkeitsmaß besteht aus Ähnlichkeitsfunktion oder -koeffizient","Ähnlichkeitsmatrix: S := (sᵢₖ), symmetrisch","Zusätzliche Bedingungen: sᵢₖ ≥ 0, sᵢᵢ = 1"],"answerJa":["類似度尺度には、類似度関数または類似度係数がある","類似度行列 S := (sᵢₖ) は対称行列である","追加条件として、sᵢₖ ≥ 0、および sᵢᵢ = 1 が成り立つ"],"explanationDe":["Ein Ähnlichkeitsmaß beschreibt, wie ähnlich zwei Objekte sind. Dies kann als Funktion oder als Koeffizient formuliert werden.","Die Ähnlichkeitsmatrix S enthält die Ähnlichkeitswerte sᵢₖ zwischen allen Objekten. Da sᵢₖ = sₖᵢ, ist die Matrix symmetrisch.","Zusätzlich gelten zwei wichtige Bedingungen: Alle Werte sind ≥ 0, da negative Ähnlichkeit keinen Sinn ergibt. Außerdem ist die Ähnlichkeit eines Objekts mit sich selbst immer 1 (sᵢᵢ = 1)."],"explanationJa":["類似度尺度とは、2つのデータ（オブジェクト）がどれだけ似ているかを定量的に表すものです。これには関数形式や単なる数値係数（スコア）が用いられます。","類似度行列 S := (sᵢₖ) は、すべてのデータペア間の類似度をまとめたもので、対角対称（sᵢₖ = sₖᵢ）です。","追加の性質として、類似度は必ず 0以上であり（sᵢₖ ≥ 0）、同じデータ同士の類似度は必ず1になります（sᵢᵢ = 1）。これは直感的に「自分自身とは完全に同一」という意味です。"],"originalSlideText":"- Ähnlichkeitsmaß:\\n  – Ähnlichkeitsfunktion\\n  – Ähnlichkeitskoeffizient\\n\\n- Ähnlichkeitsmatrix:\\n  – S := (sᵢₖ)\\n  – Symmetrisch\\n\\n- Zusätzliche Bedingungen:\\n  – sᵢₖ ≥ 0\\n  – sᵢᵢ = 1"},{"id":20,"questionDe":"(s.15) Nennen Sie drei Eigenschaften von Abstandsmaßen und Abstandsmatrizen.","questionJa":"距離尺度と距離行列の性質を3つ挙げよ。","answerDe":["Abstandsmaß wird durch eine Abstands­funktion definiert","Abstandsmatrix: T := (tᵢₖ), symmetrisch","Distanzen messen Unähnlichkeit – je kleiner, desto ähnlicher"],"answerJa":["距離尺度は距離関数によって定義される","距離行列 T := (tᵢₖ) は対称行列である","距離は非類似度を表し、小さいほど似ていることを意味する"],"explanationDe":["Ein Abstandsmaß gibt an, wie \'weit entfernt\' zwei Objekte im Merkmalsraum sind. Dies geschieht über eine Abstands­funktion – wie z. B. die euklidische Distanz.","Die Abstandsmatrix T enthält alle Paarabstände tᵢₖ und ist ebenfalls symmetrisch, da der Abstand von A zu B genauso groß ist wie von B zu A.","Typischerweise gilt: Je kleiner der Abstand, desto größer die Ähnlichkeit. Null bedeutet identisch, große Werte bedeuten große Unterschiede."],"explanationJa":["距離尺度とは、2つのデータ点が特徴空間内でどれだけ離れているかを測るものです。ユークリッド距離などの関数が使われます。","距離行列 T := (tᵢₖ) は、すべてのペア間の距離を記録したもので、sᵢₖ = sₖᵢ のように対称性があります。つまり、AからBまでの距離は、BからAまでと同じです。","一般的に、距離が小さいほどデータ同士は似ており、距離が大きければ大きいほど異なると解釈します。距離が0なら、2つのデータは完全に一致しています。"],"originalSlideText":"- Abstandsmaß:\\n  – Abstands­funktion\\n\\n- Abstandsmatrix:\\n  – T := (tᵢₖ)\\n  – Symmetrisch"},{"id":21,"questionDe":"(s.16) Nennen Sie drei Varianten von Distanzfunktionen, die auf der Minkowski-Metrik basieren.","questionJa":"Minkowski距離に基づく距離関数のバリエーションを3つ挙げよ。","answerDe":["Minkowski-Distanz (Lp-Norm): tₚ(dᵢ, dₖ) := (∑ⱼ=1ᵐ |aᵢⱼ − aₖⱼ|ᵖ)¹/ᵖ","Euklidische Distanz als Spezialfall (p = 2): √∑ⱼ=1ᵐ (aᵢⱼ − aₖⱼ)²","Manhattan-Distanz (p = 1): ∑ⱼ=1ᵐ |aᵢⱼ − aₖⱼ|"],"answerJa":["Minkowski距離（Lpノルム）: tₚ(dᵢ, dₖ) := (∑ⱼ=1ᵐ |aᵢⱼ − aₖⱼ|ᵖ)¹/ᵖ","ユークリッド距離（p = 2 の場合）: √∑ⱼ=1ᵐ (aᵢⱼ − aₖⱼ)²","マンハッタン距離（p = 1 の場合）: ∑ⱼ=1ᵐ |aᵢⱼ − aₖⱼ|"],"explanationDe":["Im Nearest-Neighbor-Verfahren werden Distanzfunktionen genutzt, um den Abstand zwischen Punkten im Merkmalsraum zu bestimmen.","Die Minkowski-Metrik ist eine Familie von Distanzfunktionen, die durch den Parameter p gesteuert wird. Sie umfasst viele bekannte Distanzen als Spezialfälle.","Für p = 2 ergibt sich die euklidische Distanz – der direkte, \'Luftlinien\'-Abstand zwischen zwei Punkten.","Für p = 1 ergibt sich die Manhattan-Distanz – wie man durch ein Straßengitter läuft (horizontal + vertikal).","Beispiel: Für Punkte A=(1,2), B=(4,6):\\n- Euklidisch: √[(4−1)² + (6−2)²] = √25 = 5\\n- Manhattan: |4−1| + |6−2| = 3 + 4 = 7"],"explanationJa":["Nearest Neighbor法では、特徴ベクトル間の距離を定義するために距離関数が使われます。","Minkowski距離はpというパラメータに基づいて定義される距離関数の総称であり、有名なユークリッド距離やマンハッタン距離もこの仲間です。","p=2 の場合はユークリッド距離となり、2点間の直線距離（ピタゴラスの定理）になります。","p=1 の場合はマンハッタン距離となり、縦横の移動量の合計として表現されます。","例：点A=(1,2)、点B=(4,6)\\n- ユークリッド距離：√[(4−1)² + (6−2)²] = √25 = 5\\n- マンハッタン距離：|4−1| + |6−2| = 3 + 4 = 7"],"originalSlideText":"- Minkowski Metriken (Lp Norm):\\n  tₚ(dᵢ, dₖ) := (∑ⱼ=1ᵐ |aᵢⱼ − aₖⱼ|ᵖ)¹/ᵖ\\n- Meist: Euklid‘scher Abstand\\n  √∑ⱼ=1ᵐ (aᵢⱼ − aₖⱼ)²\\n- Alternative: Manhattan (city-block) Metrik\\n  ∑ₖ=1ᵐ |aᵢⱼ − aₖⱼ|","explanationImage":"lecture01/lecture04_ex03.png"},{"id":22,"questionDe":"(s.16) Nennen Sie drei Eigenschaften der Minkowski-Distanzfunktionen im Kontext der Nearest-Neighbor-Klassifikation.","questionJa":"Nearest Neighbor法におけるMinkowski距離関数の性質や注意点を3つ挙げよ。","answerDe":["Für Vergleiche kann die Wurzel weggelassen werden","Invariant gegenüber Verschiebungen (Translationsinvarianz)","Nicht invariant gegenüber Skalierung"],"answerJa":["比較目的なら平方根を省略しても問題ない","平行移動に対しては距離が変わらない（不変）","スケーリングに対しては不変ではない（特徴量の大きさが影響する）"],"explanationDe":["Beim Vergleich von Distanzen, z. B. um den nächsten Nachbarn zu finden, bleibt die Rangordnung auch ohne Wurzel erhalten. Daher kann man sie zur Effizienz weglassen.","Wenn alle Vektoren um denselben Betrag verschoben werden, bleiben ihre Abstände gleich – das nennt man Translationsinvarianz.","Wird jedoch eine einzelne Dimension stark vergrößert (z. B. Einkommen ×100), dominiert diese die Distanz. Daher ist die Metrik nicht skaleninvariant.","Man sollte daher vor der Anwendung Daten standardisieren – z. B. z-Transformation."],"explanationJa":["最近傍法では、どの点が最も近いかを判定するだけなので、距離の大小関係が保たれていれば平方根を取る必要はありません。これにより計算を高速化できます。","すべてのデータに対して同じだけ平行移動しても、距離そのものは変化しません（これは移動に対して不変という性質です）。","ただし、ある特徴量（例：収入）を拡大すると、その次元が距離に与える影響が大きくなり、結果が歪む可能性があります。これがスケーリングに対して不変でないという意味です。","対策として、事前にすべての特徴量を標準化（zスコアなど）することが推奨されます。"],"originalSlideText":"- Bemerkungen\\n  – Für Vergleiche kann die Wurzel weggelassen werden\\n  – Invariant bezüglich Verschiebungen\\n  – Nicht invariant bezüglich Skalierung"},{"id":23,"questionDe":"(s.17) Nennen Sie drei Bestandteile der Definition des Mahalanobis-Abstands.","questionJa":"マハラノビス距離の定義に含まれる3つの構成要素を挙げよ。","answerDe":["tₘ(dᵢ, dₖ) := (dᵢ − dₖ)ᵀ M⁻¹ (dᵢ − dₖ)","M := (1/n) ∑ᵢ (dᵢ − d̄)(dᵢ − d̄)ᵀ","d̄ := (1/n) ∑ᵢ dᵢ"],"answerJa":["tₘ(dᵢ, dₖ) := (dᵢ − dₖ)ᵀ M⁻¹ (dᵢ − dₖ)：マハラノビス距離の数式","M := (1/n) ∑ᵢ (dᵢ − d̄)(dᵢ − d̄)ᵀ：共分散行列の定義","d̄ := (1/n) ∑ᵢ dᵢ：平均ベクトル"],"explanationDe":["Der Mahalanobis-Abstand ist eine Distanzfunktion, die im Gegensatz zur euklidischen Distanz die Streuung (Varianz und Korrelationen) der Daten berücksichtigt.","M ist die sogenannte Kovarianzmatrix – sie beschreibt die gemeinsame Streuung der Merkmale und ist zentral für die Berechnung.","d̄ ist der Mittelwert aller Datenpunkte. Die Matrix M basiert auf der Abweichung jedes Punktes von diesem Mittelwert.","Die Distanz zwischen zwei Punkten dᵢ und dₖ wird so berechnet, dass Richtungen mit größerer Varianz weniger Gewicht bekommen – dies verhindert Verzerrungen bei stark korrelierten oder unterschiedlich skalierten Daten."],"explanationJa":["マハラノビス距離は、ユークリッド距離とは異なり、データのばらつき（分散）や属性間の相関を考慮する距離関数です。","Mは共分散行列で、すべてのデータの平均からの偏差をもとに作られます。これは各特徴量の広がりや、特徴量間の関係を表します。","d̄はデータ全体の平均ベクトルであり、これに基づいて各データ点がどの程度ずれているかを計算します。","マハラノビス距離では、ばらつきの大きい方向ほど距離への寄与が小さくなり、特徴量のスケールや相関の違いに強くなります。"],"originalSlideText":"- Mahalanobis Abstand:\\n  tₘ(dᵢ, dₖ) := (dᵢ − dₖ)ᵀ M⁻¹ (dᵢ − dₖ)\\n  M := (1/n) ∑ᵢ (dᵢ − d̄)(dᵢ − d̄)ᵀ\\n  d̄ := (1/n) ∑ᵢ dᵢ","explanationImage":"lecture01/lecture04_ex04.png"},{"id":24,"questionDe":"(s.17) Nennen Sie drei Eigenschaften des Mahalanobis-Abstands.","questionJa":"マハラノビス距離の性質を3つ挙げよ。","answerDe":["M ist eine Kovarianzmatrix","Invariant bezüglich Verschiebungen","Invariant bezüglich Skalierung"],"answerJa":["Mは共分散行列である","平行移動に対して不変である","スケーリングに対して不変である"],"explanationDe":["Die Matrix M ist die Kovarianzmatrix der Daten und enthält Informationen über Varianz und Korrelationen.","Wenn alle Datenpunkte im Raum um denselben Vektor verschoben werden, bleibt der Mahalanobis-Abstand gleich – er ist translationsinvariant.","Auch wenn die Daten skaliert werden (z. B. alle Werte verdoppelt), bleibt der Abstand erhalten, weil M dies mitkorrigiert – er ist skaleninvariant.","Deshalb ist dieser Abstand besonders nützlich, wenn die Merkmale unterschiedliche Maßeinheiten oder Korrelationen aufweisen."],"explanationJa":["Mは共分散行列であり、データの分散や特徴量同士の相関関係を反映します。","すべてのデータを同じベクトルだけ平行移動しても、マハラノビス距離は変化しません。これを『平行移動に対して不変』と言います。","また、すべてのデータを同じスケールで拡大・縮小しても、共分散行列が同様に変化するため、距離としては変化しません。これは『スケーリングに対して不変』という性質です。","これにより、異なる単位をもつ特徴量や、相関を含む特徴量の扱いにおいても、適切な距離が計算できます。"],"originalSlideText":"- Bemerkungen\\n  – M: Co-Varianz Matrix\\n  – Invariant bezüglich Verschiebungen\\n  – Invariant bezüglich Skalierung"}]');const u={class:"container py-4"},h={class:"text-center mb-5"},g={class:"display-5 fw-bold text-primary"},c={class:"fs-5 text-muted"},k={class:"text-dark"};var m={__name:"Lecture04Page",setup(e){const n=(0,t.lq)(),i=(0,r.KR)(""),m=(0,r.KR)(""),b=(0,r.KR)(""),w=(0,r.KR)([]);return(0,a.sV)(()=>{const e="lecture01",a=parseInt(n.name.split("_")[1]),s=d[e];i.value=s.title,b.value=a.toString().padStart(2,"0");const r=s.lectures.find(e=>e.number===a);m.value=r?r.title:"",w.value=o}),(e,n)=>((0,a.uX)(),(0,a.CE)("div",u,[(0,a.Lk)("div",h,[(0,a.Lk)("h1",g,(0,s.v_)(i.value),1),(0,a.Lk)("p",c,[(0,a.eW)(" Lecture "+(0,s.v_)(b.value)+": ",1),(0,a.Lk)("span",k,(0,s.v_)(m.value),1)]),n[0]||(n[0]=(0,a.Lk)("hr",{class:"w-25 mx-auto border-primary"},null,-1))]),((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(w.value,e=>((0,a.uX)(),(0,a.Wv)(l.A,{key:e.id,question:e},null,8,["question"]))),128))]))}};const b=m;var w=b}}]);
//# sourceMappingURL=543.7f0c85c5.js.map